<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch | Maximilian Seeth </title> <meta name="author" content="Maximilian Seeth"> <meta name="description" content="Tutorial with full code"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://omseeth.github.io/blog/2024/CNN_sentiment_analysis/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Maximilian</span> Seeth </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch</h1> <p class="post-meta"> Created in December 27, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/cnn"> <i class="fa-solid fa-hashtag fa-sm"></i> CNN</a>   <a href="/blog/tag/sentiment"> <i class="fa-solid fa-hashtag fa-sm"></i> sentiment</a>   <a href="/blog/tag/analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> analysis</a>   <a href="/blog/tag/text"> <i class="fa-solid fa-hashtag fa-sm"></i> text</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/tag/deep"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep</a>   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Learning</a>   <a href="/blog/tag/tutorial"> <i class="fa-solid fa-hashtag fa-sm"></i> tutorial</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This tutorial is an introduction to <strong>Convolutional Neural Networks</strong> (CNNs) for <strong>sentiment analysis</strong> with PyTorch. There are already a few tutorials and solutions for this task by <a href="https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7" rel="external nofollow noopener" target="_blank">Gal Hever</a>, <a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/" rel="external nofollow noopener" target="_blank">Jason Brownlee</a>, or <a href="https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/3%20-%20Convolutional%20Neural%20Networks.ipynb" rel="external nofollow noopener" target="_blank">Ben Trevett</a>. However, they are either written in Keras or lack some explanations that I find essential for understanding the underlying mechanics of CNNs as well as their PyTorch specific implementations. I hope this tutorial will therefore help the interested reader to learn more about CNNs and how to implement them for NLP tasks, such as sentiment analysis.</p> <p>What follows is partially the result of a lecture given by <a href="https://bplank.github.io/" rel="external nofollow noopener" target="_blank">Barbara Plank</a> at LMU in 2024, whom I’d like to thank along with her lab <a href="https://mainlp.github.io/" rel="external nofollow noopener" target="_blank">MaiNLP</a> for letting me use some of their code. The theoretical part discussed in this tutorial is also highly indebted to <a href="https://github.com/rasbt/machine-learning-book" rel="external nofollow noopener" target="_blank">Raschka et al. (2022)</a>, whose book on machine learning is for me one of the best resources.</p> <p>This tutorial is divided into the following chapters:</p> <ul> <li> <strong>Preliminaries</strong> imports</li> <li> <strong>Section 1)</strong> Preprocessing the dataset</li> <li> <strong>Section 2)</strong> Theoretical foundations of CNNs for classification</li> <li> <strong>Section 3)</strong> Implementing a CNN with PyTorch</li> <li> <strong>Section 4)</strong> Evaluation of results</li> </ul> <h2 id="preliminaries-imports">Preliminaries: imports</h2> <p>In this tutorial, we’ll be using several different libraries. The following imports shall become clear as we develop our project. I’m using Python <strong>3.12.0</strong>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">In case the packages are not installed on your machine, let</span><span class="sh">'</span><span class="s">s begin with 
installing the ones that we</span><span class="sh">'</span><span class="s">ll need for our task. We can run commands with an 
exclamation mark in Jupyter Notebooks.</span><span class="sh">"""</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span> <span class="n">transformers</span> <span class="n">tokenizers</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span> <span class="n">torch</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">trainers</span>
<span class="kn">from</span> <span class="n">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</code></pre></div></div> <h2 id="section-1-preprocessing-the-dataset">Section 1) Preprocessing the dataset</h2> <h4 id="loading-the-data">Loading the data</h4> <p>Our goal is to use data to train a model that can identify the sentiment of a given text instance. In other words, we’ll implement a classifier using supervised learning. The backbone of our sentiment classifier will be a CNN. The data we’re using is taken from <a href="https://huggingface.co/datasets/dair-ai/emotion" rel="external nofollow noopener" target="_blank">Saravia et al. (2018)</a>. It consists of English Twitter messages labeled with six “emotions”: <em>anger</em>, <em>fear</em>, <em>joy</em>, <em>love</em>, <em>sadness</em>, and <em>surprise</em>. The dataset is available on HuggingFace and we can load it with the <a href="https://pypi.org/project/datasets/" rel="external nofollow noopener" target="_blank">datasets package</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">dair-ai/emotion</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can have a quick look at how the dataset is splitted and structured.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">16000</span>
    <span class="p">})</span>
    <span class="n">validation</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
<span class="p">})</span>
</code></pre></div></div> <p>The labels are already numericalized and the text is lowercased, as we can see when inspecting a single instance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The label IDs correspond to the following label order:
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">sadness</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">joy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">anger</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fear</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">surprise</span><span class="sh">"</span><span class="p">]</span>

<span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">i didnt feel humiliated</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">im grabbing a minute to post i feel greedy wrong</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am ever feeling nostalgic about the fireplace i will know that it is still on the property</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am feeling grouchy</span><span class="sh">'</span><span class="p">],</span>
 <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
</code></pre></div></div> <p>To allow easier processing, it helps to bind the respective splits to different variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <h4 id="tokenization">Tokenization</h4> <p>Before we can use the data to train our model, we need to split up the sentences into tokens. We also want to batch our data so that the model can be trained with more instances at the same time. In this tutorial, we’ll do the tokenization and batching with custom functions.</p> <p>For our tokenization, we can use a subword tokenization algorithm that is called <a href="https://huggingface.co/learn/nlp-course/en/chapter6/5" rel="external nofollow noopener" target="_blank">Byte Pair Encoding (BPE)</a>, which was introduced in Sennrich et al. (2016) and is based on an algorithm introduced by Gage (1994). The motivation of this tokenization technique is to let the data decide what subword tokens we’ll have. Another advantage of BPE is that it allows us to deal with unknown words that won’t be part of the training vocabulary but might appear in test data. With BPE, unknown words can be decomposed into their respective subword parts and in this fashion still be processed.</p> <p>Instead of implementing the BPE tokenizer completely from scratch, we can use the <a href="https://pypi.org/project/tokenizers/" rel="external nofollow noopener" target="_blank">tokenizers package</a>. We also want to pad and truncate each text with a fixed sequence length. If text instances are short, we pad them with 0s; if they are long, we truncate them to the maximum length. This will make the training and inference processes easier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We want to begin with only 5000 words in our vocabulary and a fixed sequence 
# length of 64
</span><span class="n">vocab_n</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Initialize a tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="nc">BPE</span><span class="p">())</span>

<span class="c1"># We can have a preliminary splitting of the text on spaces and punctuations 
# with Whitespace()
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="nc">Whitespace</span><span class="p">()</span>

<span class="c1"># Padding instances to the right with 0s
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_padding</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># Truncating long sequences
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_truncation</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># We limit our vocabulary and train the tokenizer
</span><span class="n">tokenizer_trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="p">.</span><span class="nc">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_n</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">train_from_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">trainer</span><span class="o">=</span><span class="n">tokenizer_trainer</span><span class="p">)</span>
</code></pre></div></div> <p>After training our custom BPE tokenizer, we aim to transform the raw training data into vector representations, where the input is tokenized and converted into corresponding token IDs. Fortunately, the labels are already numerical. That said, we still need to convert these numerical representations into PyTorch tensors to enable the PyTorch model to process the training instances. To accomplish this, we can write a few custom functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to tokenize text and return corresponding token IDs as tensors.

    Args:
        text, str: Text instance from training data.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        Tensor: One-dimensional PyTorch tensor with token IDs.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">ids</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to return label as tensor.

    Args:
        label, int: Label from instance.
    Returns:
        Tensor: One-dimensional PyTorch tensor containing the label index.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Transforms input dataset to tokenized vector representations.

    Args:
        data, dict: Dictionary with text instances and labels.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        list: List with tensors for the input texts and labels.
    </span><span class="sh">"""</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        
        <span class="n">instances</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">instances</span>
</code></pre></div></div> <p>Let’s tokenize, pad, and truncate our datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">val_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">validation_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>We shall inspect the second training instance, which was:</p> <p><code class="language-plaintext highlighter-rouge">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake.</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="p">(</span><span class="nf">tensor</span><span class="p">([</span>   <span class="mi">8</span><span class="p">,</span>  <span class="mi">161</span><span class="p">,</span>  <span class="mi">103</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>   <span class="mi">55</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span> <span class="mi">1173</span><span class="p">,</span>   <span class="mi">36</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span>  <span class="mi">807</span><span class="p">,</span>  <span class="mi">587</span><span class="p">,</span> <span class="mi">1129</span><span class="p">,</span>
            <span class="mi">130</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>  <span class="mi">219</span><span class="p">,</span>  <span class="mi">382</span><span class="p">,</span>  <span class="mi">444</span><span class="p">,</span>  <span class="mi">197</span><span class="p">,</span> <span class="mi">2819</span><span class="p">,</span>   <span class="mi">42</span><span class="p">,</span>   <span class="mi">47</span><span class="p">,</span> <span class="mi">2670</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">]),</span>
    <span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <p>We can observe that “i” has token index 8. Also notice how “so” appears in the vector representation of the text with index 58 twice. The corresponding label for the sequence is 0 and would be “sadness”.</p> <h4 id="batching">Batching</h4> <p>Batching the instances will enable the model to process multiple instances simultaneously. To achieve this, let’s write a custom batching function. There are different ways to perform batching, which involves concatenating instances and dividing them into manageable groups. One convenient method is to use the <a href="https://pytorch.org/docs/main/generated/torch.stack.html" rel="external nofollow noopener" target="_blank">torch.stack()</a> function, which concatenates a sequence of tensors. However, all tensors must be of the same length. Since we already padded and truncated our text instances, this requirement is satisfied.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Batches input instances along the given size and returns list of batches.

    Args:
        instances, list: List of instances, containing a tuple of two tensors 
            for each text as well as corresponding label.
        batch_size, int: Size for batches.
        shuffle, bool: If true, the instances will be shuffled before batching.
    Returns:
        list: List containing tuples that correspond to single batches.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>

    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># We iterate through the instances with batch_size steps
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">instances</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>

        <span class="c1"># Stacking the instances with dim=0 (default value)
</span>        <span class="n">batch_texts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">batch_texts</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">batches</span>
</code></pre></div></div> <h2 id="section-2-theoretical-foundations-of-cnns-for-classification">Section 2) Theoretical foundations of CNNs for classification</h2> <p>After having prepared our data, we can start with implementing our model. In this tutorial, we’ll be using a Convolutional Neural Network (CNN), which was introduced in LeCun et al. (1989). The CNN bears resemblance to the Neocognitron (Fukushima 1980) as well as Time-Delay Neural Networks (Waibel et al. 1989). This section is dedicated to the theoretical background of CNNs. In <strong>Section 3</strong>, we’ll implement the model with PyTorch.</p> <h4 id="the-architecture-of-a-cnn">The architecture of a CNN</h4> <p>A CNN is a fully connected network that operates similar to Feedforward Neural Networks (FNNs) in a feedforward fashion. The input is processed along layers and forwarded to the final layer, which can be used for predictions. The idea of a CNN is to consecutively extract salient features, such as shapes in terms of their respective pixels, building an abstract feature hierarchy for any given input. In this fashion, the model will be aligned through its training to reoccurring patterns, which it can “recognize.” It’s no surprise that CNNs were invented for handwritten digit recognition in images.</p> <p>The basic architecture of a CNN is as follows: For any given input, the CNN uses <strong>convolutions</strong>, that is, a <strong>filter</strong> (also called <strong>kernel</strong>) technique to extract local features from the input (e.g., pixels). After this filter has <strong>shifted</strong> over the whole input, the extractions are combined together into <strong>feature maps</strong> and then transformed with <strong>pooling</strong>. Pooling helps with singling out dominant features and allows to reduce the dimensions of the feature maps. This process can be repeated multiple times where additional filters can shift again over the pooled output from previous convolutions. Finally, for the last part of the network a FNN is used to produce logits for the desired task.</p> <p>One CNN layer consists of one convolutional and one pooling layer.</p> <h4 id="filtering">Filtering</h4> <p>Filters shift over the input. If the input is one-dimensional, that is, a vector, then the filter is itself a one-dimensional, usually smaller vector. If the input is two-dimensional, a matrix, then the filter needs to be a matrix, too.</p> <p>For a one-dimensional input of length \(n=8\), such as \([2, 1, 0, 3, 6, 7, 9, 1]\), we can define a filter of size \(m=3\), such as \([1, -1, 0]\). As the shifting starts, we’ll multiply the first three entries of the input vector with the filter:\([2, 1, 0] * [1, -1, 0]^T = 1\) In this fashion, we obtain the first entry for our feature map: \(y[0]=1\). Next, we shift the filter one step forward (with <strong>stride</strong>=1) and repeat the multiplication, until the filter reaches the end of the input, bounded by its last index (see <strong>Fig. 1</strong>). We can formalize this operation between the filter vector <em>f</em> and the input vector <em>e</em> as follows:</p> \[e*f\rightarrow y[i] = \sum_{k=1}^m{e[i+k-1]\cdot f[k]}\] <p>In fact, the filter size is a hyperparameter that can be set to any positive integer smaller than the input length. In those scenarios where the filter is very large and exceeds the input, we can pad the input with additional elements so that the filter operation would be possible. There a several padding strategies to avoid that elements from the middle of the input will be covered more frequently by the filter than those from the edges.</p> <p>In our implementation (<strong>Section 3</strong>), we’ll be using the <em>same</em> padding strategy, which adds zeros to the input on all sides so that the output of the filter operation can have the same size as the input. Therefore, the amount of padding will depend on the size of the filter(/kernel). Consider Raschka et al. (2022, p. 457) for further details.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_1d-480.webp 480w,/assets/img/CNN_sentiment/convolution_1d-800.webp 800w,/assets/img/CNN_sentiment/convolution_1d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/CNN_sentiment/convolution_1d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 1</strong>: One-dimensional convolution</p> <p>The convolution described previously for one-dimensional inputs works in exactly the same way for two-dimensional inputs (see <strong>Fig. 2</strong>). If \(E_{n_1\times n_2}\) is the input matrix and \(F_{m_1\times m_2}\) the filter where \(m_1\leq n_1\) and \(m_2\leq n_2\), for each stride we can compute our feature map as follows:</p> \[E*F\rightarrow y[i][j] = \sum_{k_1=1}^{m_1}\sum_{k_2=1}^{m_2}{e[i+k_{1}-1][j+k_{2}-1]\cdot f[k_1][k_2]}\] <p>with \(y[i][j]\) corresponding to the respective feature representation in the map, which is also a matrix.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_2d-480.webp 480w,/assets/img/CNN_sentiment/convolution_2d-800.webp 800w,/assets/img/CNN_sentiment/convolution_2d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/CNN_sentiment/convolution_2d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 2</strong>: Two-dimensional convolution</p> <h4 id="subsampling-layers-pooling">Subsampling layers: pooling</h4> <p>The idea of pooling layers is to reduce the size of the feature map and abstract from the features, either by picking out one local maximum feature value or averaging over a group of feature values. The former strategy is called max pooling, the latter is called average pooling. We’ll focus on max pooling (see <strong>Fig. 3</strong>).</p> <p>A pooling layer can be defined as \(P_{n_1 \times n_2}\), where the subscript indicates the area size over which the max operation is performed, while shifting across the entire map.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/max_pooling-480.webp 480w,/assets/img/CNN_sentiment/max_pooling-800.webp 800w,/assets/img/CNN_sentiment/max_pooling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/CNN_sentiment/max_pooling.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 3</strong>: Max pooling</p> <h4 id="adding-activation-functions">Adding activation functions</h4> <p>A FNN usually consists of a layer that is defined as \(z=Wx + b\) where \(W\) are the weights and \(b\) is an extra bias for input \(x\). Such a layer is often wrapped by a non-linear activation function \(\sigma\), as in \(A=\sigma (z)\). We adapt the convolutional layer so that it resembles the previous FNN construction. Let \(c = E*F + b\) be our layer with an additional bias term \(b\). As with the FFN we can wrap an activation function \(\sigma\) around \(c\). In many CNN implementations, the ReLu activation function is used for this purpose.</p> <p>LeCun et al. (1989) published their paper titled ‘Handwritten Digit Recognition with a Back-Propagation Network.’ Let us now examine which parts of a CNN are trainable using back-propagation. It’s the convolutional layers (filters) with their weights \(F\) and biases \(b\) (if included) that can be updated through back-propagation, guided by a loss function and gradient optimization. Recall that most CNNs also include a final fully connected feedforward neural network (FNN) layer for producing the output, and this layer is trainable as well. However, the pooling layer is not involved in the training process, as it does not contain learnable parameters.</p> <h4 id="multiple-channels">Multiple channels</h4> <p>The input to a CNN can be greater than one. If we’re dealing with images that are encoded in terms of RGB colors, we can split up the input into three two-dimensional matrices corresponding to red, green, and blue color information and feed them to the CNN. Each input matrix would be called one <strong>channel</strong>. For this matter, in most CNN implementations the convolutional layers expect an input with <strong>3 dimensions</strong> where \(E_{n_1\times n_2 \times c_{in}}\) would be the input of dimensions \(n_1\times n_2\) times \(c_{in}\), such as \(c_{in} = 3\) for three different color matrices.</p> <p>The further processing of the input allows a lot of variability: Each \(c_{in}\) input will have its own filter. The filters can be also stored as three-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}}\). Usually, the filtered results from each respective input will be element-wise added to create the output feature map. But sometimes it also helps to have multiple feature maps as outputs to capture different aspects from the input. In this case, the filters can be changed to four-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}\times c_{out}}\) where \(c_{out}\) determines the numbers of feature maps that we want (see <strong>Fig. 4</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/four_dim_filter-480.webp 480w,/assets/img/CNN_sentiment/four_dim_filter-800.webp 800w,/assets/img/CNN_sentiment/four_dim_filter-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/CNN_sentiment/four_dim_filter.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 4</strong>: Multiple filters (kernels) for three input channels, producing four feature maps</p> <p>Note, if we have for example four feature maps, we’ll also need four pooling layers. As the size of convolutional layers changes we’ll also have more parameters to train. This being said, remember that a FNN processing an input image would require weights of same size for its first layer. In contrast, the parameters of CNNs are bounded by the filter size. LeCun et al. (1989, p. 399) describe this aspect as “weight sharing” and note as one distinctive feature of CNNs that they significantly reduce the amount of parameters to be trained.</p> <h2 id="section-3-implementing-a-cnn-with-pytorch">Section 3) Implementing a CNN with PyTorch</h2> <p>Let’s begin with defining our classifier model. It’s reasonable to use embeddings for our input tokens. For that matter, our model should start with an embedding layer. The embeddings will be handed over to our CNN, which will consist of two convolutional layers with two different filters(/kernels). Finally, the input abstracted throughout the network needs to be passed to a final FFN layer for our sentiment prediction.</p> <p>To build an intuition for the whole process, let’s think again of the input. The input is a sequence (a tweet) from our data. This sequence will be translated into its correspoding token representation based on our custom trained BPE encoder. In other words, any incoming sequence will be a tensor of different indices. Next, the indices are handed over to our embedding layer, which serves as a look-up table for all tokens in the vocabulary and assigns the embedding representation to the input indices. This layer is a simple linear layer that is going to be trained along with the whole network later on. The initial values of the embeddings are random numbers.</p> <p>Up to this point, we can think of two strategies to run a first convolution over the input. If the input is 12 tokens long and each token has an embedding vector of size 300, the input to a potential convolutional layer would be \(12 \times 300\). The first strategy could be to apply a two-dimensional filter striding over this input matrix. The second strategy would be to use a one-dimensional filter that is going over one embedding dimension per time for the whole sequence, that is, over \(12 \times 1\) but with 300 channels for all embedding dimensions. If we want the output to be of the same length as the input, that is, 12, we need to pad the input to adjust for the filter length.</p> <p>In our implementation, we shall try the second strategy. Remember that the input channels will all be added together so before having our 300 embedding dimensions shrunk to one, we shall define an appropriate output channel size for the convolution. In other words, we’ll use as many as 100 filters for example to create an output of 100 dimensions for each respective input token.</p> <p>After having defined our model, we need to implement the training loop with a fitting method. To understand our training process we also need an evaluation mechanism. Finally, we need another method for making predictions from the trained classifier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN_Classifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    CNN for sentiment classification with 6 classes, consisting of an embedding 
    layer, two convolutional layers with different filter sizes, different 
    pooling sizes, as well as one linear output layer.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># We can implement embeddings as a simple lookup-table for given word 
</span>        <span class="c1"># indices
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_vocab_size</span><span class="p">(),</span> <span class="mi">300</span><span class="p">)</span>

        <span class="c1"># One-dimensional convolution-layer with 300 input channels, and 100  
</span>        <span class="c1"># output channels as well as kernel size of 3; note that the
</span>        <span class="c1"># one-dimensional convolutional layer has 3 dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with with a one-dimensional sliding window of length 3, 
</span>        <span class="c1"># reducing in this fashion the sequence length 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># The input will be the reduced number of maximum picks from the
</span>        <span class="c1"># previous operation; the dimension of those picks is the same as the
</span>        <span class="c1"># output channel size from self.conv_1. We apply a different filter of 
</span>        <span class="c1"># size 5.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with window size of 5
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># Final fully connected linear layer from the 50 output channels to the
</span>        <span class="c1"># 6 sentiment categories 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Defining the forward pass of an input batch x.

        Args:
            x, tensor: The input is a batch of tweets from the data.
        Returns:
            y, float: The output are the logits from the final layer.
        </span><span class="sh">"""</span>
        <span class="c1"># x will correspond here to a batch; therefore, the input dimensions of 
</span>        <span class="c1"># the embedding will be by PyTorch convention as follows:
</span>        <span class="c1"># [batch_size, seq_len, emb_dim]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Unfortunately the embedding tensor does not correspond to the shape 
</span>        <span class="c1"># that is needed for nn.Conv1d(); for this reason, we must switch its 
</span>        <span class="c1"># order to [batch_size, emb_dim, seq_len] for PyTorch
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We can wrap the ReLu activation function around our convolution layer
</span>        <span class="c1"># The output tensor will have the following shape: 
</span>        <span class="c1"># [batch_size, 100, seq_len]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Applying max pooling of size 3 means that the output length of the 
</span>        <span class="c1"># sequence is shrunk to seq_len//3
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Output of the following layer: [batch_size, 50, seq_len//3]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Shrinking the sequence length by 5
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>
        <span class="c1"># At this point we have a tensor with 3 dimensions; however, the final layer 
</span>        <span class="c1"># requires an input of size [batch_size x 50]. To get this value we can 
</span>        <span class="c1"># aggregate the values and continue only with their mean
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># In this fasion, the linear layer can be used to make predictions
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Gradient based fitting method with Adam optimization and automatic 
        evaluation (F1 score) for each epoch.

        Args:
            train_instances, list: List of instance tuples.
            val_instances, list: List of instance tuples.
            epochs, int: Number of training epochs.
            batch_size, int: Number of batch size.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">train_batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span>
                <span class="n">train_instances</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_batches</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            
            <span class="n">train_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">val_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">val_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> train F1 score: </span><span class="si">{</span><span class="n">train_f1</span><span class="si">}</span><span class="s">, validation F1 score: </span><span class="si">{</span><span class="n">val_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make inferences from the model.

        Args:
            input, tensor: Single instance.
        Returns:
            int: Integer for most probable class.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make evaluations against the gold standard (true labels) from the 
        data.

        Args:
            instances, list: List of instance tuples.
            batch_size, int: Batch size.
        Returns:
            float: Macro F1 score for given instances.
        </span><span class="sh">"""</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">true</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
            <span class="n">true</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">pred</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

        <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="sh">"</span><span class="s">macro</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can now train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="nc">CNN_Classifier</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">26</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">37.29</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.6349625340555586</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.5885997748733948</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">27</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">36.16</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">2</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9181883927660527</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8391332797173209</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">31</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">31.86</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">3</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9548322090243025</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8665508014136801</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">30</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">32.63</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">4</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9715161826479329</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.851259672136001</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">33</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">30.13</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">5</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9830196392925995</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8649424900902641</span>
</code></pre></div></div> <h2 id="section-4-evaluation-of-results">Section 4) Evaluation of results</h2> <p>After we have trained our model, we can use our test set to see how well it predicts sentiments for unseen examples.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f1_test</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">test_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">F1 score for test set: </span><span class="si">{</span><span class="n">f1_test</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">F1 score for test set: 0.8403978699590415</code></p> <h4 id="hyperparameters">Hyperparameters</h4> <p>There are many hyperparameters in the entire modeling process that we can adjust to achieve even better results. These include different layer sizes, filters, poolings, and more efficient optimization techniques. Typically, dropout is applied during the training of such models to prevent overfitting. It is also worthwhile to explore the possibility of using pre-trained embedding representations.</p> <p>The <strong>Jupyter notebook</strong> for this project can be found here: <a href="https://github.com/omseeth/cnn_sentiment_analysis" rel="external nofollow noopener" target="_blank">https://github.com/omseeth/cnn_sentiment_analysis</a></p> <h2 id="references">References</h2> <p>Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biol. Cybernetics 36</em>, pages 193–202.</p> <p>Gage, P. (1994). A new algorithm for data compression. <em>C Users J.</em>, 12(2):23–38.</p> <p>LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. (1989). Handwritten digit recognition with a back-propagation network. In Touretzky, D., editor, <em>Advances in Neural Information Processing Systems</em>, volume 2. Morgan-Kaufmann</p> <p>Saravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y.-S. (2018). CARER: Contextualized affect representations for emotion recognition. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 3687–3697, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Sebastian, R., Yuxi, L., and Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Erk, K. and Smith, N. A., editors, <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics</p> <p>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. and Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. In <em>IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37</em>, no. 3, pages 328-339.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Explaining_BERT/">Explaining BERT-based hate speech detection with LIME and Saliency using Captum</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/AI_ethics_primer/">AI ethics, a primer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/responspibility_protraction/">Responsibility Protraction with Remotely Controlled Combat Vehicles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms_syntactic_research/">Using LLMs in Syntactic Research</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/argument_mining/">Was ist Argument Mining?</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Maximilian Seeth. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-explaining-bert-based-hate-speech-detection-with-lime-and-saliency-using-captum",title:"Explaining BERT-based hate speech detection with LIME and Saliency using Captum",description:"Tutorial with full code",section:"Posts",handler:()=>{window.location.href="/blog/2025/Explaining_BERT/"}},{id:"post-how-to-use-a-convolutional-neural-network-cnn-for-text-classification-sentiment-analysis-with-pytorch",title:"How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis)...",description:"Tutorial with full code",section:"Posts",handler:()=>{window.location.href="/blog/2024/CNN_sentiment_analysis/"}},{id:"post-ai-ethics-a-primer",title:"AI ethics, a primer",description:"Introduction to AI ethics that\u2019s less than 1000 words long",section:"Posts",handler:()=>{window.location.href="/blog/2024/AI_ethics_primer/"}},{id:"post-responsibility-protraction-with-remotely-controlled-combat-vehicles",title:"Responsibility Protraction with Remotely Controlled Combat Vehicles",description:"This paper examines drone warfare from an ethical perspective",section:"Posts",handler:()=>{window.location.href="/blog/2024/responspibility_protraction/"}},{id:"post-using-llms-in-syntactic-research",title:"Using LLMs in Syntactic Research",description:"Unpublished paper exploring how to use LLMs for empirical studies of syntax",section:"Posts",handler:()=>{window.location.href="/blog/2024/llms_syntactic_research/"}},{id:"post-was-ist-argument-mining",title:"Was ist Argument Mining?",description:"Eine Einf\xfchrung zum Forschungsfeld des Argument Minings",section:"Posts",handler:()=>{window.location.href="/blog/2024/argument_mining/"}},{id:"post-questioning-recursion-as-a-distinctive-feature-of-human-language",title:"Questioning Recursion as a Distinctive Feature of Human Language",description:"Putting human language into perspective with animal linguistics",section:"Posts",handler:()=>{window.location.href="/blog/2024/recursive_language/"}},{id:"post-transformer-models-such-as-gpt-and-bert-explained-english-version",title:"Transformer models such as GPT and BERT explained (English version)",description:"From neural networks to GPT and BERT, a contextualized explanation of the Transformer architecture",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer_en/"}},{id:"post-transformer-modelle-wie-gpt-und-bert-erkl\xe4rt-german-version",title:"Transformer-Modelle wie GPT und BERT erkl\xe4rt (German version)",description:"Vom neuronalen Netz bis zu GPT und BERT, eine kontextualisierte Erkl\xe4rung der Transformer-Architektur",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"news-happy-to-discuss-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-responsibility-and-ai-title-why-quot-responsible-ai-quot-is-a-misnomer-abstract-the-meaning-of-responsibility-is-closely-attached-to-the-idea-of-quot-responding-quot-in-a-justified-sense-for-one-39-s-deeds-for-that-matter-responsibility-is-also-often-interpreted-as-quot-answerability-quot-the-same-holds-for-the-german-quot-verantwortung-quot-and-the-correlated-idea-of-quot-rede-und-antwort-stehen-quot-however-current-ai-cannot-be-responsible-for-different-reasons-any-existing-ai-system-up-to-date-lacks-the-capacity-to-reasonably-justify-itself-another-fundamental-aspect-of-taking-responsibility-is-also-to-accept-consequences-for-certain-actions-such-as-punishments-for-unlawful-behavior-however-a-machine-learning-system-cannot-be-punished-in-any-meaningful-way-in-short-ai-is-not-responsible-responsible-ai-is-a-misnomer-but-who-is-responsible-and-how-can-responsibility-with-ai-look-like",title:"Happy to discuss at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) responsibility and AI. \\...",description:"",section:"News"},{id:"news-in-my-45-minutes-presentation-i-discussed-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-llms-and-the-possibility-of-lying-title-when-moral-rules-reach-their-limits-should-llms-lie-abstract-one-of-the-most-famous-ideas-in-philosophy-is-immanuel-kant-39-s-categorical-imperative-quot-act-only-according-to-that-maxim-whereby-you-can-at-the-same-time-will-that-it-should-become-a-universal-law-quot-this-imperative-gives-us-a-concrete-solution-as-to-how-to-act-morally-it-allows-us-to-define-ethical-rules-from-non-ethical-ones-at-the-same-time-kant-is-also-known-for-proposing-that-to-not-lie-should-be-such-a-binding-moral-rule-however-we-encounter-many-situations-where-humans-lie-and-also-some-where-lying-appears-even-ethically-licensed-if-so-following-a-rule-such-as-to-not-lie-does-not-seem-to-be-always-what-we-want-from-an-ethical-point-of-view-if-we-as-humans-allow-degrees-of-variabilities-to-truth-telling-what-does-that-mean-if-we-try-to-quot-teach-quot-an-llm-to-only-produce-true-and-factual-content",title:"In my 45 minutes presentation, I discussed at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154)...",description:"",section:"News"},{id:"news-i-spoke-again-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-about-agency-and-ai-title-agentic-ai-what-can-philosophy-teach-us-about-agency-abstract-agentic-ai-such-as-autogen-crew-ai-and-langgraph-has-recently-garnered-significant-attention-stanford-39-s-andrew-ng-renowned-for-his-neural-network-teachings-on-coursera-has-also-lauded-the-use-of-multi-agent-design-patterns-with-llms-in-the-batch-the-concept-is-straightforward-multiple-llm-powered-agents-collaborate-to-solve-problems-by-incorporating-local-memories-and-internal-reflection-capabilities-the-resulting-system-of-agents-can-be-both-powerful-and-complex-however-as-critical-ai-philosophers-we-must-ask-ourselves-do-these-agents-truly-possess-agency-that-is-the-ability-to-take-action-to-explore-the-potential-of-non-human-agents-this-talk-will-introduce-fundamental-definitions-and-conceptual-tools-to-discuss-agency-in-the-context-of-llm-agents-it-appears-that-agency-is-a-continuous-attribute-rather-than-a-categorical-one",title:"I spoke again at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) about agency and AI....",description:"",section:"News"},{id:"news-i-39-m-presenting-a-few-machine-learning-fundamentals-for-understanding-llms-at-rewe-digital-https-www-rewe-digital-com",title:"I&#39;m presenting a few machine learning fundamentals for understanding LLMs at [Rewe Digital](https://www.rewe-digital.com/)....",description:"",section:"News"},{id:"news-my-next-talk-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-is-scheduled-for-september-9-at-13-00-via-microsoft-teams-please-contact-me-for-the-link-title-can-we-trust-ai-abstract-trustworthiness-in-the-ai-scene-is-often-grounded-in-the-notion-of-having-a-system-that-produces-reliable-and-factually-correct-outputs-but-is-believing-that-an-output-is-true-equal-to-quot-trusting-quot-the-system-that-created-it-human-trust-is-a-complex-philosophical-and-psychological-notion-trust-may-be-a-state-of-belief-as-well-as-a-form-of-social-binding-for-example-trust-between-humans-often-implies-a-certain-degree-of-mutual-vulnerability-taking-this-into-account-is-it-even-possible-to-trust-in-machines",title:"My next talk at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) is scheduled for September...",description:"",section:"News"},{id:"news-i-39-m-attending-the-conference-on-the-ethics-of-conversational-agents-amp-generative-ai-hosted-by-the-munich-center-for-machine-learning-https-mcml-ai",title:"\ud83d\udc40 I&#39;m attending the Conference on the Ethics of Conversational Agents &amp; Generative...",description:"",section:"News"},{id:"news-my-latest-talk-on-trust-in-ai-is-now-online-https-www-youtube-com-watch-v-phndlgwnlsu-https-www-youtube-com-watch-v-phndlgwnlsu",title:"My latest talk on Trust in AI is now online: [https://www.youtube.com/watch?v=PhNdlgwNlsU](https://www.youtube.com/watch?v=PhNdlgwNlsU) \u2728\ud83c\udfa5",description:"",section:"News"},{id:"news-back-at-rewe-digital-https-www-rewe-digital-com-introducing-the-fundamentals-of-language-models-to-a-group-of-managers",title:"Back at [Rewe Digital](https://www.rewe-digital.com/), introducing the fundamentals of language models to a group...",description:"",section:"News"},{id:"news-we-re-launching-www-dailogues-ai-https-www-dailogues-ai-our-discursive-space-around-ai",title:"We\u2019re launching [www.dailogues.ai](https://www.dailogues.ai) \ud83d\ude80, our discursive space around AI!",description:"",section:"News"},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/omseeth","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/oseeth","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/maxseeth.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>