<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Run LLMs on an External NVIDIA GPU Without Local Hardware (Step-by-Step Guide) | Maximilian Seeth </title> <meta name="author" content="Maximilian Seeth"> <meta name="description" content="Tutorial on how to rent an external NVIDIA GPU to run experiments with open-source models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://omseeth.github.io/blog/2025/experiment_llms/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Maximilian</span> Seeth </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Run LLMs on an External NVIDIA GPU Without Local Hardware (Step-by-Step Guide)</h1> <p class="post-meta"> Created in October 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a>   <a href="/blog/tag/vlms"> <i class="fa-solid fa-hashtag fa-sm"></i> vlms</a>   <a href="/blog/tag/vast-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> vast.ai</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/machine"> <i class="fa-solid fa-hashtag fa-sm"></i> machine</a>   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> learning</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/nets"> <i class="fa-solid fa-hashtag fa-sm"></i> nets</a>   <a href="/blog/tag/unsloth-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> unsloth.ai</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This tutorial explains how to run experiments with open-source foundation models (e.g.,<a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" rel="external nofollow noopener" target="_blank">Qwen/Qwen2.5-7B-Instruct</a> with external NVIDIA GPUs. All you need is €5–10 and an account on <a href="https://vast.ai/" rel="external nofollow noopener" target="_blank">vast.ai</a>. Alternatively, you could use services like <a href="https://www.runpod.io/" rel="external nofollow noopener" target="_blank">runpod.io</a>, or <a href="https://lambda.ai/" rel="external nofollow noopener" target="_blank">lambda.ai</a>. In this tutorial, I’ll walk you through setting up a GPU-powered instance on vast.ai.</p> <p><em>Why choose this approach?</em> With a small investment, you can work directly with core LLM libraries and experiment with open-source models. For example, I own a MacBookAir; however, essential libraries such as unsloth don’t support Apple chips, nor are many other libraries optimized for them. In my experience, Google Colab is less reliable when it comes to GPU availability (even for paid options). It’s also limited to notebooks, which restricts flexibility. Using a remote instance with GPUs is also helpful if you want to prepare your scripts before submitting heavy jobs on a high performance cluster. Finally, setting this up is a fun way to gain some hands-on ML Ops experience.</p> <h2 id="create-account-on-vastai-and-add-credit">Create account on vast.ai and add credit</h2> <p>After having created an account on vast.ai, charge your credit with 5€-10€.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/charge_credit-480.webp 480w,/assets/img/experiment_llms/charge_credit-800.webp 800w,/assets/img/experiment_llms/charge_credit-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/charge_credit.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 1:</strong> Account page on vast.ai</p> <h2 id="rent-instance">Rent instance</h2> <p>You can rent an instance immediately by clicking on rent. There are a few technical details to keep in mind to ensure the model runs smoothly. For this example, I want to inference from and fine-tune a <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" rel="external nofollow noopener" target="_blank">Qwen/Qwen2.5-7B-Instruct</a> model, using <a href="https://unsloth.ai/" rel="external nofollow noopener" target="_blank"><strong>unsloth</strong></a>. <strong>unsloth</strong> is a convenient Python library for fine-tuning models; it serves as a wrapper around the PEFT (parameter efficient fine-tuning) package, which is part of the PyTorch and HuggingFace universe.</p> <p>The instance from vast.ai should therefore have an NVIDIA GPU and a fitting Linux distribution.</p> <p>I recommend the following specifications:</p> <ul> <li> <strong>GPU</strong>: RTX 3060, RTX A4000 or other RTX; if more compute is needed an A100. The GPUs should have more than &gt;12GB</li> <li> <strong>RAM</strong>: Usually 2xVRAM is best, but at least 16GB should suffice</li> <li> <strong>HDD</strong>: 100GB</li> <li> <strong>CPU</strong>: whatever comes with the instance</li> <li> <strong>Bandwith</strong>: At least 100 Mb/s, since you’ll need to download large libraries and models.</li> </ul> <p>Each instance can use a different template. For LLM experiments, it’s easiest to start with the PyTorch template, which comes with CUDA, cuDNN, and many essential libraries pre-installed. This template runs inside an unprivileged Docker container.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/template-480.webp 480w,/assets/img/experiment_llms/template-800.webp 800w,/assets/img/experiment_llms/template-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/template.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 2:</strong> Template on vast.ai</p> <p>A few tips for selecting the right instance:</p> <ul> <li>Check that the instance’s resources (CPUs, RAM) aren’t already heavily used. Some servers share hardware, and it’s possible to find instances on vast.ai where CPU cores are busy.</li> <li>I’ve had problems with several instances; just delete them and search for a different one.</li> <li>As soon as the instance is being stopped, the system continues to exist and vast.ai will charge some money everyday for storage costs (not much though).</li> <li>Once you stop an instance, someone else can rent its GPU. You won’t be able to restart your instance until the other user finishes, which can take hours or even days.</li> </ul> <h2 id="connect-to-your-instance">Connect to your instance</h2> <p>You’ll connect to the instance via <code class="language-plaintext highlighter-rouge">ssh</code>. The example below uses macOS with <code class="language-plaintext highlighter-rouge">zsh</code>, but the setup is similar on other systems — mostly the file paths differ.</p> <p>Open your terminal and go to your hidden <code class="language-plaintext highlighter-rouge">.ssh</code> directory:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/.ssh
</code></pre></div></div> <p>If the directory doesn’t exist, create it in your home folder and set the correct permissions:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> ~/.ssh
<span class="nb">chmod </span>700 ~/.ssh
</code></pre></div></div> <p>Next we want to create a key, which we will call “vast_ai”, by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-t</span> ed25519 <span class="nt">-f</span> ~/.ssh/vast_ai <span class="nt">-C</span> <span class="s2">"Name or E-Mail address"</span>
</code></pre></div></div> <p>from <code class="language-plaintext highlighter-rouge">/.ssh</code>.</p> <p>You’ll be asked to assign a password.</p> <p>The key pair consists of a public and a private file. We want to open:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>vast_ai.pub
</code></pre></div></div> <p>and copy the publicly accessible content. We insert the content on our vast.ai account page and save it.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/ssh_key-480.webp 480w,/assets/img/experiment_llms/ssh_key-800.webp 800w,/assets/img/experiment_llms/ssh_key-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/ssh_key.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 3:</strong> Adding ssh key to vast.ai</p> <p>Next, we navigate to our instance. I’m using the following for this example:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/instance-480.webp 480w,/assets/img/experiment_llms/instance-800.webp 800w,/assets/img/experiment_llms/instance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/instance.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 4:</strong> Instance on vast.ai</p> <p>If we click on the small key symbol (our ssh key should appear, if not we add the ssh key here again) and on <code class="language-plaintext highlighter-rouge">ADD SSH KEY</code> then the section <code class="language-plaintext highlighter-rouge">direct ssh connect</code> should appear. vast.ai will show us which port we can use to connect to the instance.</p> <p>Copy the provided SSH command, add your key name, and run it in your terminal:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-i</span> ~/.ssh/vast_ai <span class="nt">-p</span> 10532 root@194.26.196.132 <span class="nt">-L</span> 8080:localhost:8080
</code></pre></div></div> <p>Enter the password you set for the key, and you’re in! vast.ai will automatically start a <code class="language-plaintext highlighter-rouge">tmux</code> session for our instance.</p> <h2 id="check-gpu-availability">Check GPU availability</h2> <p>At this point, we can check if the GPU is accessible with</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div> <p>We should see something like the following:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/nvidia-480.webp 480w,/assets/img/experiment_llms/nvidia-800.webp 800w,/assets/img/experiment_llms/nvidia-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/nvidia.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 5:</strong> nvidia-smi showing the GPU</p> <p>If not, for example, if there’s an issue with the driver, the easiest way might be just to delete the instance and look for another one.</p> <h2 id="activate-mamba-environment">Activate mamba environment</h2> <p>Luckily, the instance comes with <code class="language-plaintext highlighter-rouge">conda</code>and <code class="language-plaintext highlighter-rouge">mamba</code> pre-installed. For this reason, we can simply run the following to create a mamba environment which we can call “test_env”:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba create <span class="nt">-n</span> test_env <span class="nv">python</span><span class="o">=</span>3.12
</code></pre></div></div> <p>After the installation, initialize the shell for the environment:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>mamba shell hook <span class="nt">--shell</span> bash<span class="si">)</span><span class="s2">"</span>
</code></pre></div></div> <p>will do it and then we can activate it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba activate test_env
</code></pre></div></div> <h2 id="connect-to-your-instance-with-vs-code">Connect to your instance with VS Code</h2> <p>To make life much easier, we can also connect to our instance via VS Code. To do this, open your local terminal, create an <code class="language-plaintext highlighter-rouge">.ssh/config</code> file if it doesn’t already exist, and set the correct permissions.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/.ssh
<span class="nb">touch </span>config
<span class="nb">chmod </span>700 config
</code></pre></div></div> <p>We’ll have to add (e.g., with <code class="language-plaintext highlighter-rouge">nano</code>) the following information, depending on the instance respectively:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Host vast_ai_instance
    HostName &lt;IP-of-instance&gt;
    IdentityFile ~/.ssh/vast_ai    
    Port &lt;Port-of-instance&gt;
    User root
    LocalForward 8080 localhost:8080
</code></pre></div></div> <p>Next, we can open VS Code and try to connect to the remote host. The name from the config should appear as an option, when we click <code class="language-plaintext highlighter-rouge">Connect to Host...</code>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/experiment_llms/vs_code-480.webp 480w,/assets/img/experiment_llms/vs_code-800.webp 800w,/assets/img/experiment_llms/vs_code-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/experiment_llms/vs_code.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Fig. 6:</strong> ssh to remote host with VS Code</p> <h2 id="run-python-scripts">Run python scripts</h2> <p>At this point, we can use VS Code to create new python scripts on our instance. If we go back to our instance terminal, we can install the following packages:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> unsloth transformers accelerate bitsandbytes
</code></pre></div></div> <p>With these libraries installed, we can write a simple Python script (e.g., <code class="language-plaintext highlighter-rouge">test.py</code>) that automatically downloads an LLM and runs an inference on a prompt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">unsloth</span> <span class="kn">import</span> <span class="n">FastLanguageModel</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Qwen/Qwen2.5-7B-Instruct</span><span class="sh">"</span>

<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span><span class="p">,</span>
    <span class="n">load_in_4bit</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="c1"># Load in 4-bit precision to save memory (optional)
</span>    <span class="n">device_map</span> <span class="o">=</span> <span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># Auto place model on GPU if available
</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Explain what a high performance computer is.</span><span class="sh">"</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Generating response...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
    <span class="n">do_sample</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">=== Model Response ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></div> <p>Simply, run the script in your mamba environment with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 test.py
</code></pre></div></div> <p>In my case, the model was saved under <code class="language-plaintext highlighter-rouge">/workspace/.hf_home/hub</code> from where we can also remove it, if we want to make space.</p> <h2 id="fine-tune-model">Fine-tune model</h2> <p>From here on, you can fine-tune your model with unsloth, using one of their many notebooks as an inspiration: <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks" rel="external nofollow noopener" target="_blank">https://docs.unsloth.ai/get-started/unsloth-notebooks</a></p> <p>You may also connect to your git repository to save code or data for later use.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/MLLM_for_ARC/">Multimodal Reasoning to Solve the ARC-AGI Challenge</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/MCoT_sketching/">Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Tracing_early_texts/">Tracing Early Texts. A Linguistic and Historical Inquiry into Textuality</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/SoftBank_Pepper_gender_robotics/">How SoftBank’s Pepper set a positive example for gender design in robotics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Explaining_BERT/">Explaining BERT-based hate speech detection with LIME and Saliency using Captum</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Maximilian Seeth. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-run-llms-on-an-external-nvidia-gpu-without-local-hardware-step-by-step-guide",title:"Run LLMs on an External NVIDIA GPU Without Local Hardware (Step-by-Step Guide)",description:"Tutorial on how to rent an external NVIDIA GPU to run experiments with open-source models",section:"Posts",handler:()=>{window.location.href="/blog/2025/experiment_llms/"}},{id:"post-multimodal-reasoning-to-solve-the-arc-agi-challenge",title:"Multimodal Reasoning to Solve the ARC-AGI Challenge",description:"Ameliorating strategies for ARC-AGI with multimodal reasoning",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLLM_for_ARC/"}},{id:"post-research-directions-in-multimodal-chain-of-thought-mcot-with-sketching",title:"Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching",description:"This text explores adding sketching to Multimodal Chain-of-Thought (MCoT) reasoning to enhance AI capabilities",section:"Posts",handler:()=>{window.location.href="/blog/2025/MCoT_sketching/"}},{id:"post-tracing-early-texts-a-linguistic-and-historical-inquiry-into-textuality",title:"Tracing Early Texts. A Linguistic and Historical Inquiry into Textuality",description:"Identifying a 3100 BC Mesopotamian contract as an early instance of textuality",section:"Posts",handler:()=>{window.location.href="/blog/2025/Tracing_early_texts/"}},{id:"post-how-softbank-s-pepper-set-a-positive-example-for-gender-design-in-robotics",title:"How SoftBank\u2019s Pepper set a positive example for gender design in robotics",description:"This text explores the assignment of gender to humanoids",section:"Posts",handler:()=>{window.location.href="/blog/2025/SoftBank_Pepper_gender_robotics/"}},{id:"post-explaining-bert-based-hate-speech-detection-with-lime-and-saliency-using-captum",title:"Explaining BERT-based hate speech detection with LIME and Saliency using Captum",description:"Tutorial with full code",section:"Posts",handler:()=>{window.location.href="/blog/2025/Explaining_BERT/"}},{id:"post-how-to-use-a-convolutional-neural-network-cnn-for-text-classification-sentiment-analysis-with-pytorch",title:"How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis)...",description:"Tutorial with full code",section:"Posts",handler:()=>{window.location.href="/blog/2024/CNN_sentiment_analysis/"}},{id:"post-ai-ethics-a-primer",title:"AI ethics, a primer",description:"Introduction to AI ethics that\u2019s less than 1000 words long",section:"Posts",handler:()=>{window.location.href="/blog/2024/AI_ethics_primer/"}},{id:"post-responsibility-protraction-with-remotely-controlled-combat-vehicles",title:"Responsibility Protraction with Remotely Controlled Combat Vehicles",description:"This paper examines drone warfare from an ethical perspective",section:"Posts",handler:()=>{window.location.href="/blog/2024/responspibility_protraction/"}},{id:"post-using-llms-in-syntactic-research",title:"Using LLMs in Syntactic Research",description:"Unpublished paper exploring how to use LLMs for empirical studies of syntax",section:"Posts",handler:()=>{window.location.href="/blog/2024/llms_syntactic_research/"}},{id:"post-was-ist-argument-mining",title:"Was ist Argument Mining?",description:"Eine Einf\xfchrung zum Forschungsfeld des Argument Minings",section:"Posts",handler:()=>{window.location.href="/blog/2024/argument_mining/"}},{id:"post-questioning-recursion-as-a-distinctive-feature-of-human-language",title:"Questioning Recursion as a Distinctive Feature of Human Language",description:"Putting human language into perspective with animal linguistics",section:"Posts",handler:()=>{window.location.href="/blog/2024/recursive_language/"}},{id:"post-a-deep-dive-into-transformer-models-like-gpt-and-bert-english-version",title:"A deep dive into Transformer models like GPT and BERT (English version)",description:"From neural networks to GPT and BERT, a contextualized explanation of the Transformer architecture",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer_en/"}},{id:"post-transformer-modelle-wie-gpt-und-bert-im-detail-german-version",title:"Transformer-Modelle wie GPT und BERT im Detail (German version)",description:"Vom neuronalen Netz bis zu GPT und BERT, eine kontextualisierte Erkl\xe4rung der Transformer-Architektur",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"news-happy-to-discuss-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-responsibility-and-ai-title-why-quot-responsible-ai-quot-is-a-misnomer-abstract-the-meaning-of-responsibility-is-closely-attached-to-the-idea-of-quot-responding-quot-in-a-justified-sense-for-one-39-s-deeds-for-that-matter-responsibility-is-also-often-interpreted-as-quot-answerability-quot-the-same-holds-for-the-german-quot-verantwortung-quot-and-the-correlated-idea-of-quot-rede-und-antwort-stehen-quot-however-current-ai-cannot-be-responsible-for-different-reasons-any-existing-ai-system-up-to-date-lacks-the-capacity-to-reasonably-justify-itself-another-fundamental-aspect-of-taking-responsibility-is-also-to-accept-consequences-for-certain-actions-such-as-punishments-for-unlawful-behavior-however-a-machine-learning-system-cannot-be-punished-in-any-meaningful-way-in-short-ai-is-not-responsible-responsible-ai-is-a-misnomer-but-who-is-responsible-and-how-can-responsibility-with-ai-look-like",title:"Happy to discuss at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) responsibility and AI. \\...",description:"",section:"News"},{id:"news-in-my-45-minutes-presentation-i-discussed-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-llms-and-the-possibility-of-lying-title-when-moral-rules-reach-their-limits-should-llms-lie-abstract-one-of-the-most-famous-ideas-in-philosophy-is-immanuel-kant-39-s-categorical-imperative-quot-act-only-according-to-that-maxim-whereby-you-can-at-the-same-time-will-that-it-should-become-a-universal-law-quot-this-imperative-gives-us-a-concrete-solution-as-to-how-to-act-morally-it-allows-us-to-define-ethical-rules-from-non-ethical-ones-at-the-same-time-kant-is-also-known-for-proposing-that-to-not-lie-should-be-such-a-binding-moral-rule-however-we-encounter-many-situations-where-humans-lie-and-also-some-where-lying-appears-even-ethically-licensed-if-so-following-a-rule-such-as-to-not-lie-does-not-seem-to-be-always-what-we-want-from-an-ethical-point-of-view-if-we-as-humans-allow-degrees-of-variabilities-to-truth-telling-what-does-that-mean-if-we-try-to-quot-teach-quot-an-llm-to-only-produce-true-and-factual-content",title:"In my 45 minutes presentation, I discussed at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154)...",description:"",section:"News"},{id:"news-i-spoke-again-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-about-agency-and-ai-title-agentic-ai-what-can-philosophy-teach-us-about-agency-abstract-agentic-ai-such-as-autogen-crew-ai-and-langgraph-has-recently-garnered-significant-attention-stanford-39-s-andrew-ng-renowned-for-his-neural-network-teachings-on-coursera-has-also-lauded-the-use-of-multi-agent-design-patterns-with-llms-in-the-batch-the-concept-is-straightforward-multiple-llm-powered-agents-collaborate-to-solve-problems-by-incorporating-local-memories-and-internal-reflection-capabilities-the-resulting-system-of-agents-can-be-both-powerful-and-complex-however-as-critical-ai-philosophers-we-must-ask-ourselves-do-these-agents-truly-possess-agency-that-is-the-ability-to-take-action-to-explore-the-potential-of-non-human-agents-this-talk-will-introduce-fundamental-definitions-and-conceptual-tools-to-discuss-agency-in-the-context-of-llm-agents-it-appears-that-agency-is-a-continuous-attribute-rather-than-a-categorical-one",title:"I spoke again at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) about agency and AI....",description:"",section:"News"},{id:"news-i-39-m-presenting-a-few-machine-learning-fundamentals-for-understanding-llms-at-rewe-digital-https-www-rewe-digital-com",title:"I&#39;m presenting a few machine learning fundamentals for understanding LLMs at [Rewe Digital](https://www.rewe-digital.com/)....",description:"",section:"News"},{id:"news-my-next-talk-at-deutsche-telekom-39-s-ai-competence-center-https-www-telekom-com-en-company-digital-responsibility-details-artificial-intelligence-at-deutsche-telekom-1055154-is-scheduled-for-september-9-at-13-00-via-microsoft-teams-please-contact-me-for-the-link-title-can-we-trust-ai-abstract-trustworthiness-in-the-ai-scene-is-often-grounded-in-the-notion-of-having-a-system-that-produces-reliable-and-factually-correct-outputs-but-is-believing-that-an-output-is-true-equal-to-quot-trusting-quot-the-system-that-created-it-human-trust-is-a-complex-philosophical-and-psychological-notion-trust-may-be-a-state-of-belief-as-well-as-a-form-of-social-binding-for-example-trust-between-humans-often-implies-a-certain-degree-of-mutual-vulnerability-taking-this-into-account-is-it-even-possible-to-trust-in-machines",title:"My next talk at [Deutsche Telekom&#39;s AI Competence Center](https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-at-deutsche-telekom-1055154) is scheduled for September...",description:"",section:"News"},{id:"news-i-39-m-attending-the-conference-on-the-ethics-of-conversational-agents-amp-generative-ai-hosted-by-the-munich-center-for-machine-learning-https-mcml-ai",title:"\ud83d\udc40 I&#39;m attending the Conference on the Ethics of Conversational Agents &amp; Generative...",description:"",section:"News"},{id:"news-my-latest-talk-on-trust-in-ai-is-now-online-https-www-youtube-com-watch-v-phndlgwnlsu-https-www-youtube-com-watch-v-phndlgwnlsu",title:"My latest talk on Trust in AI is now online: [https://www.youtube.com/watch?v=PhNdlgwNlsU](https://www.youtube.com/watch?v=PhNdlgwNlsU) \u2728\ud83c\udfa5",description:"",section:"News"},{id:"news-back-at-rewe-digital-https-www-rewe-digital-com-introducing-the-fundamentals-of-language-models-to-a-group-of-managers",title:"Back at [Rewe Digital](https://www.rewe-digital.com/), introducing the fundamentals of language models to a group...",description:"",section:"News"},{id:"news-we-re-launching-www-dailogues-ai-https-www-dailogues-ai-our-discursive-space-around-ai",title:"We\u2019re launching [www.dailogues.ai](https://www.dailogues.ai) \ud83d\ude80, our discursive space around AI!",description:"",section:"News"},{id:"news-podcast-episode-with-me-at-dranbleiben-andre-cramer-and-i-discuss-alternative-ai-utopias-link-to-episode-https-dranbleiben-letscast-fm-episode-3-ueber-alternative-ki-utopien-https-dranbleiben-letscast-fm-episode-3-ueber-alternative-ki-utopien",title:"\ud83c\udfa7 Podcast episode with me at #DRANBLEIBEN. Andre Cramer and I discuss alternative...",description:"",section:"News"},{id:"news-\ufe0f-i-delivered-a-keynote-at-rewe-group-where-i-introduced-the-concept-of-agentic-ai-my-talk-covered-the-core-foundations-of-ai-agents-reflection-tool-use-planning-and-multi-agent-collaboration-along-with-practical-applications-such-as-service-bot-orchestration-web-pilots-and-mcp-servers",title:"\ud83c\udf99\ufe0f I delivered a keynote at REWE Group, where I introduced the concept...",description:"",section:"News"},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/omseeth","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/oseeth","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/maxseeth.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>