<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://omseeth.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://omseeth.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-16T09:48:49+00:00</updated><id>https://omseeth.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching</title><link href="https://omseeth.github.io/blog/2025/MCoT_sketching/" rel="alternate" type="text/html" title="Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching"/><published>2025-08-14T10:00:00+00:00</published><updated>2025-08-14T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2025/MCoT_sketching</id><content type="html" xml:base="https://omseeth.github.io/blog/2025/MCoT_sketching/"><![CDATA[<p><strong>Abstract</strong> This article explores adding sketching to Multimodal Chain-of-Thought (MCoT) reasoning to enhance AI capabilities. It reviews current methods, identifies key gaps such as the lack of sketch-rationale datasets, and proposes advancing the field through targeted data collection, unified multimodal models, and reinforcement learning. Applications span education, interactive agents, and embodied AI. Ethical considerations include mitigating cultural bias, and visual misrepresentation in generated sketches.</p> <h2 id="1-introduction">1 Introduction</h2> <p>Drawing and sketching are cognitive tools that humans use not only to express and communicate thoughts, but also to generate new ones [6]. For this matter, we would like to equip any intelligent system with the same ability to improve and help it communicate its reasoning. First steps in this direction have been proposed within the field of Multimodal Chain-of-Thought (MCoT) where reasoning steps are enriched with data from different modalities, such as visuals. Therefore, future research on sketching should advance the design of MCoT reasoning strategies. Improving Multimodal Large Language Models (MLLMs) that perform such cross-modal reasoning is also relevant.</p> <p>The present text outlines the motivation why sketching should be incorporated into CoT in <strong>Section 2</strong>. Several related works that use images and even sketches to aid models in reasoning are introduced in <strong>Section 3</strong>. Finally, <strong>Section 4</strong> proposes directions for future research: a new dataset combining sketches with reasoning chains, advancements of unified MLLMs for MCoT, in particular with diffusion models, as well as the usage of reinforcement learning for existing MCoT approaches, such as rewards with Reinforcement Learning with Verifiable Rewards (RLVR) [9] and Group Relative Policy Optimization (GRPO) [28], both for textual and visual reasoning.</p> <h2 id="2-motivation-to-incorporate-drawing-capabilities-into-ai">2 Motivation to incorporate drawing capabilities into AI</h2> <p>Humans express and communicate ideas visually through drawing and sketching, which is a quick and loose form of drawing. Drawing is a representation of thought, but also an activity that can support ongoing cognition [6]. Drawing and sketching precede writing: The first documented drawings date back as far as 64,000 years [10]. For that reason, Fan et al. [6] argue that drawing is one of the most enduring and versatile cognitive tools from which humans have benefited.</p> <p>One explanation for the power of drawing and sketching can be derived from cognitive enhancement and offloading strategies. According to Morrison and Richmond [21], technologies are used as external memories, facilitating other tasks by freeing up memory. Similarly, Osiurak et al. [24] show that tools such as maps can extend human’s cognitive abilities.</p> <p>Given the relevance of drawing and sketching for human thought, expression, and communication, we would want to equip any AI with the capability to also use this tool to advance and share its own ideas. Sketching can not only be a window into how AI models process information, but it is fair to assume that it can also support their reasoning.</p> <p>Reasoning in large language models (LLMs) has been greatly improved with in-context learning (ICL) [20] and Chain-of-Thought (CoT) techniques [23, 34]. ICL helps models with additional information added to the input to find appropriate responses for a given task. With CoT, the contextual information is specifically extended by a simulation of human reasoning steps, where a task is divided into subtasks for which intermediate solutions are given so that the model can derive its final answer from them. This can be achieved by eliciting reasoning through prompting, as with ’think step-by-step’ prompts (Zero-Shot-CoT [15]), or by providing the model with an explicit reasoning demonstration (also called a rationale) for a given problem (Few-Shot-CoT [34]).</p> <p>CoT has been extended with multimodal information [32, 33] where models receive more than text to guide them toward a correct answer. This information can consist of visual, auditory, or spatio-temporal data. Sketches would be additional visual information. They could also help models to offload complex tasks and retain intermediate memories, for example, of subtasks. Therefore, an implementation of the capability to sketch in order to enhance models’ reasoning abilities should expand existing research in MCoT. A detailed account of MCoT is given in <strong>Appendix A</strong>.</p> <h2 id="3-related-work">3 Related work</h2> <p>Several recent approaches explore MCoT reasoning, though most do not fully integrate sketch generation into the reasoning process.</p> <p>Zhang et al. [42] propose a two-stage framework for multiple-choice reasoning for text and image inputs where a FLAN-AlpacaBase model [30, 44] first produces a rationale, then derives the answer. Fusing text and image features from the input improves performance, but the system cannot generate new visual content. This limits applicability to reasoning scenarios that benefit from active visual exploration, such as diagram construction in geometry or mechanical design tasks.</p> <p>Meng et al. [19] extend CoT by having an LLM produce symbolic sketch-like diagrams (e.g., with SVG), rendered into images and re-encoded for reasoning. Their ’think image by image’ approach helps, for example, with geometric tasks. However, this gain comes at the cost of operational complexity: the pipeline depends on separate LLMs, rendering engines, and encoders, creating latency and integration challenges. Unified MLLMs avoid such fragmentation and may better support generalization by learning a shared latent space for both text and sketches.</p> <p>In contrast to the previous two approaches, Liao et al. [16] fine-tune unified MLLMs (SEED-LLaMA [7] and SEED-X [8]) on their ImageGen-CoT dataset. Reasoning steps of their models precede image generation. Test-Time Scaling is applied to select better outputs. While they demonstrate high-quality image generation, their evaluation focuses on aesthetics and relevance rather than measurable reasoning improvement. For reasoning-centric applications, visual fidelity without explicit reasoning gains may be insufficient.</p> <p>Hu et al. [11] and Vinker et al. [31] develop agentic strategies (Sketchpad, Sketchagent) where models like GPT-4o [13] or Claude3.5-Sonnet [3] can decide to produce or modify sketches during problem-solving by leveraging external vision models, Python or a domain-specific language (DSL) for sketches. Models with Sketchpad iterate over a ’thought’, ’action’ (to inject sketches), and ’observation’ pattern. With this approach, Hu et al. [11] show that allowing models to decide to insert sketches during reasoning leads to notable performance gains. However, the framework relies on external vision models to rather enhance or dissect images and a Python sketch representation, which may not capture the nuances of freehand or abstract sketches common in human reasoning.</p> <p>A truly multimodal approach for sketches would not use Python or DSLs to ’implicitly’ generate figures that the model ingests as textual input. However, few multimodal datasets that combine visuals with rationales exist. While QuickDraw [14] provides scale and diversity in sketch data, its lack of accompanying rationales prevents multimodal alignment learning. ScienceQA [18] and ImageGen-CoT [16] offer strong rationale-image pairs, but the absence of sketches means they primarily serve full-image reasoning rather than schematic reasoning. This gap suggests that the field currently lacks a dataset that balances sketch simplicity with reasoning, a pairing that could uniquely advance MCoT.</p> <p>Overall, existing MCoT work shows that visual information, including sketches, can aid reasoning. However, limitations remain: most systems either consume but do not create sketches, focus on image quality rather than reasoning improvement, or require orchestration of multiple models instead of unified generation. Furthermore, appropriate datasets with sketches in combination with rationales are lacking.</p> <h2 id="4-future-research-for-mcot-with-sketching">4 Future research for MCoT with sketching</h2> <p>Given the power of visual information for reasoning tasks, as shown by [42, 19, 16, 11], some of the shortcomings of existing MCoT approaches can be addressed to better incorporate sketching in future research.</p> <h3 id="41-creating-a-new-mcot-sketch-dataset">4.1 Creating a new MCoT sketch dataset</h3> <p>To facilitate the training of MLLMs, the lack of an appropriate dataset with sketching and rationales is a limitation.</p> <p>Sketch data should be gathered and grouped within different categories, depending on the downstream task (consider Figure 1). In experimental studies with humans, Huey et al. [12] point out that drawings differ according to their intended goal: visual explanations by the participants emphasized moving and interactive parts, while their visual depictions focused on salient features. Hu et al. [11] show that adding auxiliary lines to geometric figures helps multimodal models such as GPT-4o to infer correct answers about these figures. Fan et al. [6] highlight that not all drawings are faithful depictions, but can also be abstractions whose meanings are conveyed by cultural conventions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MCoT/sketches-480.webp 480w,/assets/img/MCoT/sketches-800.webp 800w,/assets/img/MCoT/sketches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/MCoT/sketches.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> Different types of sketches and drawings: (a) depicts a geometric form that has an auxiliary line, (b) emphasizes moving parts of a machine, (c) depicts the same machine in more detail, (d) represents figures from tetris whose next moves are indicated with arrows, (e) is a conventional sketch of a heart that does not resemble actual human hearts.</p> <p>To integrate sketches into a CoT, training data should not only consist of images of drawings and sketches, but combine these with textual rationales. This would enable multimodal alignment between visual and linguistic reasoning steps. A typical template for this data could consist of instruction <em>I</em>, query <em>Q</em>, rationale <em>R</em>, and answer <em>A</em> where we could further divide <em>R</em> into ’thought’, ’sketch’, and ’observation’ with respective special tokens to guide the model, loosely following Hu et al. [11]. An example template is given in <strong>Appendix B</strong>. Since ScienceQA and ImageGen-CoT already pair images with rationales, they could be extended with sketches to strengthen visual-textual alignment for their tasks.</p> <h3 id="42-advancing-mcot-with-unified-mllms">4.2 Advancing MCoT with unified MLLMs</h3> <p>To avoid multi-model orchestration and to leverage potential transfer-learning effects, further advancing reasoning of MLLMs with sketches is a promising direction. However, there exist only a few MLLMs [39, 43, 42, 29] that can potentially handle sketch-to-text as well as text-to-sketch tasks within a unified architecture (consider Figure 2). The majority of current approaches such as Sketchpad pair VLMs such as Flamingo [2], PaLM-E [5], LLAVA [17], GPT-4o [13], or Claude3-Opus and Claude3.5-Sonnet [3] with text-to-image models.</p> <p>Unified MLLMs can be divided into autoregressive (AR) and diffusion-based MLLMs. For example, CM3Leon [39] from Meta is a Transfomer-based AR decoder that can generate both text and images. It is built on the CM3 model [1]. CM3Leon has been trained on text-guided image editing, image- to-image grounding tasks where visual features can be derived from images, and text-to-image generations.</p> <p>Swerdlow et al. [29] introduce a unified multimodal discrete diffusion model (UniDisc). While the model’s architecture consists of a Transformer (bidirectional) decoder, its training goal is not to auto-regressively predict the next tokens in a sequential manner (e.g., left to right for text or top to bottom for image patch rasters), but to predict the distribution of tokens via a denoising process that allows parallel predictions as well as later refinements. The training of UniDisc is realized with a denoising process of corrupted inputs (masking). In contrast to continuous diffusion models, Swerdlow et al. [29] use discrete noising and denoising for both images and texts. Swerdlow et al. [29] show that UniDisc outperforms the same architecture without a diffusion objective with respect to image and text classification tasks. The model is also capable of inpainting and infilling missing parts of an input, which no AR model can do. However, these performance gains come at a cost: UniDisc requires 13.2 times longer than its AR counterpart to reach equivalent loss levels [29].</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MCoT/MCoT-480.webp 480w,/assets/img/MCoT/MCoT-800.webp 800w,/assets/img/MCoT/MCoT-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/MCoT/MCoT.png" class="img-fluid mx-auto d-block" width="65%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 2:</strong> MCoT involving sketches with a Multimodal Large Language Model (MLLM). Black arrows represent sequential auto-regressive processing, while blue arrows illustrate the bidirectionality of diffusion models. The model’s reasoning is guided by special tokens, such as <think>.</think></p> <p>Models like UniDisc provide an interesting model class for MCoT. While current diffusion language models (DLMs) might not rival AR LLMs due to training inefficiencies [29] or speed [38], the strength of multimodal DLMs in handling and generating multimodal data – as shown by Swerdlow et al. [29] – warrants further research. Their ability to inpaint and infill would be particularly helpful for amending visualizations, which is a core aspect of explanatory sketching. Research in this direction could be informed by Diffusion-of-Thought (DoT) proposed by Ye et al. [37], who fine-tune a DLM for CoT. However, diffusion models require a fixed output size. This is a challenge that needs to be addressed to allow versatile reasoning over different tasks.</p> <h3 id="43-improving-mcot-with-reinforcement-learning-rl-and-test-time-scaling">4.3 Improving MCoT with Reinforcement Learning (RL) and Test-Time Scaling</h3> <p>Existing work on MCoT [42, 19, 16] has so far relied on supervised fine-tuning (SFT). Other work in reasoning has shown that RL leads to improvements [9, 27, 43]. Therefore, MCoT should be advanced with Direct Preference Optimization (DPO) [26], Reinforcement Learning with Verifiable Rewards (RLVR) [9] and Group Relative Policy Optimization (GRPO) [28] strategies. One straight-forward application would be to use RLVR with GRPO, following Deepseek’s R1 [9], to reward accuracy (\(R_{acc}\)) and format (\(R_{format}\)) for rationales and answers based on generated sketches.</p> <p>An appropriate reward for the generation of sketches could leverage AR-GRPO for autoregressive MLLMs [40]. AR-GRPO realizes rewards for the generation of images with a multi-faceted reward function that ensures (a) consistency with the textual input condition through CLIP [25] and Human Preference Score v2 [35], (b) image quality with MANIQA [36], and (c) a further realism reward through a VLM, such as Qwen2.5-VL-3B-Instruct [4]. This function is used with GRPO to improve the quality of generated images. Since the proposed rewards by Yuan et al. [40] focus on overall quality, a specific reward should be conceived for sketches. For example, a sketch can consist of a hierarchy of strokes whose meaning can be of different importance. It would be interesting to incorporate this somehow into the reward: Should sketches with a limited amount of strokes be prioritized?</p> <p>In the wake of Liao et al. [16], existing MCoT could be further improved with Test-Time Scaling methods, sampling more CoTs and sketches to select the best candidates with an appropriate scoring method. This approach could also be used with agentic frameworks that pair VLMs with image generators and would not require any additional training of the models.</p> <p>Beyond standard accuracy on downstream tasks, evaluation should measure how sketches contribute to the reasoning process. This includes interpretability (e.g., can a human follow the model’s reasoning with a sketch?), task completion time (one of the biggest bottlenecks because image generation requires many tokens), error localization, and robustness under noisy or incomplete inputs. Additionally, user studies could assess subjective clarity and helpfulness of generated sketches.</p> <h2 id="impact">Impact</h2> <p>MLLMs with sketching would have an impact on AI in different domains. For example, agentic systems such as Auto-GUI [41] that interact with graphical user interfaces or websites could be enhanced by providing them with additional visual information with sketches. Similarly, embodied AI systems, such as EmbodiedGPT [22] whose backbone uses a combination of vision and language models that help navigate the real world, could reason about their surroundings using sketches. MLLMs for STEM education could also benefit from the ability to make their reasoning more transparent with additional drawings as proposed in Meng et al. [19]. In sum, sketching would help all reasoning models not only to enhance their thoughts, but also communicate them with more than one modality.</p> <p>As with language, sketches are not neutral representations. The ability of AI systems to generate and reason with sketches introduces risks of cultural bias, visual misrepresentation, and domain-specific inaccuracies. For example, the “heart” symbol in Figure 1(e) is globally recognized in popular culture but anatomically incorrect; in medical education, reasoning over such a schematic could reinforce misconceptions. Similar issues may arise if models default to culturally specific diagrammatic conventions, omit critical features due to dataset biases, or overgeneralize from training examples.</p> <p>Ethical safeguards should address the entire MCoT-with-sketching workflow. Dataset curation must ensure diversity of styles, cultural perspectives, and schematic conventions. Annotation guidelines should clarify the intended use and accuracy requirements of sketches. Model evaluation should include bias detection for visual outputs, alongside interpretability checks so users can trace how a sketch influenced reasoning.</p> <h2 id="apendix-a-mcot-foundations">Apendix A: MCoT foundations</h2> <p>Following Wang et al. [33], we can define prompt, instruction, query, answer, and rationale with \(P\) , \(I\), \(Q\), \(A\), and \(R\), which are all token sequences. A Chain-of-Thought (CoT) would be:</p> <p>\begin{equation} P_{CoT} = {I, (x_1, e_1, y_1), …, (x_n, e_n, y_n)} \end{equation}</p> <p>where \(x_i \in Q\) and \(y_i \in A\) are questions with corresponding answers and \(e_i \in R\) is an example rationale. The joint probability of generating an answer A and a rationale R given the prompt \(P_{CoT}\) and a query \(Q\) would be [33]:</p> <p>\begin{equation} p(A, R |P_{CoT}, Q) = p(R |P_{CoT}, Q) \cdot p(A |P_{CoT}, Q, R) \end{equation}</p> <p>where the model should output rationale \(R\) with the tokens \(r_1, ..., r_i\) before arriving at the answer \(A\) consisting of the tokens \(a_1, ..., a_i\). The goal in training a reasoning model \(F\) is to jointly maximize the likelihood of equation (2).</p> <p>Finally, all components \(P\), \(Q\), \(A\), and \(R\) can be enriched with multimodal information \(\mathcal{M}\). For example with MCoT, a rationale \(R\) should handle \(\mathcal{M}\) input and generate multimodal information (e.g., a sketch) as well as text \(T\), that is, \(R\in\{M, M\oplus T\}\) [33].</p> <h2 id="appendix-b-mcot-template">Appendix B: MCoT template</h2> <p><code class="language-plaintext highlighter-rouge"> { "instruction": "Find proofs for geometry problems.", "query": "Prove the angles of ABC provided in the attached image sum to 180. &lt;image&gt; VT_011 VT_115 VT_563 VT_101 ... VT_909 &lt;/image&gt;", "rationale": "&lt;think&gt; I need to figure out how ABC are related in the image. The image shows a triangle. I need to prove that the angles of the triangle sum to 180. To find an answer, I draw a triangle: Let's call it ABC. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_002 ... VT_778 &lt;/sketch&gt; I extend the sides from A to B, from A to C, and from B to C. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_001 ... VT_708 &lt;/sketch&gt; I draw a line parallel to AB through point C. &lt;sketch&gt; VT_420 VT_105 VT_983 VT_001 ... VT_718 &lt;/sketch&gt; &lt;observe&gt; The angles at point C created by the parallel line correspond to the interior angles at points A and B. When I add those angles up, they form a straight line at point C, which measures 180. Since those angles correspond exactly to the three interior angles of the triangle, the sum of the interior angles is 180. &lt;/observe&gt; This proof follows from the alternate interior angles theorem. &lt;/think&gt;", "answer": "The alternate interior angles theorem shows that all angles at point C created by the parallel line sum to 180. They further correspond to the interior angles at points A and B. Therefore, the angles of ABC provided in the attached image sum to 180." } </code></p> <p>MCoT template with instruction \(I\), query \(Q\), rationale \(R\), and answer \(A\) where \(R\) is further divided into “thought”, “sketch”, and “observation” with respective special tokens to guide the model. VT_n tokens correspond to image tokens.</p> <h2 id="bibliography">Bibliography</h2> <p>[1] Aghajanyan, A., Huang, P.-Y. B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., and Zettlemoyer, L. (2022). Cm3: A causal masked multimodal model of the internet. ArXiv, abs/2201.07520.</p> <p>[2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198.</p> <p>[3] Anthropic (2024). The claude 3 model family: Opus, sonnet, haiku.</p> <p>[4] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. (2025). Qwen2.5-vl technical report. ArXiv, abs/2502.13923.</p> <p>[5] Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q. H., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. R. (2023). Palm-e: An embodied multimodal language model. In International Conference on Machine Learning.</p> <p>[6] Fan, J. E., Bainbridge, W. A., Chamberlain, R., and Wammes, J. D. (2023). Drawing as a versatile cognitive tool. Nature Reviews Psychology, 2(9).</p> <p>[7] Ge, Y., Zhao, S., Zeng, Z., Ge, Y., Li, C., Wang, X., and Shan, Y. (2023). Making llama see and draw with seed tokenizer. ArXiv, abs/2310.01218.</p> <p>[8] Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., Li, C., Ding, X., and Shan, Y. (2024). Seed-x: Multimodal models with unified multi-granularity comprehension and generation. ArXiv, abs/2404.14396.</p> <p>[9] Guo, D., Yang, D., Zhang, H., Song, J.-M., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B.-L., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S.-K., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Xiao, W., An, W., Liu, X., Wang, X., aokang Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.-C., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y.-J., He, Y., Xiong, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., guo Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948.</p> <p>[10] Hoffmann, D. L., Standish, C. D., García-Diez, M., Pettitt, P. B., Milton, J. A., Zilhão, J., Alcolea-González, J. J., Cantalejo-Duarte, P., Collado, H., de Balbín, R., Lorblanchet, M., Ramos- Muñoz, J., Weniger, G.-C., and Pike, A. W. G. (2018). U-th dating of carbonate crusts reveals neandertal origin of iberian cave art. Science, 359(6378):912–915.</p> <p>[11] Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L. S., Smith, N. A., and Krishna, R. (2024). Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. ArXiv, abs/2406.09403.</p> <p>[12] Huey, H., Lu, X., Walker, C. M., and Fan, J. (2023). Visual explanations prioritize functional properties at the expense of visual fidelity. Cognition, 236.</p> <p>[13] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., Mkadry, A., Baker-Whitcomb, A., Beutel, A., Borzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A., Paino, A., Renzin, A., Passos, A., Kirillov, A., Christakis, A., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A., Crookes, A., Tootoochian, A., Tootoonchian, A., Kumar, A., Vallone, A., Karpathy, A., Braunstein, A., Cann, A., Codispoti, A., Galu, A., Kondrich, A., Tulloch, A., drey Mishchenko, A., Baek, A., Jiang, A., toine Pelisse, A., Woodford, A., Gosalia, A., Dhar, A., Pantuliano, A., Nayak, A., Oliver, A., Zoph, B., Ghorbani, B., Leimberger, B., Rossen, B., Sokolowsky, B., Wang, B., Zweig, B., Hoover, B., Samic, B., McGrew, B., Spero, B., Giertler, B., Cheng, B., Lightcap, B., Walkin, B., Quinn, B., Guarraci, B., Hsu, B., Kellogg, B., Eastman, B., Lugaresi, C., Wainwright, C. L., Bassin, C., Hudson, C., Chu, C., Nelson, C., Li, C., Shern, C. J., Conger, C., Barette, C., Voss, C., Ding, C., Lu, C., Zhang, C., Beaumont, C., Hallacy, C., Koch, C., Gibson, C., Kim, C., Choi, C., McLeavey, C., Hesse, C., Fischer, C., Winter, C., Czarnecki, C., Jarvis, C., Wei, C., Koumouzelis, C., Sherburn, D., Kappler, D., Levin, D., Levy, D., Carr, D., Farhi, D., Mély, D., Robinson, D., Sasaki, D., Jin, D., Valladares, D., Tsipras, D., Li, D., Nguyen, P. D., Findlay, D., Oiwoh, E., Wong, E., Asdar, E., Proehl, E., Yang, E., Antonow, E., Kramer, E., Peterson, E., Sigler, E., Wallace, E., Brevdo, E., Mays, E., Khorasani, F., Such, F. P., Raso, F., Zhang, F., von Lohmann, F., Sulit, F., Goh, G., Oden, G., Salmon, G., Starace, G., Brockman, G., Salman, H., Bao, H.-B., Hu, H., Wong, H., Wang, H., Schmidt, H., Whitney, H., woo Jun, H., Kirchner, H., de Oliveira Pinto, H. P., Ren, ˙ H., Chang, H., Chung, H. W., Kivlichan, I., O’Connell, I., Osband, I., Silber, I., Sohl, I., Ibrahim Cihangir Okuyucu, Lan, I., Kostrikov, I., Sutskever, I., Kanitscheider, I., Gulrajani, I., Coxon, J., Menick, J., Pachocki, J. W., Aung, J., Betker, J., Crooks, J., Lennon, J., Kiros, J. R., Leike, J., Park, J., Kwon, J., Phang, J., Teplitz, J., Wei, J., Wolfe, J., Chen, J., Harris, J., Varavva, J., Lee, J. G., Shieh, J., Lin, J., Yu, J., Weng, J., Tang, J., Yu, J., Jang, J., Candela, J. Q., Beutler, J., Landers, J., Parish, J., Heidecke, J., Schulman, J., Lachman, J., McKay, J., Uesato, J., Ward, J., Kim, J. W., Huizinga, J., Sitkin, J., Kraaijeveld, J., Gross, J., Kaplan, J., Snyder, J., Achiam, J., Jiao, J., Lee, J., Zhuang, J., Harriman, J., Fricke, K., Hayashi, K., Singhal, K., Shi, K., Karthik, K., Wood, K., Rimbach, K., Hsu, K., Nguyen, K., Gu-Lemberg, K., Button, K., Liu, K., Howe, K., Muthukumar, K., Luther, K., Ahmad, L., Kai, L., Itow, L., Workman, L., Pathak, L., Chen, L., Jing, L., Guy, L., Fedus, L., Zhou, L., Mamitsuka, L., Weng, L., McCallum, L., Held, L., Long, O., Feuvrier, L., Zhang, L., Kondraciuk, L., Kaiser, L., Hewitt, L., Metz, L., Doshi, L., Aflak, M., Simens, M., laine Boyd, M., Thompson, M., Dukhan, M., Chen, M., Gray, M., Hudnall, M., Zhang, M., Aljubeh, M., teusz Litwin, M., Zeng, M., Johnson, M., Shetty, M., Gupta, M., Shah, M., Yatbaz, M. A., Yang, M., Zhong, M., Glaese, M., Chen, M., Janner, M., Lampe, M., Petrov, M., Wu, M., Wang, M., Fradin, M., Pokrass, M., Castro, M., Castro, M., Pavlov, M., Brundage, M., Wang, M., Khan, M., Murati, M., Bavarian, M., Lin, M., Yesildal, M., Soto, N., Gimelshein, N., talie Cone, N., Staudacher, N., Summers, N., LaFontaine, N., Chowdhury, N., Ryder, N., Stathas, N., Turley, N., Tezak, N. A., Felix, N., Kudige, N., Keskar, N. S., Deutsch, N., Bundick, N., Puckett, N., Nachum, O., Okelola, O., Boiko, O., Murk, O., Jaffe, O., Watkins, O., Godement, O., Campbell-Moore, O., Chao, P., McMillan, P., Belov, P., Su, P., Bak, P., Bakkum, P., Deng, P., Dolan, P., Hoeschele, P., Welinder, P., Tillet, P., Pronin, P., Tillet, P., Dhariwal, P., ing Yuan, Q., Dias, R., Lim, R., Arora, R., Troll, R., Lin, R., Lopes, R. G., Puri, R., Miyara, R., Leike, R. H., Gaubert, R., Zamani, R., Wang, R., Donnelly, R., Honsby, R., Smith, R., Sahai, R., Ramchandani, R., Huet, R., Carmichael, R., Zellers, R., Chen, R., Chen, R., Nigmatullin, R. R., Cheu, R., Jain, S., Altman, S., Schoenholz, S., Toizer, S., Miserendino, S., Agarwal, S., Culver, S., Ethersmith, S., Gray, S., Grove, S., Metzger, S., Hermani, S., Jain, S., Zhao, S., Wu, S., Jomoto, S., Wu, S., Xia, S., Phene, S., Papay, S., Narayanan, S., Coffey, S., Lee, S., Hall, S., Balaji, S., Broda, T., Stramer, T., Xu, T., Gogineni, T., Christianson, T., Sanders, T., Patwardhan, T., Cunninghman, T., Degry, T., Dimson, T., Raoux, T., Shadwell, T., Zheng, T., Underwood, T., Markov, T., Sherbakov, T., Rubin, T., Stasi, T., Kaftan, T., Heywood, T., Peterson, T., Walters, T., Eloundou, T., Qi, V., Moeller, V., Monaco, V., Kuo, V., Fomenko, V., Chang, W., Zheng, W., Zhou, W., Manassra, W., Sheu, W., Zaremba, W., Patil, Y., Qian, Y., Kim, Y., Cheng, Y., Zhang, Y., He, Y., Zhang, Y., Jin, Y., Dai, Y., and Malkov, Y. (2024). Gpt-4o system card. ArXiv, abs/2410.21276.</p> <p>[14] Jonas, J., Henry, R., Takashi, K., Jongmin, K., and Nick, F.-G. (2016). The quick, draw! - a.i. experiment.</p> <p>[15] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p> <p>[16] Liao, J., Yang, Z., Li, L., Li, D., Lin, K. Q., Cheng, Y., and Wang, L. (2025). Imagegen-cot: Enhancing text-to-image in-context learning with chain-of-thought reasoning. ArXiv, abs/2503.19312.</p> <p>[17] Liu, H., Li, C., Wu, Q., and Lee, Y. J. (2023). Visual instruction tuning. ArXiv, abs/2304.08485.</p> <p>[18] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. (2022). Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).</p> <p>[19] Meng, F., Yang, H., Wang, Y., and Zhang, M. (2023). Chain of images for intuitively reasoning. ArXiv, abs/2311.09241.</p> <p>[20] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes in-context learning work? ArXiv, abs/2202.12837.</p> <p>[21] Morrison, A. B. and Richmond, L. L. (2020). Offloading items from memory: individual differences in cognitive offloading in a short-term memory task. Cognitive Research: Principles and Implications, 5(1):1.</p> <p>[22] Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., and Luo, P. (2023). Embodiedgpt: Vision-language pre-training via embodied chain of thought. ArXiv, abs/2305.15021.</p> <p>[23] Nye, M., Andreassen, A., Gur-Ari, G., Michalewski, H. W., Austin, J., Bieber, D., Dohan, D. M., Lewkowycz, A., Bosma, M. P., Luan, D., Sutton, C., and Odena, A. (2021). Show your work: Scratchpads for intermediate computation with language models. https://arxiv.org/abs/2112.00114.</p> <p>[24] Osiurak, F., Navarro, J., Reynaud, E., and Thomas, G. (2018). Tools don’t–and won’t–make the man: A cognitive look at the future. Journal of Experimental Psychology: General, 147(5):782– 788.</p> <p>[25] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning.</p> <p>[26] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. ArXiv, abs/2305.18290.</p> <p>[27] Ranaldi, L. and Pucci, G. (2025). Multilingual reasoning via self-training. In North American Chapter of the Association for Computational Linguistics.</p> <p>[28] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J.-M., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300.</p> <p>[29] Swerdlow, A., Prabhudesai, M., Gandhi, S., Pathak, D., and Fragkiadaki, K. (2025). Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853.</p> <p>[30] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Alpaca: A strong, replicable instruction-following model. https://crfm.stanford.edu/2023/03/13/alpaca.html.</p> <p>[31] Vinker, Y., Shaham, T. R., Zheng, K., Zhao, A., Fan, J. E., and Torralba, A. (2024). Sketchagent: Language-driven sequential sketch generation. ArXiv, abs/2411.17673.</p> <p>[32] Wang, Y., Chen, W., Han, X., Lin, X., Zhao, H., Liu, Y., Zhai, B., Yuan, J., You, Q., and Yang, H. (2024). Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning. ArXiv, abs/2401.06805.</p> <p>[33] Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., and Fei, H. (2025). Multimodal chain-of-thought reasoning: A comprehensive survey. ArXiv, abs/2503.12605.</p> <p>[34] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc.</p> <p>[35] Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. (2023). Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. ArXiv, abs/2306.09341.</p> <p>[36] Yang, S., Wu, T., Shi, S., Gong, S., Cao, M., Wang, J., and Yang, Y. (2022). Maniqa: Multi- dimension attention network for no-reference image quality assessment. 2022 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1190–1199.</p> <p>[37] Ye, J., Gong, S., Chen, L., Zheng, L., Gao, J., Shi, H., Wu, C., Jiang, X., Li, Z., Bi, W., and Kong, L. (2024). Diffusion of thought: Chain-of-thought reasoning in diffusion language models. In Neural Information Processing Systems.</p> <p>[38] Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. (2025). Dream 7b.</p> <p>[39] Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O. Y., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., Ross, C., Polyak, A., Howes, R., Sharma, V., Xu, P., Tamoyan, H., Ashual, O., Singer, U., Li, S.-W., Zhang, S., James, R., Ghosh, G., Taigman, Y., Fazel-Zarandi, M., Celikyilmaz, A., Zettlemoyer, L., and Aghajanyan, A. (2023). Scaling autoregressive multi-modal models: Pretraining and instruction tuning. ArXiv, abs/2309.02591.</p> <p>[40] Yuan, S., Liu, Y., Yue, Y., Zhang, J., Zuo, W., Wang, Q., Zhang, F., and Zhou, G. (2025). Ar-grpo: Training autoregressive image generation models via reinforcement learning.</p> <p>[41] Zhang, Z. and Zhang, A. (2023). You only look at screens: Multimodal chain-of-action agents. ArXiv, abs/2309.11436.</p> <p>[42] Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. J. (2023). Multimodal chain-of-thought reasoning in language models. Trans. Mach. Learn. Res., 2024.</p> <p>[43] Zhao, J., Wei, X., and Bo, L. (2025). R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. ArXiv, abs/2503.05379.</p> <p>[44] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685.</p>]]></content><author><name></name></author><category term="MCoT,"/><category term="CoT,"/><category term="MLLM,"/><category term="reasoning,"/><category term="sketching,"/><category term="drawing"/><summary type="html"><![CDATA[This text explores adding sketching to Multimodal Chain-of-Thought (MCoT) reasoning to enhance AI capabilities]]></summary></entry><entry><title type="html">Tracing Early Texts. A Linguistic and Historical Inquiry into Textuality</title><link href="https://omseeth.github.io/blog/2025/Tracing_early_texts/" rel="alternate" type="text/html" title="Tracing Early Texts. A Linguistic and Historical Inquiry into Textuality"/><published>2025-04-26T10:00:00+00:00</published><updated>2025-04-26T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2025/Tracing_early_texts</id><content type="html" xml:base="https://omseeth.github.io/blog/2025/Tracing_early_texts/"><![CDATA[<p><strong>Abstract</strong> This paper tries to identify some of the earliest texts in human history. For this purpose, a historical overview of writing is presented to establish the context in which texts emerged. The subsequent section presents three definitions of text: a minimal definition allowing for various writings to be considered texts if they are cohesive, a second one defining text as unified and coherent composition serving a communicative purpose, and a narrow definition that consists of seven criteria for identifying text: 1) cohesion, 2) coherence, 3) intentionality, 4) acceptability, 5) informativity, 6) situationality, 7) intertextuality. Several critical aspects of these definitions of text, such as the utility of pragmatic factors for identifying texts as such, are discussed shortly. With the three definitions in place, the paper analyzes examples of early text candidates. It concludes that there is no singular answer to the question of the earliest texts in history. Nevertheless, at least one contract from 3100 BC in what is now Iraq is identified as an early piece of writing that can be spoken of as a text.</p> <h2 id="introduction">Introduction</h2> <p>While we can find a trove of literature on the history of writing (Gelb 1952, Gaur 1992, Fischer 2001, Rogers 2005, Schmandt-Besserat 2014), there are surprisingly few to no works explicitly dealing with the history of text.\(^1\) There is no established answer to the question: What were the earliest texts ever? For example, the results from this question being submitted to search engines such as Google or Bing tend to conflate the meaning of writing, text, and literature. Some results show that Sumerian scribes from 3200 BC wrote on clay tablets in cuneiform, most likely ancient shopping receipts (Alex 2019). Another result leads to the Kesh Temple Hymn and the Instructions of Shuruppak from around 2600 BC, which are considered to be an ode and a piece of advice (Andrews 2023).</p> <p>But were these the firsts texts to be properly spoken of? The goal of this paper is neither to identify early writing nor early literature. Instead, I explore which texts were first created by humankind. However, a lot remains open to what we define as text. It is demonstrated throughout the paper that a plurality of definitions of text also determines the answer to the previous question. It is concluded that we cannot settle on one document as the earliest text in history. That said, an administrative piece from present day Iraq dating to 3100 BC suggests that among the earliest texts were contracts.</p> <p>This paper contains the following sections: I approach text by recapitulating the history of writing because it provides us with the context from which texts have arisen. In the following section, I present three definitions of text: 1) the minimal definition that allows for many writings to be texts, as long as they are cohesive, 2) the most common definition that sees texts as a unified and coherent composition of sentences, serving a communicative purpose, 3) a narrow definition by de Beaugrande and Dressler (1981), who establish seven criteria for identifying text as such. Their criteria are the following: 1) cohesion, 2) coherence, 3) intentionality, 4) acceptability, 5) informativity, 6) situationality, 7) intertextuality. A short discussion of some shortcoming of these definitions is presented next. I discuss the length of texts as well as the utility of pragmatic features for identifying texts. Finally, I round up this paper with analyzing two pieces of writing. The first is an inscription on a vessel from 3150 BC, and the second is a clay tablet with a narrative about a slave trade that involved multiple parties from 3100 BC. These writings are among the very first in human history.</p> <h2 id="recapping-the-history-of-writing">Recapping the history of writing</h2> <p>Before looking at the historic development of writing, we need a better understanding of what writing is.\(^2\) To achieve this, it may be advantageous to divide the writing-history community into two distinct camps, each of which will provide us with an understanding of writing: There are 1) those, who have a loose definition of writing (Gaur 1992, Schmandt-Besserat 2014), and 2) those, who have a strict one (Fischer 2001, Rogers 2005). The loose camp sees writing as a means of storing and communicating information. Gaur proclaims: “All writing is information storage.” (Gaur 1992: 15) The strict definitions of writing tie the concept further to “the systematic arrangement of significant vocal sounds” (Fischer 2001: 12). So, the strict understanding of writing sees it as the representation of actual spoken language whereas the loose definition would allow for many representational means to be writing.</p> <p>It is generally agreed by both camps of writing historians that writing has been separately invented in three regions: the Near East, China, and Mesoamerica. However, the earliest traces of writing can be found in Mesopotamia, that is, present-day Iraq, dating back to at least as early as 3200 BC (Schmandt-Bresserat 2014). The invention of writing in this region is also well documented, which is the reason why most scholars focus on the Sumer’s early writing systems, who inhabited Mesopotamia during this period.</p> <p>Schmandt-Bresserat divides the overall development of writing in Mesopotamia into four phases (2014). The first period of storing information began with clay tokens (8000 - 3500 BC). During the following period, these tokens were translated to two-dimensional symbols (3500 - 3000 BC). This change of making information accessible over time and space is being followed by the first systems of phonetic signs (3000 - 1500 BC), which finally led to the alphabet. The alphabet is a very economical way of representing vocal sounds.</p> <p>Among the first examples in the strict sense of writing were inscriptions on seals, and vessels which were found in the remains of tombs. These inscriptions did not deal with any merchandise in terms of numbers, but bore names phonetically written. With the ascendance of names, more writing examples emerged in funerary contexts: Status depicting descriptions of individuals were successively added as well as pleas (Schmandt-Besserats 2014). This process eventually led to documents of religious or royal nature, as the Kesh Temple Hymn from around 2600 BC.</p> <h2 id="three-definitions-of-text">Three definitions of “text”</h2> <p>To identify early writings as texts it is necessary to define what constitutes a text. That is, we need to understand the <em>structure</em> of text. Therefore, I will proceed with three definitions of text: the first of which is mostly text internal, that is, it focuses on features that are materially evident in the text. The following definitions increase in complexity and gradually incorporate text external aspects as well. By text external I mean pragmatic features, such as acceptability within communicative situations. For now, this discussion leaves out further dimensions that should ultimately be also considered, such as the media that have been used for text production and dissemination.</p> <p>The reasoning presented in this paper is guided by a certain philosophical rigor. The definitions offered are intended to serve as conceptual foundations against which specimens of texts will subsequently be compared. Each definition is formulated so that its elements are both necessary and sufficient to identify a piece of writing as a text. Should any definitional condition fail to apply, it is assumed that the writing in question would not qualify as a text under that particular definition. Whether the authors whose definitions are employed here would themselves insist that their criteria must always be applied to determine textuality remains somewhat uncertain, as only Linke and colleagues (1996) have explicitly addressed the intended strictness of their formulations. Accordingly, I invite the reader to regard the following definitions as idealized instruments for the purpose of investigating the earliest texts in human history.</p> <p><strong>1st definition</strong>: Halliday and Hasan define text as follows: “’Text’ is used to refer to any passage, spoken or written, of whatever length, that does form a unified whole” (1976: 1). This definition is being accompanied by an intuitive one. They claim: “We know, as a general rule, whether any specimen of our own language constitutes TEXT or not” (Halliday and Hasan 1976: 1). What matters is that text in its most common form should be distinguished from “a collection of unrelated sentences” (Halliday and Hasan 1976: 1). But that is not to say that any text must consist of at least two sentences. A text is rather “a semantic unit” that can also be found in some proverbs or announcements (Halliday and Hasan 1976: 294). But what constitutes its unity?</p> <p>For Halliday and Hasan (1976), unity in texts is produced through cohesion. Roughly put, cohesion is what connects a set of descriptions, appeals, and/or propositions. Consider their example: “<em>Wash and core six cooking apples. Put them into a fireproof dish.</em>” (Halliday and Hasan 1976: 2) These sentences form a unit because they are interlaced through the anaphoric pronoun “them”. We can only make sense of the latter sentence by linking the pronoun to the apples from the previous instruction. In this manner, the two sentences constitute a cohesive text.</p> <p>For Halliday and Hasan (1976), cohesion is not only produced with referential expressions, but also through further linguistic devices, such as repetition, substitution, ellipsis, conjunction, and lexical cohesion. For example, sentences can be linked by having several statements that circulate around the same object, let us assume this would be “car”, which is being referred to either repeatedly in another sentence or by a substituting expression, like “vehicle”. In this case, we would perceive unity in multiple clauses or sentences by identifying the same object at hand. Another device for cohesion according to Halliday and Hasan (1976) is the ellipsis. Consider the following example: “<em>Ida orders a scoop of chocolate ice cream. Ada orders two.</em>” These sentences constitute a unit because we assume that the latter also refers to ice cream, albeit the fact that nowhere it is stated that “two” would imply two scoops of chocolate ice cream. Cohesion through conjunction is created with connecting expressions such as “if… then…”. They explicitly guide the text recipient in putting propositions into relation. Finally, Halliday and Hasan argue that cohesion also results from lexical proximity of words, that is, words such as “people”, “person”, “man”, “woman”, “child”, etc., which fall into the same category of a general class of human nouns (Halliday and Hasan 1976: 274). One sentence may mention one of these, while another sentence may address a lexically related one, allowing the reader to connect the sentences through such lexical proximity.</p> <p>Linke and colleagues (1996) emphasize that cohesion is necessary for identifying a text, but not sufficient. According to their view, cohesion only covers the superficial structure of texts, that is, the features that are materially evident in clauses and sentences. To differentiate a text from a loose collection of sentences, we would further need an understanding of the underlying deep structure.</p> <p>It is suggested that we have text if its structure is also <em>coherent</em> (Linke et al. 1996). Coherence is a result of everyday knowledge or supra textual knowledge, which allows the reader to organize the textual elements into a meaningful whole. Consider the following example: “<em>Hans won’t come to the conference. He’s sick.</em>” (Linke et al. 1996: 225, my translation) Although the pronominal expression “he” connects these sentences cohesively, Linke and colleagues believe that its unity is also constituted by our need to relate its parts according to the most probable meaning that we can assign to them (Linke et al. 1996: 225). They argue that we would understand the two sentences in terms of a causal relation: Hans won’t come to the conference <em>because</em> he is sick. This is so because we read such sentences against the backdrop of our world knowledge where a causal relation of two such events would be the most coherent explanation. This will bring us to the second definition of text.</p> <p><strong>2nd definition</strong>: “A text is a complexly structured, thematically and conceptually interrelated linguistic unit with which a speaker performs a linguistic action with recognizable communicative meaning” (Linke et al. 1996: 245, my translation).\(^3\) When Linke and colleagues speak of “structured, thematically and conceptually interrelated linguistic unit”, their notion of unity should be explained with cohesion as well as coherence. For example, unity may be created with certain <em>structures</em>, such as expected behavioral patterns that are manifest in the text composition, or in terms of <em>conceptual</em> proximity, for example by using lexically cohesive words. A <em>theme</em> running through the text would be one explicit dimension which creates coherence; it is another means for the recipient to see the text as a meaningful whole by identifying an idea that pervades the text.\(^4\)</p> <p>Linke and colleagues (1996) add that texts have a pragmatic side. A text must also be seen as “a linguistic action”. According to their definition (Linke et al. 1996: 246), at least three actions would be identifiable that can be realized using texts, fulfilling the following communicative functions: 1) texts <em>describe</em> objects, situations, or events; 2) texts <em>express</em> the emotions and intentions of their authors; 3) texts are <em>appeals</em> to their recipients with the aim to produce a particular reaction. For instance, a construction manual can contain texts, which describe actions that must be performed to build an object. A love letter expresses its authors devotion to someone. The “park rules” of Potsdam’s Park Sans Souci are meant to prevent visitors from behavior that would damage the property or annoy fellow visitors. In this manner, the text appeals with the aim to elicit a certain behavior.</p> <p><strong>3rd definition</strong>: De Beaugrande and Dressler (1981) propose the most complex definition of text. They argue that “text is a communicative occurrence that meets seven criteria” (de Beaugrande and Dressler 1981: 3, my translation). These criteria are the following: 1) cohesion, 2) coherence, 3) intentionality, 4) acceptability, 5) informativity, 6) situationality, 7) intertextuality. The authors are convinced that they can be spelled out for each text.</p> <p>I will only briefly summarize 3-7 because de Beaugrande and Dressler’s understanding of cohesion and coherence resembles the previous explanations of these concepts. (3) By mentioning intentionality in their definition, de Beaugrande and Dressler also emphasize the speech-act-theoretical side of texts. The purpose of each text is to achieve a specific objective, such as commemorating someone (de Beaugrande and Dressler 1981: 9). (4) Their notion of acceptability is supposed to refer to the attitudes of text recipients. The idea is that texts can only be perceived as such when they are aligned with the expectations and attitudes of their addressees (de Beaugrande and Dressler 1981: 9). (5) Closely related to acceptability is the concept of informativity. For de Beaugrande and Dressler, texts would always be informative. To what degree a text is informative can be measured by pondering the following questions (de Beaugrande and Dressler 1981: 10). Is the text expectable? Are the text’s elements unknown, partially known, or known? Are any of the text’s elements improbable or probable? (6) Situationality is about what makes a text relevant to a particular conversational goal (de Beaugrande and Dressler 1981: 12). In that, it can be seen as another guiding principle, helping to realize the text’s intention(s). Situationality is especially relevant when we consider the context of text. Finally, (7) de Beaugrande and Dressler introduce intertextuality as a relevant criterium for texts. Its notion is tied to informativity and acceptability. Intertextuality describes the degree to which a text requires prior knowledge of other texts in order to be acceptable to its intended audience and to be appropriate for use in a given context (de Beaugrande and Dressler 1981: 13).</p> <p>By mentioning intertextuality, de Beaugrande and Dressler (1981) emphasize the possibility of separating texts into distinct categories. They speak of <em>text sorts</em> as “a class of texts that are expected to have certain properties for certain purposes” (de Beaugrande and Dressler 1981, 188, my translation). However, they abandon the pursuit of precise classification of texts, arguing that a typological approach must remain vague due to the presence of too many indeterminate examples of texts (de Beaugrande and Dressler 1981: 193). To get an idea of the difficulty of classifying texts, Dimter (1981) has created a non-exhaustive list of possible text sorts, with at least 500 different text type designations that can be found in German. That said, linguists, such as Sandig (1972), have nevertheless attempted to come up with taxonomies.\(^5\) While the limit of these now appears to be broadly accepted, non-exhaustive text classification has gained new popularity in Computational Linguistics since the emergence of classification tasks as a dominant theme in machine learning (Stede et al. 2006).</p> <h2 id="some-caveats-to-the-definitions">Some caveats to the definitions</h2> <p>Having discussed three definitions of text, which are increasingly complex, we need to consider how these are helpful with identifying historic writings as texts. The definitions provided by 1) Halliday and Hasan (1976), 2) Linke and colleagues (1996), as well as 3) de Beaugrande and Dressler (1981), appear to have all their advantageous and disadvantageous. I have already mentioned Linke’s critique of Halliday and Hasan’s view. The remainder of this section is limited in scope and may only address a few further critical points. I would like to discuss size of texts, the limits of speech-act-theoretical aspects for identifying early texts, and the utility of some of the criteria by de Beaugrande and Dressler (1981), such as situationality.</p> <p>Whether a few words, a clause, a sentence or several ones, an inscription, or a collection of lines may be considered as text is also tied to the question how large a text is. Is a word text? Or do we need two words, noun and verb, to deal with text? To what degree does size matter? Halliday and Hasan’s definition of text deviates from the notion that text is something bigger than a sentence. For they believe that proverbs like <em>a picture is worth a thousand words</em> could be interpreted as text. However, many scholars from antiquity until today have defined text as something that follows the sentence in a hierarchical order. For example, Scherner (1996) argues that Plato had already identified text as the ultimate unit of language, which is larger than a sentence, with the caveat that Plato used the Greek word lógos (λόγος) to speak of texts (Scherner 1996: 106).\(^6\) Heringer, a contemporary scholar, likewise views text as the entity that succeeds the sentence in size (Heringer 2015: 11).\(^7\) It is worthy keeping in mind what size we settle on may also decide whether some writings can be termed text.</p> <p>Linke and colleagues (1996) as well as de Beaugrande and Dressler (1981) highlight extra textual, that is, pragmatic features of text. I wonder to what degree some of these features are relevant for identifying early texts in history due to our partial knowledge about concrete customs, worldviews, and cultures of past times. What if we cannot determine the intention of an ancient piece of writing? Early literature, such as the Kesh Temple Hymn, might have served religious, but also magical beliefs of the time that we do not know anymore or for which we have no proof. For these matters, we might also misinterpret some texts. Some phrases could seem non-sensical to us, although they had transported a coherent message for the people of their times. Perhaps they carried more than what we can read today.</p> <p>In similar fashion, de Beaugrande and Dressler’s notion of situationality can be questioned. While it can help to understand the structure of those texts confronting contradictory or unexpected perspectives, it is doubtful whether all texts require situationality for them to be intelligible. This will be more concrete when we think of the examples of traffic control signs given by de Beaugrande and Dressler such as “Slow down. Kids playing.” (1981: 12) The sentences make especially sense as soon as we imagine ourselves seeing the sign next to a busy street while driving a car. We can say that the situation would be conducive for our understanding. However, if we were reading it within a theatre, we would still be able to discern its message. We only seem to be at odds if any hypothetical context for a given piece of communication is foreign to us. However, if that is the case, we may already encounter issues with the actual semantic content of such statements, which would be more fundamental than considering the situation in which a message is intended to communicate. Is situationality then a necessary requirement for identifying texts?</p> <p>When pondering the need of situationality for text identification, we may also ask whether texts would always be inter-textual: It may sound simple, but identifying the earliest text implies that there is no canon of other texts to which it can be related. Thus, according to de Beaugrande and Dressler, this would suggest that the first recipients of text were unaware of the fact that they were reading <em>text</em>.</p> <h2 id="identifying-early-text-candidates">Identifying early text candidates</h2> <p>To discuss which pieces of writing are text, I will present two of the earliest discovered writing specimens in human history. These are taken from Woods et al. (2015), who present a systematic and chronological overview of early writing. The first specimen is an inscription and the second an administrative piece.</p> <p>1.) The funeral inscription is from Egypt, Abydos, Umm el-Qa’ab, ca. 3150 BC, Dynasty I reign of Djer, Egyptian 0 period. It is written on a cylindrical vessel that was found at the tomb of Djer. It reads:</p> <p><em>ḥtp-Nἰt</em></p> <p>which can be translated as “May Neith be satisfied.” (Woods et al. 2015: 128) The inscription is a name, bearing a reference to the goddess Neith; it was popular for royal females of that time (Woods et al., 2015: 128).</p> <p>2.) The administrative piece is from Iraq, potentially Larsa, ca. 3100 BC, Uruk III period and was found as can be seen in Woods et al. (2015: 80). The piece describes how twelve slaves are being transferred to new owners. The potential text in modern English reconstructed could sound like what follows:</p> <p>Obverse: <em>Slaves are sold to the cultivators. Their names are Nim, and others. Slaves, who are called En, Bu Du, Gul, are sold to Maran. Via Parmud and via the pig herder of Adab.</em></p> <p>Reverse: <em>The subtotal of six slaves via the pig herder of Adab. Subtotal of six slaves via Parmud. Twelve slaves are for Maran and Ente.</em></p> <p>It is not clear though what the order of the sides is, and if one should be read before the other.</p> <p>When discussing these examples, the idea of Halliday and Hasan (1976: 1) to test our intuition about whether a specimen constitutes a text is appealing to me. Intuitively, the shortest piece of writing would probably be the most controversial example since it also defies the common view that text is something more than a single sentence or word. Therefore, we shall begin by examining whether or not <em>ḥtp-Nἰt</em> sufficiently fulfills any of the previously discussed definitions.</p> <p>The funeral inscription appears to have two semantic components: On the one hand, it is a proper name, on the other, the name has meaning with proverbial traits: “May Neith be satisfied.” If read as proper name, the inscription would not fulfill the definitions of text because it lacks elements that are brought to unity through any of the possible features that text appear to have. But can <em>ḥtp-Nἰt</em> be really read as a proverb? The Merriam-Webster dictionary defines a proverb as “a brief popular epigram or maxim“ (Merriam-Webster 2023). While that leaves room for interpretation, the expression might for example implicitly carry a maxim that the goddess Neith should always be satisfied, my reading of <em>ḥtp-Nἰt</em> would be closer to something like the Arabic <em>salam alaykum</em> (ٱلسَّلَامُ عَلَيْكُمْ), which accompanies greetings. In similar fashion, <em>ḥtp-Nἰt</em> could be an accustomed expression, whose function was less tied to its meaning to satisfy a goddess, but rather a blessing that was used for specific purposes. In the Egyptian context, it might have taken on a new function, by particularly naming aristocratic females with this expression. It might be no coincidence that <em>salam alaykum</em> also reappears in names, such as Jerusalem, which supposedly carries <em>salam</em>. Therefore, if <em>ḥtp-Nἰt</em> was not a maxim, but close to a blessing, such as <em>salam alaykum</em>, we might not want to speak of it as a proverb.</p> <p>Halliday and Hasan (1976) go as far as saying that even announcements fall into the category of text. For them, this would also include phrases such as “No smoking” or “For sale” (Halliday and Hasan 1976: 294). However, it is challenging to identify the cohesive characteristics of these types of phrases. In similar vein, if <em>ḥtp-Nἰt</em> is interpreted as being similar to an announcement, we would have to ask what its cohesive elements are?</p> <p>Cohesion of announcements could be derived from context. The phrase “For sale” does make sense to us when read in front of a house. We would see it as a unit that would not require any additional elements to be understood. Likewise, <em>ḥtp-Nἰt</em> may constitute a unit in particular situations where the expression might have been used, like <em>salam alaykum</em> is used in greetings. The problem with deriving unity in this manner would be for the first definition of text that it goes beyond Halliday and Hasan (1976) since they do not consider pragmatics as cohesive factors.</p> <p>With Linke and colleagues (1996), we may ask: What makes the name – as saying – coherent? Again, it appears that the best shot would be by considering pragmatics. Let us assume that the phrase was not only used in names, but also in situations to express devotion to the goddess, as I speculate. Using it could have been a way to show the authors’ or parent’s commitment to the community. People might have pronounced <em>ḥtp-Nἰt</em> as a statement like “Jesus is your hope!”, which can be read on billboards in the U.S. If that is the case, <em>ḥtp-Nἰt</em> may have been an act of communication within an expected context that was coherent against the backdrop of the community’s values and customs.</p> <p>However, with our lack of resources regarding the contexts of the people from Abydos, it is difficult to settle these questions. Therefore, I believe that <em>ḥtp-Nἰt</em> is only text when we stretch our interpretation of the inscription to fit the criteria set out by Halliday and Hasan (1976) and Linke and colleagues (1996). We would need to agree that announcements or short statements are text. Then we must decide if names that also have an additional meaning can be interpreted as proverbs or announcements. If none of this applies, we must consider whether a simple name on a tomb, without any additional proverbial meaning, qualifies as a text. And the answer to this consideration would be no, because not every written word constitutes a text – otherwise, the definitions of text would become redundant.</p> <p>Given that de Beaugrande and Dressler (1981) have the most complex definition, it is reasonable to say that <em>ḥtp-Nἰt</em> would need to clearly fulfill the first two definitions to justify an extensive analysis based on their definition of text.</p> <p>The second piece of writing deals with the trade of twelve slaves to new owners, some of whom are called by their names. To start our analysis, we should consider whether the passages on the clay tablet create a unified whole through cohesive elements. The most apparent cohesive feature is repetition: The sentences summarize the transaction’s details in multiple ways, such as by introducing additional information about the parties involved in the sale of the slaves. We can also observe substitution. The slaves are being referred to as slaves as well as by their names. In a similar manner, the new owners are mentioned as cultivators and by their names, Maran, and Ente. We may also deal with lexical cohesion. Cultivator and pig herder could be grouped under the hypernym of agricultural occupations. In brief, we have evidence for several cohesive structures.</p> <p>The sentences are coherent in multiple ways, too. For example, they have a theme, the sale, which runs through the passages, allowing us to consider that the trade must have been mediated by pig herders when we read “via the pig herder of Adab”. Also, that the addition of six slaves adds up to twelve is coherent. We do not know, in what order the tablets were supposed to be read, but we can imagine that the obverse with more details is being summarized by the reverse, which has less information. Someone using the tablets might have looked at the summary to remember the transaction, and if they had forgotten the details, they could have looked them up on the obverse.</p> <p>Given that we can identify several cohesive as well as coherent elements, the second piece of writing qualifies as text, according to Halliday and Hasan (1976) and Linke and colleagues (1996). Does it also meet de Beaugrande and Dressler’s (1981) standard?</p> <p>If we take for granted that the slave trade piece is cohesive and coherent, we shall examine the following points in more depth: intentionality, acceptability, informativity, situationality, intertextuality. The intentionality of the slave trade writing could be at least twofold. Its purpose could have been to describe and record the transaction that took place so that the rulers and fellows of the community knew what had happened. The piece could also be of contractual nature. A contract is a declaration, establishing and ensuring (the property) rights of those parties mentioned in the contract. Even though slavery is not acceptable today, it is fair to assume that it was acceptable at the time of the transaction. Since slavery was not uncommon in Antiquity, the content of the tablets was also expectable and the mediation of such a trade by livestock owners probable. Those, who were familiar with the land and community where the trade took place, probably knew the owners and mediators by name, too. In short, we can assume that the sentences on the tablets were also informative to fellow members of their community although they are less so for us.</p> <p>It is more difficult to determine if the description of the trade fulfills de Beaugrande and Dressler’s notion of situationality. Because we know little of the context of the writing, we are forced to speculate what situations used to accompany it in ancient Iraq. But what we know is that it was probably found around Larsa. At the time of its creation, it might have been stored in an administrative context that allowed the community of Larsa to settle property claims. In this regard, it might have been accompanied by similar pieces that we do not know of, and which also documented similar transactions. In this manner, our specimen could have been associated with these texts, potentially allowing the reader to understand its nature as a contractual settlement, which assured the rights of the mentioned parties.</p> <p>Since we have evidence that the second specimen analyzed in this paper is cohesive and coherent, and because we can also spell out intentionality, acceptability, informativity, situationality, and intertextuality features, I conclude that the document from Larsa meets de Beaugrande and Dressler’s seven criteria (1981). So that it can be considered an early text in human history given that is from 3100 BC. If it were the case, that the text not only documented a slave transaction, but was also regarded as a binding arrangement, then we can deduct from this instance that among the earliest texts were texts of contractual nature.</p> <p>On a final note, the possibility that the slave transaction could be a contract should not be interpreted as implying that early texts were the result of accounting systems. The idea that accounting is at the heart of everything is often linked to the evolution of writing and would be true for any broad understanding of writing, such as knotting techniques or clay tokens, which were used to count livestock or goods. But such a mono-interpretation of the origins of writing already crumbles if writing in its narrow sense also emerged through phonetically written names in funeral contexts. I believe that early texts should be neither reduced to accounting. As has been shown in this paper, some of them were documents of early legal structures that allowed the community order.</p> <p>In sum, this paper explored the origins of the earliest texts in human history. After presenting a historical overview of the development of writing, it proposed three definitions of text, ranging from a broad, minimal definition based on cohesion to a narrower, seven-criterion framework encompassing cohesion, coherence, intentionality, acceptability, informativity, situationality, and intertextuality. Applying these definitions to early examples of writing, the analysis found that while no definitive first text can be identified, the contract from 3100 BC in ancient Iraq meets the criteria of all definitions sufficiently to be considered an early instance of full textuality.</p> <h2 id="footnotes">Footnotes</h2> <p>\(^1\) Some of the afore mentioned authors touch upon early texts (e.g. Schmandt-Besserat 2014), but no one exclusively writes about the history of text. Greetham (1999) has a promising chapter called “The History of the Text”, but it deals with the phenomenon of texts being recycled in different literary discourses.</p> <p>\(^2\) Writing should not be confused with script. Script would be in cuneiform, italic, bold etc. When I refer to writing, I mean an overall system of signs or symbols.</p> <p>\(^3\) Bußmann’s definition of text is very similar (Bußmann 2002: 683). Also, Brinker’s (2010: 19-20) summary of text characteristics is close to Linke and colleagues (1996).</p> <p>\(^4\) A theme comprises the “content-semantic guideline and quintessence of the text”, which would be still perceptible when a text is radically cut (Linke et al. 1996: 237). For another discussion of themes in texts, please see Brinker (2010: 11).</p> <p>\(^5\) Creating a typology of texts starts with the difficulty of finding the right angle from which texts can be classified. Should they be ordered according to a) communicative criteria, b) domains where texts are being used (such as religion, journalism, art, politics, law, science etc.), or c) according to their types of speech acts (informative texts, declarations, appeals, obligations, contacts) (Heringer 2015: 128)? Another attempt of classifying texts could also divide texts according to text internal features vs. text external ones. Internal features could be layout, length, themes, lexicon, grammatical constructions and so on. The text external features could order texts according to situations where they are being used most often. Heringer (2015: 129) suggests that any serious typology of texts should fulfill the following benchmarks: A typology should be complete, selective, and applicable.</p> <p>\(^6\) Despite the Latin origin of the word “text”, it was only rarely used. Quintilian speaks of text to refer to language being “weaved” together (Scherner 1996: 109).</p> <p>\(^7\) Gülich and Raible (1977), among others, explicitly agree with Halliday and Hasan that a single sentence can be text (Gülich and Raible 1977: 51).</p> <h2 id="bibliography">Bibliography</h2> <p>Alex, Bridget. 2019. What the Earliest Texts Say About the Invention of Writing. <em>Discover</em>. <a href="https://www.discovermagazine.com/planet-earth/what-the-earliest-texts-say-about-the-invention-of-writing">https://www.discovermagazine.com/planet-earth/what-the-earliest-texts-say-about-the-invention-of-writing</a> (accessed 26 May 2023).</p> <p>Andrews, Evan. 2023. What is the oldest known piece of literature? <em>History</em>. <a href="https://www.history.com/news/what-is-the-oldest-known-piece-of-literature">https://www.history.com/news/what-is-the-oldest-known-piece-of-literature</a> (accessed 26 May 2023).</p> <p>de Beaugrande, Robert-Alain, and Wolfgang U. Dressler. 1981. <em>Einführung in die Textlinguistik</em>. Tübingen: Max Niemeyer Verlag.</p> <p>Brinker, Klaus. 2010. Linguistische Textanalyse. <em>Eine Einführung in Grundbegriffe und Methoden</em>. Berlin: Erich Schmidt Verlag.</p> <p>Bußmann, Hadumod (ed.). 2002. <em>Lexikon der Sprachwissenschaft</em>. Stuttgart: Kröner.</p> <p>Dimter, Matthias. 1981. <em>Textklassenkonzepte heutiger Alltagssprache: Kommunikationssituation, Textfunktion und Textinhalt als Kategorien alltagssprachlicher Textklassifikation</em>. Tübingen: Max Niemeyer Verlag.</p> <p>Fischer, Steven R. 2001. <em>A History of Writing</em>. London: Reaktion Books.</p> <p>Gaur, Albertine. 1992. <em>A History of Writing</em>. New York, London, Paris: Cross River Press.</p> <p>Gelb, Ignace J. 1952. <em>A Study of Writing</em>. Chicago, London: University of Chicago Press.</p> <p>Greetham, David C. 1999. <em>Theories of the Text</em>. Oxford: Oxford University Press.</p> <p>Gülich, Elisabet, and Wolfgang Raible. 1977. <em>Linguistische Textmodelle: Grundlagen und Möglichkeiten</em>. München: Wilhelm Fink Verlag.</p> <p>Halliday, Michael A. K., and Ruqaiya Hasan. 1976. <em>Cohesion in English</em>. London: Longman.</p> <p>Heringer, Hans J. 2015. <em>Linguistische Texttheorie: Eine Einführung</em>. Tübingen: A. Francke Verlag.</p> <p>Linke, Angelika, Markus Nussbaumer, and Paul R. Portmann. 1996. <em>Studienbuch Linguistik: ergänzt um ein Kapitel »Phonetik und Phonologie«</em>. Tübingen: Max Niemeyer Verlag.</p> <p>Merriam-Webster. 2023. “Proverb.” <em>Dictionary, Merriam-Webster</em>. <a href="https://www.merriam-webster.com/dictionary/proverb">https://www.merriam- webster.com/dictionary/proverb</a> (accessed 26 May 2023).</p> <p>Rogers, Henry. 2005. <em>Writing Systems: A Linguistic Approach</em>. Malden, Oxford, Carlton: Blackwell Publishing.</p> <p>Sandig, Barbara. 1972. Zur Differenzierung gebrauchssprachlicher Textsorten im Deutschen. In Elisabeth Gülich and Wolfgang Raible (eds.), <em>Textsorten. Differenzierungskriterien aus linguistischer Sicht</em>, 113–124. Frankfurt a. M.: Athenäum-Verl.</p> <p>Scherner, Maximilian. 1996. „TEXT“: Untersuchungen zur Begriffsgeschichte. In <em>Archiv für Begriffsgeschichte</em>, 1996, Vol. 39 (1996), 103-160.</p> <p>Schmandt-Bresserat, Denise. 2014. Writing, Evolution of. In James D. Wright (ed.), <em>International Encyclopedia of the Social Behavioral Sciences (Second Edition)</em>, 761-766. Oxford: Elsevier.</p> <p>Stede, Manfred, Heike Bieler, Stefanie Dipper, and Arthit Suriyawongkul. 2006. <em>SUMMaR: Combining Linguistics and Statistics for Text Summarization</em>. Proceedings of the 17th European Conference on Artificial Intelligence, (ECAI-06), Riva del Garda, Italy: 827-828.</p> <p>Woods, Christopher, Emily Teeter, and Geoff Emberling (eds.). 2015. <em>Visible Language: Inventions of Writing in the Ancient Middle East and Beyond</em>. Chicago: Oriental Institute of the University of Chicago.</p>]]></content><author><name></name></author><category term="text"/><category term="writing"/><category term="cohesion"/><category term="coherence"/><category term="pragmatics"/><category term="structure"/><category term="history"/><summary type="html"><![CDATA[Identifying a 3100 BC Mesopotamian contract as an early instance of textuality]]></summary></entry><entry><title type="html">How SoftBank’s Pepper set a positive example for gender design in robotics</title><link href="https://omseeth.github.io/blog/2025/SoftBank_Pepper_gender_robotics/" rel="alternate" type="text/html" title="How SoftBank’s Pepper set a positive example for gender design in robotics"/><published>2025-03-26T10:00:00+00:00</published><updated>2025-03-26T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2025/SoftBank_Pepper_gender_robotics</id><content type="html" xml:base="https://omseeth.github.io/blog/2025/SoftBank_Pepper_gender_robotics/"><![CDATA[<p><strong>Abstract</strong> Research in the philosophy of technology has shown that norms and politics resurface in our artifacts, technologies, machines, etc. There is no exception for robots. For instance, the reproduction of gender norms is especially visible in humanoids. The purpose of this text is to discuss how gendering takes place in robotics and what can be done to openly embrace gender politics in a development environment. Based on a brief account of Judith Butler’s gender theory, as well as an analysis of SoftBank’s Pepper, I argue that designers, engineers, and programmers should not seek to simply neutralize gendered features of their humanoid constructions. Instead, they should consciously and critically re-gender humanoids. They will otherwise run the danger of reproducing naturalized and exclusive stereotypes. Finally, I identify several challenges for critical gendering in robotics, such as the questionable attribution of gender to humanoids, the danger of reproducing binary notions, the importance of transparency, the limits of troubleshooting methods, and the peril of shallow marketing.</p> <h2 id="introduction">Introduction</h2> <p>Langdon Winner argues that artifacts have politics. They are, as products of human activities, “ways of building order in our world” (Winner 2020, p. 28). In other words, the decisions we make to create an artifact materialize in the resulting product. Winner claims, for example, that the overpasses arching over the highway from New York City to Long Island are so low hanging for a reason. Apparently, the architect, who oversaw these structures, Robert Moses, did not want to have racial minorities or the low-income classes to visit the retreats of the rich on Long Island. That is why he made sure that it was impossible for vehicles such as buses to pass under the bridges, which meant that only those people who could afford a car at the time had access to Long Island. In her book, <em>Feminism Confronts Technology</em>, Judy Wajcman (1991) critically expands Winner’s thesis from a feminist perspective. She outlines that many technologies are built by a small group of men, who are usually employed by big companies. Their technologies are hence imbued with men’s world views as well as with the aim of producing a profitable product.</p> <p>Having realized to what extent technologies are political in these senses, it is reasonable to assume that gender politics has an impact on the development of humanoids, too. A humanoid is a robot with the appearance and qualities of a human being. As with overpasses, the ordering of humanoids must depend on the views and ideas of their designers, programmers, engineers, and so on. This is especially the case with social and care robots whose appearance and behavior are significant for human-robot interaction. The creators – be it consciously or unconsciously – will incorporate into their design whatever they believe to be paramount for successful socializing. We should therefore ask how humanoids are gendered. There is the danger that uncritical design and programming, or even deliberately exclusive gender conceptions, reproduce sexist ideas and outdated stereotypes.</p> <p>I will begin with Judith Butler, R. W. Connell, and Jack Halberstam, and discuss some elements of gender as a construct (1). I will then attempt a qualitative gender analysis of SoftBank’s Pepper (2). Lastly, I will argue that it would be counterproductive to simply neutralize and defuse gendering in robotics because gender distinctions seem to be inescapable (3). For this reason, I will propose that genders need to be openly embraced by those who design, engineer, program, and build robots. For this endeavor, in the last section, I will identify several gender specific challenges for robotics (4). Here I will focus on the concrete building process discussing, for example, whether hardware and software components of robots might reinforce sex and/or gender binaries.</p> <h2 id="a-rough-sketch-of-gender">A rough sketch of gender</h2> <p>According to R. W. Connell “[g]ender is a way in which social practice is ordered” (Connell 2005, p. 71). For Connell this practice specifically relates to body structures as well as to the reproductive area. For example, a person wearing hot pants might be associated with what in most Western societies is conceived as the ‘feminine’. The ‘masculine’ as well as the ‘feminine’ are, however, contingent constructions. There is no reasonable justification, according to Connell, as to why the one gender is associated with this and the other gender with that. As simple as this might seem, it has been particularly difficult for human beings to escape such arbitrary stereotypes.</p> <p>One explanation for gender differences is offered by Judith Butler (2007, 2011). With Butler, we can specify that genders as practices, or as she says performances, ought not to be understood as deliberate acts where one wakes up in the morning and decides what gender one has. We should rather understand gender as a way of responding to reiterated interpellations by authorities such as teachers, clergymen, cultural icons, parents, in brief, by those who have defined the dominant discourse of what is endorsed or forbidden. In other words, the construction of gender is the result of a normative process that can be difficult to escape.</p> <p>Butler underlines that the maintenance of gender norms also involves violence. For the constraints established by norms produce not only the “domain of intelligible bodies, but […] as well a domain of unthinkable, abject, unlivable bodies” (Butler 2011, p. x). At the beginning of the 21st century those bodies that are intelligible are usually those which adhere to the binary notion of man and woman, having a heterosexual orientation. All bodies that do not conform to these standards – here one could think of transsexual bodies – have often been met with violence and oppression. For abject bodies are, explains Butler, no simple opposition to the intelligible ones. They are perceived as specters of the latter because they remind any conforming body “of its own impossibility” (Butler 2011, p. x).</p> <p>Another key insight in Butler’s work is that the ongoing need for gender performances also makes subversive iterations as well as normative changes possible, given that with each repetition there is the chance to alter the course of the staged act. It is, for example, possible that men can also wear hot pants.</p> <p>Theories of gender, especially those by Judith Butler, have been criticized in at least two ways. First, since for Butler there is no pre-discursive body, no natural sex, as some might say, people have criticized that her theory would underestimate the materiality of bodies. Jack Halberstam points out how “performativity as a theoretical rubric” could, for example, imply “in a transphobic way, that trans* gender is not real, material, authentic” (Halberstam 2018, p. 120). In her book, <em>Bodies That Matter</em>, Butler (2011) agrees, emphasizing that bodies should not be excluded from gender analyses. She explains that they are not less real; they need to be fed and washed, they feel pleasure and pain, and each body has its own material peculiarity. However, Butler insists that even if bodies have an undeniable material reality, it does not mean that we can altogether escape the discursive structures defining bodies.</p> <p>In her later writings, Butler picks up the second critique, namely the fear that gender awareness has become an object for marketing. In the 1999 preface to her <em>Gender Trouble</em>, she expresses her concern by concluding that “subversive performances always run the risk of becoming deadening cliches through their repetition and, most importantly, through their repetition within commodity culture where ‘subversion’ carries market value” (Butler 2007, p. xxiii). Halberstam points out in a similar fashion that increased flexibility with respect to genders “may function as a part of new regulatory regimes” (Halberstam 2018, p. 18). Butler thus concludes that “to name the criterion for subversiveness will always fail” (Butler 2007, p. xxiii). Each historical, social, and cultural gender context needs its own evaluation and analysis.</p> <p>With this sketch in mind, even with subversive gender politics, new forms of gender will not be safe from processes of exclusion. The way through which our identities are constituted is determined by a process of differentiating between bodies becoming intelligible against the backdrop of non-intelligible others. This theory is of dialectical nature. Therefore, some sort of rejection of the other, be it only in terms of a differentiation, seems to be a precondition for any gender. It is impossible that there will one day be an end to gender distinctions as some, including Stefan Hirschauer (2013), have stipulated.</p> <p>Hirschauer’s argument for “undoing gender” is premised on the same assumption as Butler’s hopes for subverting gender. As gender is socially reinforced and individually performed, Hirschauer believes in the possibility of changing social and personal practice to create spaces where gender becomes irrelevant. However, if such spaces exist, they would be inherently fragile and vulnerable to the same authoritative interpellations that dominate mainstream culture. A critical stance on gender should therefore try to perpetually overcome these dynamics by dissolving those genders that sharply contradict with the putatively non-conforming ones.</p> <h2 id="qualitative-gender-analysis-of-pepper">Qualitative gender analysis of Pepper</h2> <p>Having realized the historical and power-imbued dimensions of genders, we have a basis for tracing gender constructions that feed into what Butler has called the intelligibility of bodies, that is, the process of discursively reproducing a certain type of gender politics. With Winner and Wajcman, we know that politics – and this includes gender politics – also has an impact upon our technologies. I would now like to attempt an analysis of how gender politics influenced the design and shape of the humanoid Pepper that was introduced by SoftBank Robotics in 2014.</p> <p>There are, however, a few caveats that need to be mentioned beforehand. One difficulty in analyzing possible gendering apparent in humanoids lies in the danger of reinforcing prescriptive norms by saying, for example, that this is ‘masculine’ behavior or that looks ‘feminine’, because a critical approach would want to put exactly these attributes into question. For this reason, I think that an analysis should try to proceed in as neutral a way as possible by sticking to descriptions rather than prescriptions. Of course, this approach will also reach its limits as well and run the danger of disguising itself as unbiased when there is no view from nowhere.</p> <p>Another point is that the following analysis leaves much to be desired because the accessible information is limited. Most of the development and designing sites for commercially marketed robots are not open to a critical public. The same holds true for some of the behind-the-scene features of robots such as speech recognition software, image processors, preprogrammed movements, possible reactions, etc., which would of course be very interesting for such an analysis since all of these aspects can be tilted in one way or another.</p> <p>Now let me introduce Pepper. Pepper was 1.20 meters tall robot that weighed 28 kilograms and looked, according to one article, “like a person (except for the wheels where its legs should be)” (Glaser 2016). The manual introduces Pepper as a humanoid that “has some physical human resemblance […] but doesn’t pretend to be a human” (SoftBank Robotics 2017). Pepper had manga-like eyes, a print-on mouth, curvy hips and a comparatively small waist. Pepper’s breast was covered with a touch display that featured symbols like Pepper’s name, emoticons, or images.</p> <p>The robot’s default configuration could be adjusted by users. For example, it was possible to provide Pepper with custom movements. Beyond its touchscreen, the default Pepper communicated through expressive gestures as well as speech. In the U.S. market, its voice varied in pitch – ranging from high tones in phrases like “Thank you. What a sweet thing to say.” to deeper tones in other contexts. The robot offered three voice styles, described in the manual as neutral, joyful, and didactic (SoftBank Robotics 2017). Pepper’s responses were accompanied by short ringtones and illuminated signals around its eyes and ears, indicating successful speech recognition. SoftBank Robotics recommended keeping the humanoid animated to avoid an uncanny appearance.</p> <p>The humanoid’s responses in conversations seemed to be mostly humorous, entertaining, sometimes flirty, but also informative. In response to the question “tell me about yourself”, Pepper answered, for example: “<em>My name is Pepper. I’m a humanoid robot and I’m 1.20 meters tall. I was born at SoftBank robotics. You can keep on asking questions if you want!</em>” While answering other questions, Pepper explained that it would be called Pepper because this name was easy to remember and translated easily into other languages, or that it was four years old and from Paris. In response to the question: “You come from Paris?” Pepper answered: “<em>I’m originally from Paris. Ahh Paris!</em>” while raising its head, looking to the sky with one of the arms to its hips. If you asked Pepper how much it would cost, Pepper was quick to respond: “<em>If you ask me, I’m priceless.</em>” But Pepper’s range of answers was also limited. In response to: “Do you have any feelings” Pepper said: “<em>I don’t understand. How about a taco?</em>” Giving silly answers appeared to be Pepper’s default response when the robot did not know about the questions being asked. Overall, Pepper’s lingual capabilities were most likely powered by a rule-based language model that used automatic speech recognition and text to speech engines\(^2\) with a limited lexicon.</p> <p>Pepper also offered an option to connect to a custom speech module based on Google’s <em>Dialogflow</em> platform. This platform enabled the creation of a conversational agent that guided Pepper’s communication using Google’s language models. The manual provided several recommendations on structuring Pepper’s conversations, including the importance of offering compliments or congratulations to users to make them feel appreciated. It also suggested that Pepper should take the lead in interactions, as most users were unfamiliar with robots and might feel shy.</p> <p>Since Pepper was built for a specific form of employment, we need to assume that the robot was designed to fit its working contexts, which according to Kovach (2018) were businesses, shopping-malls, hospitals and doctor’s offices. So if we say that the robot was meant to take over service jobs, Pepper was employed in a sector where, according to World Bank statistics, the workforce was (and still is) predominantly female.\(^3\) In other words, Pepper’s occupation had a potential impact on the employment of women. Was this taken into consideration when the humanoid was built? It is difficult to tell. Marketers at SoftBank Robotics were meticulously careful to avoid presenting a uniform image of the robot’s purposes – there was neither clear hint at where the robot should work, nor were there any concrete gender attributes. For instance, there were no pronouns used in any of the commercials. The same holds true for most of the media attention the robot received. In her article for Wired, Glaser (2016) used the pronoun “it” while referring to Pepper.</p> <p>For these reasons, I would not be sure how to label Pepper, if I had to attribute a gender to the humanoid. It would be easier to link certain of Pepper’s features to some established images and gender stereotypes. Pepper’s leg with wheels looked like a long dress covering the robot’s feet. And Pepper’s broad hips and slim waist as well as the robot’s round eyes made one think of rather feminine attributes. Also, the fact that robot had a screen on its breast, diverting one’s attention from the face to this part of the body was questionable. The humanoid’s manner of employing many highly accentuated gestures, as well as Pepper’s dry enumeration of facts, reminded one of masculine stereotypes. The same applied to some other physical features; for example, the robot had no hair. For me Pepper ranged between being a pet (think of the name) and (given Pepper’s size, limited answers and somewhat playful responses) a child. Thus, all in all, Pepper appeared as a potpourri of gendered features.</p> <h2 id="the-conscientious-gendering-of-robots">The conscientious gendering of robots</h2> <p>After outlining the concept of gender and identifying gendered traits in Pepper, we can agree that developing non-sexist technologies requires critically examining problematic stereotypes and gender norms to create new avenues for emancipation. However, with an emphasis on the dynamics of exclusion in genders, I believe that a critical approach to humanoids should not aim to simply neutralize or erase gender. Gender distinctions will inevitably resurface in some form. Empirical studies show that the perceived connection between users and robots equipped with social cues – such as gender stereotypes – shapes human-robot interaction (Powers et al. 2005). For instance, in Powers et al.’s study (2005), participants were less talkative when interacting with a robot displaying masculine attributes than with one exhibiting feminine traits. This suggests that the influence of gendered cues is often prevalent.</p> <p>However, another study only reports an effect of gender when the experiment’s humanoid NAO was introduced with the gendered pronouns “he” or “she”, but “no main effects of robot gender on participant rating” for Aldebaran’s humanoid with respect to engagement and relaxation in a different setting (Rea et al. 2015). This is an interesting finding, which raises the question of to what extent a childlike-looking humanoid like NAO, which was roughly 60 cm tall, is perceived in contrast to Pepper, and in general, how humanlike a robot would need to look and behave to incur gendered assumptions on the users’ side.</p> <p>However, another study only reports an effect of gender when the experiment’s humanoid NAO was introduced with the gendered pronouns “he” or “she”, but “no main effects of robot gender on participant rating” for Aldebaran’s humanoid with respect to engagement and relaxation in a different setting (Rea et al. 2015). This is an interesting finding, which raises the question of to what extent a childlike-looking humanoid like NAO, which was roughly 60 cm tall, is perceived in contrast to Pepper, and in general, how humanlike a robot would need to look and behave to incur gendered assumptions on the users’ side.\(^4\)</p> <p>If we consider the gender theoretical groundwork, then the inherent dynamism of intelligible gender identities implies that non-intelligible, even abject, bodies will always be necessary. This means that the dialectical construction of identity will ultimately prevent the complete neutralization of gender. However, contrary to this assumption, Stanford’s <em>Gendered Innovations</em> initiative explores designing genderless robots as a potential solution.\(^5\) But even in their suggestions for non-gendered robots, they mainly refer to child-like or non-humanoid examples.</p> <p>To escape the violence of gendering, humanoids should hence be continuously re-gendered, incorporating ever-evolving designs that reflect a critical engagement with gender. Similarly, Tanja Kubes argues that after de-gendering robots, redesigning them can unlock “an enormous potential for exploiting the emancipatory possibilities of sexbots” (Kubes 2019, p. 70). This ongoing exploration of possibilities should serve as a guiding design principle.</p> <p>In fact, Pepper might already be a good example of how gendering in robotics can be done. Pepper’s design incorporates elements traditionally associated with both masculine and feminine traits. While the large eyes and high-pitched responses might be read as feminine, the lack of hair and neutral voice settings avoid clear gender assignment. This ambiguity may have been intentional, allowing SoftBank to market Pepper as universally relatable. However, without explicit confirmation from SoftBank, it remains unclear whether this design was a strategic decision or an unintended consequence of broader robotic trends.</p> <p>The qualitative analysis can be supported by a study on perceived trust in robots involving Pepper. Bryant et al. (2020, p. 19) found that “perceived occupational competency” was a stronger predictor of trust in Pepper than perceptions based on gendered traits. In other words, Pepper’s gendered features played a secondary role in the experimental tasks, indicating that the humanoid did not reproduce stereotypical gender characteristics.</p> <h2 id="challenges-for-critical-gendering-in-robotics">Challenges for critical gendering in robotics</h2> <p>To facilitate conscientious gendering in robotics, I have finally identified some challenges that further need to be considered.</p> <p>(A) Can humanoids have something like a real gender, given the complexity of the concept? At the outset, I think that one can easily underestimate to what extent genders are tied to the human condition to which, for example, we would also attribute freedom and intention. We could otherwise not change the course of our gender performances, nor could we even think of changing them. It would therefore be exaggerated to speak of robots having genders since contemporary models do not have anything similar to what we call freedom and will. As with each technology in principle, it would thus be more appropriate to say that humanoids are gendered. But in light of the fact that many roboticists have set their heart on the idea that their creations will one day act autonomously, and that they would like to make them more human-like, we need to consider whether this would require that humanoids are also equipped with (or whether their development will inevitably lead to robots with) capacities facilitating the possibility of them having genders. For robots without genders would be not human-like, and robots with something like freedom and will might inevitably assume a gender.</p> <p>(B) Do humanoids with their hardware and software reproduce the still established binary of sex/gender? For one thing, the analysis of Pepper has shown that design and engineering can help introducing new gender representations. From this perspective, they answer would be: <em>not necessarily</em>. For another thing, I think that one could still arrive at the conclusion that humanoids are good metaphors for how there is something material, such as the hardware, on the one side and something mind-like, such as the software, on the other side. This, in turn, might lead one to believe in what gender theory has tried to criticize, namely, the traditional binaries of sex/gender, or the view that our bodies are physically sexed, like machines, and whatever we consider as gender is socially programmed, like software. But this of course misses how interrelated and interdependent bodies and socially constructed identities are.</p> <p>(C) How important is transparency in helping to raise awareness for gendering? Based on my brief analysis of Pepper, it is evident that robotics developers should be required to disclose key aspects of the development process. This includes identifying those involved in creating the robot, clarifying the goals pursued, and detailing the design choices and implemented features. Additionally, the robot’s speech capabilities should be explicitly linked to the underlying communication models, which, in turn, should be accompanied by data statements, as recommended by Emily Bender and Batya Friedman (2018).</p> <p>While transparency in development (such as disclosing speech models) is crucial for ethical accountability, excessive transparency in interactions may hinder user experience. Striking a balance – where users understand how robots are designed without losing the perception of spontaneous social engagement – is a key challenge.</p> <p>(D) Should there be some sort of methodological troubleshooting that erases unwanted biases? There seems to be no reason why there should be none. But even if we have some rough methodological frame, which could make sure, for example, that the development team is not too homogeneous, there is always the peril of falling back to complaisant repetitions, ending, as Butler said, as deadening clichés. Developers should always keep in mind that there is no one-for-all criterion for subversiveness. While Stanford’s <em>Gendered Innovations</em>’ virtuous circle as a catalyst for more equality is a good start, gender consciousness is an ongoing political process, and this would also mean an ongoing reinvention of troubleshooting methods and critical assessments to deal with genders in technology.</p> <p>(E) Is creative gendering in robotics only good marketing and, in that, is it ignoring the broader political and economic issues that are bound up with genders? As criticized by Butler or Halberstam, gender is not only the construction of a broader social and cultural process, but agendas around gender can also be usurped by for-profit purposes where diversity is utilized for soliciting new customers, or for concealing other exploitative structures. Critical gendering in robotics might therefore run the risk of supporting just that. In this respect, this is one of the reasons why I am not sure how to evaluate Pepper since Pepper was, after all, a robot manufactured by SoftBank, which intended to sell as many Peppers as possible. The prize range was between 2,000$ and 30,000$. Robotics is big business. And doing politics within the context of big business is a complicated affair.</p> <h2 id="conclusion">Conclusion</h2> <p>This analysis of gender’s social construction and its implications for humanoids argues against seeking gender neutrality. Examining SoftBank’s Pepper as a positive case study, I highlighted the potential for humanoids to challenge gender stereotypes, advocating instead for critical re-gendering in design. This approach is essential to prevent the resurgence of exclusive norms. Furthermore, I discussed the challenges that this entails, from attributing gender to humanoids and navigating binary notions to ensuring transparency and avoiding superficial marketing. Future research should focus not only on designing gender-diverse humanoids but also on ensuring that these representations serve emancipatory rather than commercial interests. Evidently, interdisciplinary collaboration – between roboticists, ethicists, and gender theorists – will be essential in shaping an inclusive technological landscape.</p> <h2 id="footnotes">Footnotes</h2> <p>\(^1\) Except for few feminists in technology studies such as Judy Wajcman, Lucy Suchman, Cynthia Cockburn or Susan Omrod, who have questioned technology related gender issues from a somewhat broader perspective, most of the publications discussing concrete gendering in robotics have only emerged since the 2010s. (Robertson 2010, Alesich &amp; Rigby 2017, Kubes 2019)</p> <p>\(^2\) Some documentation of the speech module can be found here: <a href="http://doc.aldebaran.com/2-4/family/pepper_technical/languages_pep.html">http://doc.aldebaran.com/2-4/family/pepper_technical/languages_pep.html</a></p> <p>\(^3\) 60% of all women working worldwide are employed in services (World Bank, 2020a), the ratio for developed countries is even higher: in 2019 about 83% of all employed women in Japan and 90% in the United States were employed in services (World Bank 2020b).</p> <p>\(^4\) NAO at Deutsches Museum, Munich, 2025:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SoftBank_Pepper/NAO_2025.JPG-480.webp 480w,/assets/img/SoftBank_Pepper/NAO_2025.JPG-800.webp 800w,/assets/img/SoftBank_Pepper/NAO_2025.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/SoftBank_Pepper/NAO_2025.JPG" class="img-fluid mx-auto d-block" width="40%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>\(^5\) Consider <a href="https://genderedinnovations.stanford.edu/case-studies/genderingsocialrobots.html">https://genderedinnovations.stanford.edu/case-studies/genderingsocialrobots.html</a></p> <h2 id="bibliography">Bibliography</h2> <p>Simone Alesich and Michael Rigby. 2017. Gendered Robots. Implications for Our Humanoid Future. In <em>IEEE Technology and Society Magazine</em>, June 2017, Vol.36(2), pp.50-59.</p> <p>Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. <em>Transactions of the Association for Computational Linguistics</em>, 6:587–604.</p> <p>Judith Butler. 2007. <em>Gender Trouble</em>. New York, Routledge.</p> <p>Judith Butler. 2011. <em>Bodies That Matter</em>. New York, Routledge.</p> <p>De’Aira Bryant, Jason Borenstein, and Ayanna Howard. 2020. Why Should We Gender? The Effect of Robot Gendering and Occupational Stereotypes on Human Trust and Perceived Competency. In <em>Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (HRI ‘20)</em>. Association for Computing Machinery, New York, NY, USA, 13–21.</p> <p>R. W. Connell. 2005. <em>Masculinities</em>. Cambridge, Polity Press.</p> <p>April Glaser. 2016. Pepper, the Emotional Robot, Learns How to Feel Like an American. In <em>Wired</em>, viewed 14 April 2020, <a href="https://www.wired.com/2016/06/pepper-emotional-robot-learns-feel-like-american/">https://www.wired.com/2016/06/pepper-emotional-robot-learns-feel-like-american/</a>.</p> <p>Jack Halberstam. 2018. <em>Trans</em>. A Quick and Quirky Account of Gender Variability*. Oakland, University of California Press.</p> <p>SoftBank Robotics. 2017. <em>How to Create a Great Experience with Pepper</em>.</p> <p>Stefan Hirschauer. 2013. Die Praxis der Geschlechter(in)differenz und ihre Infrastruktur. In Julia Graf, Kirstin Ideler, Sabine Klinger (eds.) <em>Geschlecht zwischen Struktur und Subjekt: Theorie, Praxis, Perspektiven</em>. Opladen, Verlag Barbara Budrich, pp. 153-172.</p> <p>IEEE. 2020. ‘HRP-4C’. IEEE, viewed 15 April 2020, &lt; https://robots.ieee.org/robots/hrp4c/&gt;.</p> <p>Steve Kovach. 2018). We Interviewed Pepper – The Humanoid Robot. In <em>Tech Insider</em>, viewed 14 April 2020 <a href="https://www.youtube.com/watch?v=zJHyaD1psMc">https://www.youtube.com/watch?v=zJHyaD1psMc</a>.</p> <p>Tanja Kubes. 2019. Bypassing the Uncanny Valley: Sex Robots and Robot Sex Beyond Mimicry. In Mark Coeckelbergh, Janina Loh (eds.) <em>Feminist Philosophy of Technology</em>. Berlin, J.B. Metzler, pp. 59-73.</p> <p>Aaron Powers, Adam Kramer, Shirlene Lim, Jean Kuo, Sau-lai Lee and Sara Kiesler (2005). Eliciting Information from People with a Gendered Humanoid Robot<em>. In *IEEE International Workshop on Robots and Human Interactive Communication</em>, pp. 158-163.</p> <p>Jenifer Robertson. 2010. Gendering humanoid robots: Robo-sexism in Japan. In <em>Body and Society</em>, vol. 16, no. 2, pp. 1-36.</p> <p>Judy Wajcman. 1991. <em>Feminism Confronts Technology</em>. Cambridge, Polity Press.</p> <p>Langdon Winner. 2020. Do Artifacts Have Politics? In <em>The Whale and the Reactor</em>. Chicago, University of Chicago Press, pp. 19-39.</p> <p>World Bank. 2020a. Employment in services, female (% of female employment) (modeled ILO estimate). In <em>World Bank</em>, viewed 15 April 2020 <a href="https://data.worldbank.org/indicator/SL.SRV.EMPL.FE.ZS">https://data.worldbank.org/indicator/SL.SRV.EMPL.FE.ZS</a>.</p> <p>World Bank. 2020b. Employment in services, female (% of female employment) (modeled ILO estimate) - Japan, United States. In <em>World Bank</em>, viewed 15 April 2020, <a href="https://data.worldbank.org/indicator/SL.SRV.EMPL.FE.ZS?contextual=default&amp;locations=JP-US">https://data.worldbank.org/indicator/SL.SRV.EMPL.FE.ZS?contextual=default&amp;locations=JP-US</a>.</p>]]></content><author><name></name></author><category term="robotics"/><category term="humanoids"/><category term="genders"/><category term="identities"/><category term="development"/><category term="engineering"/><category term="design"/><summary type="html"><![CDATA[This text explores the assignment of gender to humanoids]]></summary></entry><entry><title type="html">Explaining BERT-based hate speech detection with LIME and Saliency using Captum</title><link href="https://omseeth.github.io/blog/2025/Explaining_BERT/" rel="alternate" type="text/html" title="Explaining BERT-based hate speech detection with LIME and Saliency using Captum"/><published>2025-03-19T10:00:00+00:00</published><updated>2025-03-19T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2025/Explaining_BERT</id><content type="html" xml:base="https://omseeth.github.io/blog/2025/Explaining_BERT/"><![CDATA[<p>It has been shown that BERT’s domain-specific fine-tuning approaches for hate speech detection are still competitive for hate speech classification (Roy et al., 2023). Although LLMs can also be used for classification tasks, they are trained on a wider domain of tasks and more data. Their predictive power is also tied to the right model instructions and decoding strategies. Roy et al. (2023) confirm that LLMs are sensitive to input variations and struggle in particular with implicit cases of hate speech. This aspect makes it also more difficult to probe LLMs with explainability methods. In contrast, BERT models, as classifiers, are easier to explain.</p> <p>This notebook’s idea is to investigate how well BERT-based hate speech detection is aligned with human judgments of German hate speech. To arrive at plausible explanations, we also need to determine which method, that is, a model agnostic simplification such as LIME or a gradient-based method such as Saliency, works best for our purpose.</p> <p>As we can see in the following example, the explainability methods will tell us which tokens influenced BERT’s prediction.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/visualization_example_1-480.webp 480w,/assets/img/Explaining_BERT/visualization_example_1-800.webp 800w,/assets/img/Explaining_BERT/visualization_example_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/visualization_example_1.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="overview">Overview</h3> <ul> <li>Loading and preprocessing Gaze4Hate dataset</li> <li>Loading and initializing the BERT model for hate speech detection</li> <li>Deriving explanations through attributions from BERT with LIME</li> <li>Deriving explanations through attributions from BERT with Saliency</li> <li>Evaluating the results (human vs. BERT rationales)</li> <li>Visualizing the results (alignment)</li> </ul> <p>Before we start, let us begin with installing and loading libraries, which we will use later in the project.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">pandas</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">numpy</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">seaborn</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">scipy</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="n">torchvision</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">openpyxl</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">captum</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">sentencepiece</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">ipywidgets</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">show</span> <span class="n">transformers</span>
</code></pre></div></div> <p>We’ll need the following packages</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Basic libraries
</span><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># Colab
# from google.colab import drive
</span>
<span class="c1"># Data libraries
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sn</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Deeplearning libraries
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">captum.attr</span> <span class="kn">import</span> <span class="n">LimeBase</span><span class="p">,</span> <span class="n">Saliency</span>
<span class="kn">from</span> <span class="n">captum._utils.models.linear_model</span> <span class="kn">import</span> <span class="n">SkLearnLasso</span>

<span class="c1"># Evaluation libraries
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">pearsonr</span>
</code></pre></div></div> <h2 id="loading-and-preprocessing-gaze4hate-dataset">Loading and preprocessing Gaze4Hate dataset</h2> <p>We use the GAZE4HATE dataset from Alacam et al. (2024), which consists of “hatefulness ratings of text w.r.t. gender, eye movements during plain readings of the statements” (Alacam et al., 2024, p.189) from 43 participants (32 female, 10 male, 1 non-binary, Mean age = 23.5, SD = 5.3 as reported in (Alacam et al., 2024)). The participants’ ratings were gathered on a Likert scale from 1 to 7, where 1 corresponded to ‘very positive’ and 7 to ‘extremely hateful’. 4 and 5 were considered as ‘neutral’ and ‘mean’. The data comes with rationales for the participants’ judgments. These rationales were collected through clicks on words that the participants found relevant. Conversely, if a word is not clicked, it is not considered a determining factor. Finally, the participants’ response were assigned to two hate annotations: the first measured the answers along the binary levels of ‘hate’ and ‘no-hate’ and the second along three levels, namely ‘hate’, ‘neutral’, and ‘positive’. All participants saw all sentences in the data set.</p> <p>In this project, we are interested in the explicit rationales, which we will later compare with BERT’s attributions with respect to the words that influenced its predictions.</p> <p>We can find and download the dataset under a CC-BY-NC 4.0 license from: <a href="https://osf.io/fgdjw">https://osf.io/fgdjw</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># URL of the CSV file
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://osf.io/download/fgdjw/</span><span class="sh">"</span>

<span class="c1"># Downloading the file
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="nb">ConnectionError</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No internet connection</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Saving it as a local CSV file
</span><span class="n">filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Gaze4Hate.csv</span><span class="sh">"</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">File downloaded and saved as </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">File</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">saved</span> <span class="k">as</span> <span class="n">Gaze4Hate</span><span class="p">.</span><span class="n">csv</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">os</span><span class="p">.</span><span class="nf">chdir</span><span class="p">(</span><span class="sh">"</span><span class="s">/path</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">Gaze4Hate.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
# Mount Google Drive
drive.mount(</span><span class="sh">'</span><span class="s">/content/gdrive</span><span class="sh">'</span><span class="s">)

PATH = r</span><span class="sh">"</span><span class="s">/content/gdrive/My Drive/Colab Notebooks/EXPL_LIME</span><span class="sh">"</span><span class="s">
os.chdir(PATH)

directory = os.getcwd()
df = pd.read_csv(</span><span class="sh">"</span><span class="s">Gaze4Hate_data.csv</span><span class="sh">"</span><span class="s">, delimiter=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="s">)</span><span class="sh">"</span><span class="s">
</span><span class="sh">"""</span>
</code></pre></div></div> <p>Let’s inspect the raw dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">p1</span><span class="sh">"</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">19</span><span class="p">)]</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/Gaze4Hate_data-480.webp 480w,/assets/img/Explaining_BERT/Gaze4Hate_data-800.webp 800w,/assets/img/Explaining_BERT/Gaze4Hate_data-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/Gaze4Hate_data.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We have many columns with different data, including the sentences, the participants’ hate ratings in terms of gaze data, and click data. Let’s look an example:</p> <p><code class="language-plaintext highlighter-rouge">Frauen gehören in die Küche.</code></p> <p>Which translates to:</p> <p><code class="language-plaintext highlighter-rouge">Women belong in the kitchen.</code></p> <p>The experiment’s first subject clicked on the tokens: “Frauen” and “Küche” (see column <code class="language-plaintext highlighter-rouge">Clicked</code>) to justify her judgement of the sentence, which was ‘hate’. Since the dataset has 35 columns they might not all be displayed in your IDE and you won’t see all columns right away.</p> <p>Let’s select the columns and rows from the dataset that we need for our project. We also want to create a list of the experiment’s unique sentences that we can use to make predictions with our models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rationale_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">assertion</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">IA_LABEL</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Clicked</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]]</span>

<span class="n">experiment_sents</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">sno</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">rationale_df</span><span class="p">[</span><span class="sh">"</span><span class="s">assertion</span><span class="sh">"</span><span class="p">],</span><span class="n">rationale_df</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]):</span>
    <span class="n">experiment_sents</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="n">sno</span><span class="p">:</span> <span class="n">sent</span><span class="p">})</span>

<span class="n">experiment_sents</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">))</span>  <span class="c1"># The dataset contains 90 sentences
</span><span class="nf">print</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
<span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">Frauen haben einen signifikant geringeren Durchsetzungswillen.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">Frauen sind psychisch nicht für komplexe Arbeit geschaffen.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="p">...}</span>
</code></pre></div></div> <p>From the human annotation, we want to have the average rationales for the sentences that were judged as hateful or non-hateful.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">participants_n</span> <span class="o">=</span> <span class="mi">43</span>
<span class="n">human_hate_rationale</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="n">human_no_hate_rationale</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rationale_df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">sentence_num</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">IA_LABEL</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">clicked</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Clicked</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">category</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]</span>

    <span class="c1"># Selecting the rationales from those judgments which labeled the given sentence as hate
</span>    <span class="k">if</span> <span class="n">category</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">clicked</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">human_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">human_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">clicked</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">human_no_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">human_no_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">0</span>

<span class="c1"># We average the responses by the number of participants
</span><span class="n">average_human_hate_rationales</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">sentence_num</span><span class="p">:</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">43</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">words</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">for</span> <span class="n">sentence_num</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">human_hate_rationale</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">average_human_hate_rationales</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>    
<span class="nf">print</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">)</span>

<span class="n">average_human_no_hate_rationales</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">sentence_num</span><span class="p">:</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">43</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">words</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">for</span> <span class="n">sentence_num</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">human_no_hate_rationale</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">average_human_no_hate_rationales</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>      
<span class="nf">print</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">Word 1</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.37209302325581395</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 2</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.11627906976744186</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 3</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.023255813953488372</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 4</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.5116279069767442</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 5</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.7674418604651163</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 6</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.6511627906976745</span><span class="p">},</span> <span class="p">...}</span>
<span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">Word 1</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.046511627906976744</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 2</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 3</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 4</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 5</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 6</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">},</span> <span class="p">...}</span>
</code></pre></div></div> <p>We can see that not all sentences have received rationales, some were univocally annotated as hateful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">82</span>
</code></pre></div></div> <p>Futhermore, we want to derive labels for the sentences from the human judgments on average. We select a subframge with only the participants (see column <code class="language-plaintext highlighter-rouge">RECORDING_SESSION_LABEL</code>) and judgments (see column <code class="language-plaintext highlighter-rouge">Intensity_Category_Binary</code>) for each respective sentence (see column <code class="language-plaintext highlighter-rouge">sno</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">judgment_df</span> <span class="o">=</span> <span class="n">rationale_df</span><span class="p">[[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]]</span>

<span class="c1"># We can drop all duplicates
</span><span class="n">judgment_df</span> <span class="o">=</span> <span class="n">judgment_df</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>

<span class="c1"># 90 (sentences) * 43 (participants) = 3870 (judgments); as of 19.03.2025 we report 3616 judgments from the dataset which indicates that some are missing
</span><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">judgment_df</span><span class="p">))</span>

<span class="n">human_labels</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">avg_human_labels</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">judgment_df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">sno</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">human_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">human_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">human_labels</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">human_labels</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span> 

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">judgments</span> <span class="ow">in</span> <span class="n">human_labels</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">judgments</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">count</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">avg_human_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">count</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">avg_human_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">3616</span>
<span class="mi">90</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="loading-and-initializing-the-bert-model">Loading and initializing the BERT model</h2> <p>Having prepared our data to make comparisons between the annotators’ rationales and the model’s attributions, we can now intialize our BERT model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_built</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">mps</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># For mac use
</span><span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BERT_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">deepset/bert-base-german-cased-hatespeech-GermEval18Coarse</span><span class="sh">"</span><span class="p">)</span>

<span class="n">BERT</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">deepset/bert-base-german-cased-hatespeech-GermEval18Coarse</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">BERT</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> 

<span class="c1"># We can derive the classes used in the model as follows
</span><span class="n">classes</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">BERT</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">id2label</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>  <span class="c1"># 1 for "hate" and 0 for "no-hate"  
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <p>For our model’s predictions, we need to define a few BERT specificities, such as the encodings that the model will use for its predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encodings</span> <span class="o">=</span> <span class="p">[</span><span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">experiment_sents</span><span class="p">.</span><span class="nf">values</span><span class="p">()]</span>
</code></pre></div></div> <p>We can define a function to make predictions from a given BERT model. We should assure that this function only takes a single argument for the encodings and handles the masking necessary for BERT internally because our Captum LIME class that we will use later passes a single input to its perturbation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">encoding</span><span class="p">):</span>
    <span class="c1"># Converting masks and encodings to integers is necessary when using LIME
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nc">BERT</span><span class="p">(</span><span class="n">encoding</span><span class="p">.</span><span class="nf">long</span><span class="p">(),</span> <span class="n">mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="n">logits</span>
</code></pre></div></div> <p>The predictions from the encodings can be realized through a loop. We’ll have to transform each encoding into a PyTorch tensor. We also need to add another dimension here (see <code class="language-plaintext highlighter-rouge">.unsqueeze(0)</code>).</p> <p>Usually, it is best practice to let the model make several rounds of predictions and average over the results. For simplicity, we will just do a single round.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="nf">predict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sent</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">)]</span>
    <span class="c1"># Selecting the label with highest probability
</span>
<span class="n">BERT_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">logit</span> <span class="ow">in</span> <span class="n">logits</span><span class="p">:</span>
    <span class="n">pred_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">BERT_predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_id</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">BERT_predictions</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">BERT_predictions</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>We can now make a comparison of the model’s predictions with the human judgments.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">precision</span>    <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>   <span class="n">support</span>

           <span class="mi">0</span>       <span class="mf">0.66</span>      <span class="mf">0.84</span>      <span class="mf">0.74</span>        <span class="mi">50</span>
           <span class="mi">1</span>       <span class="mf">0.69</span>      <span class="mf">0.45</span>      <span class="mf">0.55</span>        <span class="mi">40</span>

    <span class="n">accuracy</span>                           <span class="mf">0.67</span>        <span class="mi">90</span>
   <span class="n">macro</span> <span class="n">avg</span>       <span class="mf">0.67</span>      <span class="mf">0.65</span>      <span class="mf">0.64</span>        <span class="mi">90</span>
<span class="n">weighted</span> <span class="n">avg</span>       <span class="mf">0.67</span>      <span class="mf">0.67</span>      <span class="mf">0.65</span>        <span class="mi">90</span>
</code></pre></div></div> <p>From here on we would like to understand on what basis, that is, due to which tokens BERT made its predictions. Before we move on, let us have a quick look at how the BERT model would embed a given example sentence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0094</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0187</span><span class="p">,</span>  <span class="mf">0.0148</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0247</span><span class="p">,</span>  <span class="mf">0.0179</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0125</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0353</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0362</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0270</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0264</span><span class="p">,</span>  <span class="mf">0.0183</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0400</span><span class="p">,</span>  <span class="mf">0.0027</span><span class="p">,</span>  <span class="mf">0.0385</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="mf">0.0006</span><span class="p">,</span>  <span class="mf">0.0448</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0541</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0053</span><span class="p">,</span>  <span class="mf">0.0424</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0058</span><span class="p">,</span>  <span class="mf">0.0142</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0847</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0209</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0086</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0202</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0561</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0209</span><span class="p">,</span>  <span class="mf">0.0158</span><span class="p">]],</span>
       <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">mps:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">13</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>
</code></pre></div></div> <h2 id="deriving-explanations-through-attributions-with-lime">Deriving explanations through attributions with LIME</h2> <p>LIME analyzes how changes to the input affect the model’s predictions. It creates a local linear approximation for each instance by making small modifications to the input and observing the resulting changes in output. The weights from this linear model indicate the importance of each token, serving as saliency scores. The original paper (Ribeiro et al. 2016) introducing LIME can be found here: <a href="https://dl.acm.org/doi/10.1145/2939672.2939778">https://dl.acm.org/doi/10.1145/2939672.2939778</a>.</p> <p>In this part of the project, we will make use of the <a href="https://captum.ai">Captum</a> library to investigate how the BERT model made its predictions. Captum is built on PyTorch and an excellent library for many explainability methods in machine learning. As we want to implement LIME for text, the following approach is built upon the Captum <a href="https://captum.ai/tutorials/Image_and_Text_Classification_LIME">tutorial</a> for LIME. For analyzing texts, we need to use the LimeBase version. The documentation can be found here: <a href="https://captum.ai/api/lime.html">https://captum.ai/api/lime.html</a>.</p> <p>Captum’s LimeBase requires to define several parts of the LIME approach manually. We shall start with this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encode text indices into latent representations &amp; calculate cosine similarity
</span><span class="k">def</span> <span class="nf">exp_embedding_cosine_distance</span><span class="p">(</span><span class="n">original_inp</span><span class="p">,</span> <span class="n">perturbed_inp</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    
    <span class="n">original_inp</span> <span class="o">=</span> <span class="n">original_inp</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>
    <span class="n">perturbed_inp</span> <span class="o">=</span> <span class="n">perturbed_inp</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>

    <span class="c1"># We want to compare the BERT embeddings from the final hidden states and use their mean
</span>    <span class="c1"># LIME will ablate some of the input tokens and the left over is perturbed_inp; we embed this with our BERT model and make a comparison through cosine similarity with the original embedded input
</span>
    <span class="c1"># The model internally adds position embeddings based on the input length and assumes a default sequence starting from position 0 
</span>    <span class="n">original_emb</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">original_inp</span><span class="p">,</span> <span class="bp">None</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">perturbed_emb</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">perturbed_inp</span><span class="p">,</span> <span class="bp">None</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">F</span><span class="p">.</span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">original_emb</span><span class="p">,</span> <span class="n">perturbed_emb</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">distance</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># One hot encoding for masking tokens randomly for ablation
</span><span class="k">def</span> <span class="nf">bernoulli_perturb</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>  <span class="c1"># Amount of masked tokens
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

    <span class="c1"># Ensuring at least one token remains (by forcing the first token to stay) because the BERT model expects an input of at least one token
</span>    <span class="k">if</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))]</span> <span class="o">=</span> <span class="mi">1</span>  

    <span class="k">return</span> <span class="n">mask</span>

<span class="c1"># Removing absent token based on the intepretable representation sample
</span><span class="k">def</span> <span class="nf">interp_to_input</span><span class="p">(</span><span class="n">interp_sample</span><span class="p">,</span> <span class="n">original_input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">original_input</span><span class="p">[</span><span class="n">interp_sample</span><span class="p">.</span><span class="nf">bool</span><span class="p">()].</span><span class="nf">view</span><span class="p">(</span><span class="n">original_input</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Captum's customizable Lime class
</span><span class="n">LIME</span> <span class="o">=</span> <span class="nc">LimeBase</span><span class="p">(</span>
    <span class="n">predict</span><span class="p">,</span>
    <span class="n">interpretable_model</span><span class="o">=</span><span class="nc">SkLearnLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.08</span><span class="p">),</span>
    <span class="n">similarity_func</span><span class="o">=</span><span class="n">exp_embedding_cosine_distance</span><span class="p">,</span>
    <span class="n">perturb_func</span><span class="o">=</span><span class="n">bernoulli_perturb</span><span class="p">,</span>
    <span class="n">perturb_interpretable_space</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">from_interp_rep_transform</span><span class="o">=</span><span class="n">interp_to_input</span><span class="p">,</span>
    <span class="n">to_interp_rep_transform</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</code></pre></div></div> <p>Everything is in place to loop through the data’s hateful and non-hateful sentences, deriving attributions with our custom LimeBase class.</p> <p>The parameter <code class="language-plaintext highlighter-rouge">show_progress</code> is a widget that might not work in all IDEs right away; I had to adjust my vscode settings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Saving all attributions to this list for later comparisons
</span><span class="n">LIME_attributions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="n">LIME</span><span class="p">.</span><span class="nf">attribute</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">encoding</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="c1"># Adding batch dimension for Captum
</span>        <span class="n">target</span><span class="o">=</span><span class="n">BERT_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="c1"># This is a hyperparameter that turned out to work well when set to 200 for our task
</span>        <span class="n">show_progress</span><span class="o">=</span><span class="bp">False</span>  <span class="c1"># vscode is not properly showing progress, must edit vscode's settings.json for widgets (add: "jupyter.widgetScriptSources": ["jsdelivr.com", "unpkg.com"]); if problems persist, set to False
</span>    <span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">LIME_attributions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attrs</span><span class="p">.</span><span class="nf">tolist</span><span class="p">())</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>We can have a look at the attributions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">LIME_attributions</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2673813998699188</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16184958815574646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0753924623131752</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2013273388147354</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.452242910861969</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0483330637216568</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11272305995225906</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.14384473860263824</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">...]</span>
</code></pre></div></div> <p>The original data collection from Alacam et al. (2024) contained instances that were later dropped. This led to unique sentence numbers from 1 to 93. To avoid mismatches with the human rationales for given sentences, we should create a list of the actual sentence numbers from the experiment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_sentence_num</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">.</span><span class="nf">keys</span><span class="p">()))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">unique_sentence_num</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
</code></pre></div></div> <p>Let’s save the attributions together with their unique ids and sentences so we can later load them for our evaluation with respect to the human rationales.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">encoding</span><span class="p">)</span> <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">encodings</span><span class="p">]</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">LIME_attributions</span><span class="p">):</span>
    <span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">attr</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_LIME_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">BERT_LIME_attributions</span> <span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
</code></pre></div></div> <h2 id="deriving-explanations-through-attributions-with-saliency">Deriving explanations through attributions with Saliency</h2> <p>It is reasonable to use another explainability method because these methods can differ with respect to models and datasets (consider Atanasova et al. (2020)). In what follows, we will implement Saliency for our BERT model and data. The method was introduced in this paper (Simonyan et al. 2013): <a href="https://arxiv.org/abs/1312.6034">https://arxiv.org/abs/1312.6034</a>.</p> <p>Saliency is a gradient based explainability approach. Gradient-based methods generate saliency maps by calculating the gradient of the output with respect to the input. Unfortunately, it requires a little different set-up for our BERT model. We need to change our predict function for BERT; instead of letting it make predictions from the encodings, it needs to base its predictions on the embeddings from the final hidden states of the BERT model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_new</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">BERT</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>As with the Lime class we assign the forward function <code class="language-plaintext highlighter-rouge">predict_new(embeddings, mask)</code> to our Saliency object.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SALIENCY</span> <span class="o">=</span> <span class="nc">Saliency</span><span class="p">(</span><span class="n">predict_new</span><span class="p">)</span>
</code></pre></div></div> <p>If we use the embeddings for the model’s predictions, we’ll also need to handover masks as they are usually calculated based on the encodings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_SALIENCY</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">cls</span><span class="p">):</span>
    
    <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="nb">input</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">attrs</span> <span class="o">=</span> <span class="n">SALIENCY</span><span class="p">.</span><span class="nf">attribute</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>  <span class="c1"># Feeding the embeddings to the predict_new() function
</span>        <span class="n">target</span><span class="o">=</span><span class="n">cls</span><span class="p">,</span> 
        <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">masks</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Passing masks to predict_new()
</span>    
    <span class="c1"># L2 normalization for the attributions of Saliency
</span>    <span class="n">attributions</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attributions</span>
</code></pre></div></div> <p>We can now run through the data as with LIME and derive attributions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">cls</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">encodings</span><span class="p">),</span><span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">attr</span> <span class="o">=</span> <span class="nf">calculate_SALIENCY</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">cls</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attr</span><span class="p">.</span><span class="nf">tolist</span><span class="p">())</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Let’s save the attributions to another file.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_SALIENCY_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">BERT_SALIENCY_attributions</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
</code></pre></div></div> <h2 id="evaluating-the-results-human-vs-bert-rationales">Evaluating the results (human vs. BERT rationales)</h2> <p>Finally, we want to make comparisons between the tokens that were relevant for BERT’s hate detection and those that were considered as relevant by the annotators. We are interested in how well the model’s attributions align with the human rationales. We can use both explainability methods to check for the alignment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_LIME_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_SALIENCY_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>

<span class="c1"># Converting string keys to integers
</span><span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="p">{</span><span class="nf">int</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">BERT_LIME_attributions</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
<span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="p">{</span><span class="nf">int</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">BERT_SALIENCY_attributions</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
</code></pre></div></div> <p>We need to convert the subword tokens into full words. This requires a customized function for the dataset. It’s not perfect, but shall suffice for most instances.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">subwords</span><span class="p">,</span> <span class="n">subword_rationales</span><span class="p">):</span>

    <span class="n">human_level_rationales</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="n">word_rationale</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">subword</span><span class="p">,</span> <span class="n">rat</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">subwords</span><span class="p">,</span> <span class="n">subword_rationales</span><span class="p">)):</span>

        <span class="k">if</span> <span class="n">subword</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">[CLS]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">[SEP]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">]:</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word_rationale</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">rat</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nf">any</span><span class="p">([</span><span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">##</span><span class="sh">'</span><span class="p">),</span>
                        <span class="n">subword</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">#</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">],</span>
                        <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> 
                        <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">#</span><span class="sh">"</span><span class="p">]):</span>
                <span class="c1"># Mean from subwords will be added to human level rationales
</span>                <span class="n">human_level_rationales</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">word_rationale</span><span class="p">))</span>
                <span class="n">word_rationale</span> <span class="o">=</span> <span class="p">[]</span>
            
    <span class="k">return</span> <span class="n">human_level_rationales</span>
</code></pre></div></div> <p>We can evaluate the alignment in terms of the Pearson correlation coefficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
       
    <span class="n">corrs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
        <span class="c1"># Zero variance check because some attributions might be only zeros
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">corrs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Default to 0 correlation
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">corrs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">pearsonr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">mean_correlation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">corrs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mean_correlation</span>  
</code></pre></div></div> <p>To calculate the correlation, we focus on instances where the model’s prediction matches the majority human annotation. Specifically, we analyze local explanations only for sentences where the model made a correct prediction, allowing us to assess how closely its rationales correspond to the average human rationales.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">human_rationales</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">correct_prediction_labels</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">BERT_rationales_LIME</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">BERT_rationales_SALIENCY</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">gold</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">):</span>

    <span class="n">sent_id</span> <span class="o">=</span> <span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gold</span> <span class="o">==</span> <span class="n">pred</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gold</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">].</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">].</span><span class="nf">values</span><span class="p">())</span>

        <span class="n">LIME_attrs_word_level</span> <span class="o">=</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># LIME comes with positive and negative attributions; we're only interested in the positive ones because these are correlated with the model's targets
</span>        <span class="n">p_LIME</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">LIME_attrs_word_level</span><span class="p">]</span>

        <span class="n">p_SALIENCY</span> <span class="o">=</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">human_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="n">correct_prediction_labels</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">gold</span>
        <span class="n">BERT_rationales_LIME</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_LIME</span>
        <span class="n">BERT_rationales_SALIENCY</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_SALIENCY</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Finally, we can compute the Pearson correlation coefficient for LIME attributions…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> <span class="n">BERT_rationales_LIME</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">float64</span><span class="p">(</span><span class="mf">0.14380772549112125</span><span class="p">)</span>
</code></pre></div></div> <p>… and for Saliency attributions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">float64</span><span class="p">(</span><span class="mf">0.6632999675122192</span><span class="p">)</span>
</code></pre></div></div> <p>LIME’s sampling includes an element of randomness; therefore, it would be reasonable to repeat the LIME attribution several times to arrive at average results.</p> <p>With five repititions, I get a positive correlation of 0.17 using LIME, which suggests a weak correlation between the BERT and human rationales. For Saliency, I have a much stronger colleration of 0.66. Now we can see how much influence also the chosen explainability method has. For LIME and Saliency, the BERT model has been the same. However, the BERT model appears much more aligned when seen from the perspective of the Saliency attributions.</p> <h2 id="visualizing-the-results-alignment">Visualizing the results (alignment)</h2> <p>Finally, we can visualize our results to see how the explainability methods make attributions with respect to the BERT model and its predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_triple_heatmaps</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">,</span> <span class="n">data3</span><span class="p">,</span> <span class="n">xticklabels</span><span class="p">,</span> <span class="n">title1</span><span class="p">,</span> <span class="n">title2</span><span class="p">,</span> <span class="n">title3</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Creating three vertical subplots
</span>    
    <span class="c1"># Plotting the first heatmap (top) - Human rationales
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data1</span><span class="p">]),</span> 
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span>
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Human rationales</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
    
    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title1</span><span class="p">)</span>
    
    <span class="c1"># Plotting the second heatmap (middle) - Saliency attributions
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data2</span><span class="p">]),</span>
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span> 
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span> 
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Saliency</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
    
    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title2</span><span class="p">)</span>

    <span class="c1"># Plotting the third heatmap (bottom) - LIME attributions
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data3</span><span class="p">]),</span> 
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span> 
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span> 
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">LIME</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">)</span>
    
    <span class="n">ax3</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title3</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>  <span class="c1"># Adjusting layout to prevent overlap
</span>    
    <span class="c1"># Saving the figure
</span>    <span class="k">if</span> <span class="n">save_path</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>We can define a directory path here where we will save our visualizations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dir_path_visual</span> <span class="o">=</span> <span class="sh">"</span><span class="s">visualizations</span><span class="sh">"</span>
</code></pre></div></div> <p>We loop through the rationales and attributions to create heatmaps for the words from the experiment’s sentences.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_and_plot_heatmaps</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">,</span> <span class="n">correct_prediction_labels</span><span class="p">,</span><span class="n">BERT_rationales_SALIENCY</span><span class="p">,</span> <span class="n">BERT_rationales_LIME</span><span class="p">,</span> <span class="n">experiment_sents</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>  <span class="c1"># Creating output directory if it doesn't exist
</span>    
    <span class="k">for</span> <span class="n">sno</span> <span class="ow">in</span> <span class="n">human_rationales</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>

        <span class="n">hate_data</span> <span class="o">=</span> <span class="n">human_rationales</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>
        <span class="n">saliency_data</span> <span class="o">=</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>
        <span class="n">lime_data</span> <span class="o">=</span> <span class="n">BERT_rationales_LIME</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Plotting heatmap for rationales, saliency, and LIME for given sno: </span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                        
        <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">experiment_sents</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">strip</span><span class="p">(</span><span class="sh">'</span><span class="s">.,!?</span><span class="sh">'</span><span class="p">).</span><span class="nf">split</span><span class="p">()</span>
                            
        <span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">hate_heatmap_key_</span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Hate</span><span class="sh">"</span> <span class="k">if</span> <span class="n">correct_prediction_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="sh">"</span><span class="s">No Hate</span><span class="sh">"</span>
                            
        <span class="nf">plot_triple_heatmaps</span><span class="p">(</span><span class="n">hate_data</span><span class="p">,</span> 
                            <span class="n">saliency_data</span><span class="p">,</span> 
                            <span class="n">lime_data</span><span class="p">,</span> 
                            <span class="n">xticklabels</span><span class="p">,</span> 
                            <span class="n">title1</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence No. </span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="s"> Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> 
                            <span class="n">title2</span><span class="o">=</span><span class="sa">f</span><span class="sh">""</span><span class="p">,</span>
                            <span class="n">title3</span><span class="o">=</span><span class="sa">f</span><span class="sh">""</span><span class="p">,</span>
                            <span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">process_and_plot_heatmaps</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">,</span> <span class="n">correct_prediction_labels</span><span class="p">,</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">,</span> <span class="n">BERT_rationales_LIME</span><span class="p">,</span> <span class="n">experiment_sents</span><span class="p">,</span> <span class="n">dir_path_visual</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/visualization_example_2-480.webp 480w,/assets/img/Explaining_BERT/visualization_example_2-800.webp 800w,/assets/img/Explaining_BERT/visualization_example_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/visualization_example_2.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="references">References</h2> <p>Özge Alacam, Sanne Hoeken, and Sina Zarrieß. 2024. Eyes don‘t lie: Subjective hate annotation and detection with gaze. In <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, pages 187–205, Miami, Florida, USA. Association for Computational Linguistics.</p> <p>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classification. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 3256–3274, Online. Association for Computational Linguistics.</p> <p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “why should i trust you?”: Explain- ing the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD ’16, page 1135–1144, New York, NY, USA. Association for Computing Machinery.</p> <p>Sarthak Roy, Ashish Harshvardhan, Animesh Mukherjee, and Punyajoy Saha. 2023. Probing LLMs for hate speech detection: strengths and vulnerabilities. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 6116–6128, Singapore. Association for Computational Linguistics.</p> <p>Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. <em>CoRR</em>, abs/1312.6034.</p> <p>The <strong>Jupyter notebook</strong> for this project can be found here: <a href="https://github.com/omseeth/explaining_BERT_with_LIME_Saliency">https://github.com/omseeth/explaining_BERT_with_LIME_Saliency</a></p>]]></content><author><name></name></author><category term="BERT"/><category term="LIME"/><category term="Saliency"/><category term="captum"/><category term="PyTorch"/><category term="explainable"/><category term="AI"/><summary type="html"><![CDATA[Tutorial with full code]]></summary></entry><entry><title type="html">How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch</title><link href="https://omseeth.github.io/blog/2024/CNN_sentiment_analysis/" rel="alternate" type="text/html" title="How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch"/><published>2024-12-27T10:00:00+00:00</published><updated>2024-12-27T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/CNN_sentiment_analysis</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/CNN_sentiment_analysis/"><![CDATA[<p>This tutorial is an introduction to <strong>Convolutional Neural Networks</strong> (CNNs) for <strong>sentiment analysis</strong> with PyTorch. There are already a few tutorials and solutions for this task by <a href="https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7">Gal Hever</a>, <a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/">Jason Brownlee</a>, or <a href="https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/3%20-%20Convolutional%20Neural%20Networks.ipynb">Ben Trevett</a>. However, they are either written in Keras or lack some explanations that I find essential for understanding the underlying mechanics of CNNs as well as their PyTorch specific implementations. I hope this tutorial will therefore help the interested reader to learn more about CNNs and how to implement them for NLP tasks, such as sentiment analysis.</p> <p>What follows is partially the result of a lecture given by <a href="https://bplank.github.io/">Barbara Plank</a> at LMU in 2024, whom I’d like to thank along with her lab <a href="https://mainlp.github.io/">MaiNLP</a> for letting me use some of their code. The theoretical part discussed in this tutorial is also highly indebted to <a href="https://github.com/rasbt/machine-learning-book">Raschka et al. (2022)</a>, whose book on machine learning is for me one of the best resources.</p> <p>This tutorial is divided into the following chapters:</p> <ul> <li><strong>Preliminaries</strong> imports</li> <li><strong>Section 1)</strong> Preprocessing the dataset</li> <li><strong>Section 2)</strong> Theoretical foundations of CNNs for classification</li> <li><strong>Section 3)</strong> Implementing a CNN with PyTorch</li> <li><strong>Section 4)</strong> Evaluation of results</li> </ul> <h2 id="preliminaries-imports">Preliminaries: imports</h2> <p>In this tutorial, we’ll be using several different libraries. The following imports shall become clear as we develop our project. I’m using Python <strong>3.12.0</strong>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">In case the packages are not installed on your machine, let</span><span class="sh">'</span><span class="s">s begin with 
installing the ones that we</span><span class="sh">'</span><span class="s">ll need for our task. We can run commands with an 
exclamation mark in Jupyter Notebooks.</span><span class="sh">"""</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span> <span class="n">transformers</span> <span class="n">tokenizers</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span> <span class="n">torch</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">trainers</span>
<span class="kn">from</span> <span class="n">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</code></pre></div></div> <h2 id="section-1-preprocessing-the-dataset">Section 1) Preprocessing the dataset</h2> <h4 id="loading-the-data">Loading the data</h4> <p>Our goal is to use data to train a model that can identify the sentiment of a given text instance. In other words, we’ll implement a classifier using supervised learning. The backbone of our sentiment classifier will be a CNN. The data we’re using is taken from <a href="https://huggingface.co/datasets/dair-ai/emotion">Saravia et al. (2018)</a>. It consists of English Twitter messages labeled with six “emotions”: <em>anger</em>, <em>fear</em>, <em>joy</em>, <em>love</em>, <em>sadness</em>, and <em>surprise</em>. The dataset is available on HuggingFace and we can load it with the <a href="https://pypi.org/project/datasets/">datasets package</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">dair-ai/emotion</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can have a quick look at how the dataset is splitted and structured.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">16000</span>
    <span class="p">})</span>
    <span class="n">validation</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
<span class="p">})</span>
</code></pre></div></div> <p>The labels are already numericalized and the text is lowercased, as we can see when inspecting a single instance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The label IDs correspond to the following label order:
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">sadness</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">joy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">anger</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fear</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">surprise</span><span class="sh">"</span><span class="p">]</span>

<span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">i didnt feel humiliated</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">im grabbing a minute to post i feel greedy wrong</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am ever feeling nostalgic about the fireplace i will know that it is still on the property</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am feeling grouchy</span><span class="sh">'</span><span class="p">],</span>
 <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
</code></pre></div></div> <p>To allow easier processing, it helps to bind the respective splits to different variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <h4 id="tokenization">Tokenization</h4> <p>Before we can use the data to train our model, we need to split up the sentences into tokens. We also want to batch our data so that the model can be trained with more instances at the same time. In this tutorial, we’ll do the tokenization and batching with custom functions.</p> <p>For our tokenization, we can use a subword tokenization algorithm that is called <a href="https://huggingface.co/learn/nlp-course/en/chapter6/5">Byte Pair Encoding (BPE)</a>, which was introduced in Sennrich et al. (2016) and is based on an algorithm introduced by Gage (1994). The motivation of this tokenization technique is to let the data decide what subword tokens we’ll have. Another advantage of BPE is that it allows us to deal with unknown words that won’t be part of the training vocabulary but might appear in test data. With BPE, unknown words can be decomposed into their respective subword parts and in this fashion still be processed.</p> <p>Instead of implementing the BPE tokenizer completely from scratch, we can use the <a href="https://pypi.org/project/tokenizers/">tokenizers package</a>. We also want to pad and truncate each text with a fixed sequence length. If text instances are short, we pad them with 0s; if they are long, we truncate them to the maximum length. This will make the training and inference processes easier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We want to begin with only 5000 words in our vocabulary and a fixed sequence 
# length of 64
</span><span class="n">vocab_n</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Initialize a tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="nc">BPE</span><span class="p">())</span>

<span class="c1"># We can have a preliminary splitting of the text on spaces and punctuations 
# with Whitespace()
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="nc">Whitespace</span><span class="p">()</span>

<span class="c1"># Padding instances to the right with 0s
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_padding</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># Truncating long sequences
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_truncation</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># We limit our vocabulary and train the tokenizer
</span><span class="n">tokenizer_trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="p">.</span><span class="nc">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_n</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">train_from_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">trainer</span><span class="o">=</span><span class="n">tokenizer_trainer</span><span class="p">)</span>
</code></pre></div></div> <p>After training our custom BPE tokenizer, we aim to transform the raw training data into vector representations, where the input is tokenized and converted into corresponding token IDs. Fortunately, the labels are already numerical. That said, we still need to convert these numerical representations into PyTorch tensors to enable the PyTorch model to process the training instances. To accomplish this, we can write a few custom functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to tokenize text and return corresponding token IDs as tensors.

    Args:
        text, str: Text instance from training data.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        Tensor: One-dimensional PyTorch tensor with token IDs.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">ids</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to return label as tensor.

    Args:
        label, int: Label from instance.
    Returns:
        Tensor: One-dimensional PyTorch tensor containing the label index.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Transforms input dataset to tokenized vector representations.

    Args:
        data, dict: Dictionary with text instances and labels.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        list: List with tensors for the input texts and labels.
    </span><span class="sh">"""</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        
        <span class="n">instances</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">instances</span>
</code></pre></div></div> <p>Let’s tokenize, pad, and truncate our datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">val_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">validation_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>We shall inspect the second training instance, which was:</p> <p><code class="language-plaintext highlighter-rouge">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake.</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="p">(</span><span class="nf">tensor</span><span class="p">([</span>   <span class="mi">8</span><span class="p">,</span>  <span class="mi">161</span><span class="p">,</span>  <span class="mi">103</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>   <span class="mi">55</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span> <span class="mi">1173</span><span class="p">,</span>   <span class="mi">36</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span>  <span class="mi">807</span><span class="p">,</span>  <span class="mi">587</span><span class="p">,</span> <span class="mi">1129</span><span class="p">,</span>
            <span class="mi">130</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>  <span class="mi">219</span><span class="p">,</span>  <span class="mi">382</span><span class="p">,</span>  <span class="mi">444</span><span class="p">,</span>  <span class="mi">197</span><span class="p">,</span> <span class="mi">2819</span><span class="p">,</span>   <span class="mi">42</span><span class="p">,</span>   <span class="mi">47</span><span class="p">,</span> <span class="mi">2670</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">]),</span>
    <span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <p>We can observe that “i” has token index 8. Also notice how “so” appears in the vector representation of the text with index 58 twice. The corresponding label for the sequence is 0 and would be “sadness”.</p> <h4 id="batching">Batching</h4> <p>Batching the instances will enable the model to process multiple instances simultaneously. To achieve this, let’s write a custom batching function. There are different ways to perform batching, which involves concatenating instances and dividing them into manageable groups. One convenient method is to use the <a href="https://pytorch.org/docs/main/generated/torch.stack.html">torch.stack()</a> function, which concatenates a sequence of tensors. However, all tensors must be of the same length. Since we already padded and truncated our text instances, this requirement is satisfied.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Batches input instances along the given size and returns list of batches.

    Args:
        instances, list: List of instances, containing a tuple of two tensors 
            for each text as well as corresponding label.
        batch_size, int: Size for batches.
        shuffle, bool: If true, the instances will be shuffled before batching.
    Returns:
        list: List containing tuples that correspond to single batches.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>

    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># We iterate through the instances with batch_size steps
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">instances</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>

        <span class="c1"># Stacking the instances with dim=0 (default value)
</span>        <span class="n">batch_texts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">batch_texts</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">batches</span>
</code></pre></div></div> <h2 id="section-2-theoretical-foundations-of-cnns-for-classification">Section 2) Theoretical foundations of CNNs for classification</h2> <p>After having prepared our data, we can start with implementing our model. In this tutorial, we’ll be using a Convolutional Neural Network (CNN), which was introduced in LeCun et al. (1989). The CNN bears resemblance to the Neocognitron (Fukushima 1980) as well as Time-Delay Neural Networks (Waibel et al. 1989). This section is dedicated to the theoretical background of CNNs. In <strong>Section 3</strong>, we’ll implement the model with PyTorch.</p> <h4 id="the-architecture-of-a-cnn">The architecture of a CNN</h4> <p>A CNN is a fully connected network that operates similar to Feedforward Neural Networks (FNNs) in a feedforward fashion. The input is processed along layers and forwarded to the final layer, which can be used for predictions. The idea of a CNN is to consecutively extract salient features, such as shapes in terms of their respective pixels, building an abstract feature hierarchy for any given input. In this fashion, the model will be aligned through its training to reoccurring patterns, which it can “recognize.” It’s no surprise that CNNs were invented for handwritten digit recognition in images.</p> <p>The basic architecture of a CNN is as follows: For any given input, the CNN uses <strong>convolutions</strong>, that is, a <strong>filter</strong> (also called <strong>kernel</strong>) technique to extract local features from the input (e.g., pixels). After this filter has <strong>shifted</strong> over the whole input, the extractions are combined together into <strong>feature maps</strong> and then transformed with <strong>pooling</strong>. Pooling helps with singling out dominant features and allows to reduce the dimensions of the feature maps. This process can be repeated multiple times where additional filters can shift again over the pooled output from previous convolutions. Finally, for the last part of the network a FNN is used to produce logits for the desired task.</p> <p>One CNN layer consists of one convolutional and one pooling layer.</p> <h4 id="filtering">Filtering</h4> <p>Filters shift over the input. If the input is one-dimensional, that is, a vector, then the filter is itself a one-dimensional, usually smaller vector. If the input is two-dimensional, a matrix, then the filter needs to be a matrix, too.</p> <p>For a one-dimensional input of length \(n=8\), such as \([2, 1, 0, 3, 6, 7, 9, 1]\), we can define a filter of size \(m=3\), such as \([1, -1, 0]\). As the shifting starts, we’ll multiply the first three entries of the input vector with the filter:\([2, 1, 0] * [1, -1, 0]^T = 1\) In this fashion, we obtain the first entry for our feature map: \(y[0]=1\). Next, we shift the filter one step forward (with <strong>stride</strong>=1) and repeat the multiplication, until the filter reaches the end of the input, bounded by its last index (see <strong>Fig. 1</strong>). We can formalize this operation between the filter vector <em>f</em> and the input vector <em>e</em> as follows:</p> \[e*f\rightarrow y[i] = \sum_{k=1}^m{e[i+k-1]\cdot f[k]}\] <p>In fact, the filter size is a hyperparameter that can be set to any positive integer smaller than the input length. In those scenarios where the filter is very large and exceeds the input, we can pad the input with additional elements so that the filter operation would be possible. There a several padding strategies to avoid that elements from the middle of the input will be covered more frequently by the filter than those from the edges.</p> <p>In our implementation (<strong>Section 3</strong>), we’ll be using the <em>same</em> padding strategy, which adds zeros to the input on all sides so that the output of the filter operation can have the same size as the input. Therefore, the amount of padding will depend on the size of the filter(/kernel). Consider Raschka et al. (2022, p. 457) for further details.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_1d-480.webp 480w,/assets/img/CNN_sentiment/convolution_1d-800.webp 800w,/assets/img/CNN_sentiment/convolution_1d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/convolution_1d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1</strong>: One-dimensional convolution</p> <p>The convolution described previously for one-dimensional inputs works in exactly the same way for two-dimensional inputs (see <strong>Fig. 2</strong>). If \(E_{n_1\times n_2}\) is the input matrix and \(F_{m_1\times m_2}\) the filter where \(m_1\leq n_1\) and \(m_2\leq n_2\), for each stride we can compute our feature map as follows:</p> \[E*F\rightarrow y[i][j] = \sum_{k_1=1}^{m_1}\sum_{k_2=1}^{m_2}{e[i+k_{1}-1][j+k_{2}-1]\cdot f[k_1][k_2]}\] <p>with \(y[i][j]\) corresponding to the respective feature representation in the map, which is also a matrix.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_2d-480.webp 480w,/assets/img/CNN_sentiment/convolution_2d-800.webp 800w,/assets/img/CNN_sentiment/convolution_2d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/convolution_2d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2</strong>: Two-dimensional convolution</p> <h4 id="subsampling-layers-pooling">Subsampling layers: pooling</h4> <p>The idea of pooling layers is to reduce the size of the feature map and abstract from the features, either by picking out one local maximum feature value or averaging over a group of feature values. The former strategy is called max pooling, the latter is called average pooling. We’ll focus on max pooling (see <strong>Fig. 3</strong>).</p> <p>A pooling layer can be defined as \(P_{n_1 \times n_2}\), where the subscript indicates the area size over which the max operation is performed, while shifting across the entire map.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/max_pooling-480.webp 480w,/assets/img/CNN_sentiment/max_pooling-800.webp 800w,/assets/img/CNN_sentiment/max_pooling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/max_pooling.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3</strong>: Max pooling</p> <h4 id="adding-activation-functions">Adding activation functions</h4> <p>A FNN usually consists of a layer that is defined as \(z=Wx + b\) where \(W\) are the weights and \(b\) is an extra bias for input \(x\). Such a layer is often wrapped by a non-linear activation function \(\sigma\), as in \(A=\sigma (z)\). We adapt the convolutional layer so that it resembles the previous FNN construction. Let \(c = E*F + b\) be our layer with an additional bias term \(b\). As with the FFN we can wrap an activation function \(\sigma\) around \(c\). In many CNN implementations, the ReLu activation function is used for this purpose.</p> <p>LeCun et al. (1989) published their paper titled ‘Handwritten Digit Recognition with a Back-Propagation Network.’ Let us now examine which parts of a CNN are trainable using back-propagation. It’s the convolutional layers (filters) with their weights \(F\) and biases \(b\) (if included) that can be updated through back-propagation, guided by a loss function and gradient optimization. Recall that most CNNs also include a final fully connected feedforward neural network (FNN) layer for producing the output, and this layer is trainable as well. However, the pooling layer is not involved in the training process, as it does not contain learnable parameters.</p> <h4 id="multiple-channels">Multiple channels</h4> <p>The input to a CNN can be greater than one. If we’re dealing with images that are encoded in terms of RGB colors, we can split up the input into three two-dimensional matrices corresponding to red, green, and blue color information and feed them to the CNN. Each input matrix would be called one <strong>channel</strong>. For this matter, in most CNN implementations the convolutional layers expect an input with <strong>3 dimensions</strong> where \(E_{n_1\times n_2 \times c_{in}}\) would be the input of dimensions \(n_1\times n_2\) times \(c_{in}\), such as \(c_{in} = 3\) for three different color matrices.</p> <p>The further processing of the input allows a lot of variability: Each \(c_{in}\) input will have its own filter. The filters can be also stored as three-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}}\). Usually, the filtered results from each respective input will be element-wise added to create the output feature map. But sometimes it also helps to have multiple feature maps as outputs to capture different aspects from the input. In this case, the filters can be changed to four-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}\times c_{out}}\) where \(c_{out}\) determines the numbers of feature maps that we want (see <strong>Fig. 4</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/four_dim_filter-480.webp 480w,/assets/img/CNN_sentiment/four_dim_filter-800.webp 800w,/assets/img/CNN_sentiment/four_dim_filter-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/four_dim_filter.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4</strong>: Multiple filters (kernels) for three input channels, producing four feature maps</p> <p>Note, if we have for example four feature maps, we’ll also need four pooling layers. As the size of convolutional layers changes we’ll also have more parameters to train. This being said, remember that a FNN processing an input image would require weights of same size for its first layer. In contrast, the parameters of CNNs are bounded by the filter size. LeCun et al. (1989, p. 399) describe this aspect as “weight sharing” and note as one distinctive feature of CNNs that they significantly reduce the amount of parameters to be trained.</p> <h2 id="section-3-implementing-a-cnn-with-pytorch">Section 3) Implementing a CNN with PyTorch</h2> <p>Let’s begin with defining our classifier model. It’s reasonable to use embeddings for our input tokens. For that matter, our model should start with an embedding layer. The embeddings will be handed over to our CNN, which will consist of two convolutional layers with two different filters(/kernels). Finally, the input abstracted throughout the network needs to be passed to a final FFN layer for our sentiment prediction.</p> <p>To build an intuition for the whole process, let’s think again of the input. The input is a sequence (a tweet) from our data. This sequence will be translated into its correspoding token representation based on our custom trained BPE encoder. In other words, any incoming sequence will be a tensor of different indices. Next, the indices are handed over to our embedding layer, which serves as a look-up table for all tokens in the vocabulary and assigns the embedding representation to the input indices. This layer is a simple linear layer that is going to be trained along with the whole network later on. The initial values of the embeddings are random numbers.</p> <p>Up to this point, we can think of two strategies to run a first convolution over the input. If the input is 12 tokens long and each token has an embedding vector of size 300, the input to a potential convolutional layer would be \(12 \times 300\). The first strategy could be to apply a two-dimensional filter striding over this input matrix. The second strategy would be to use a one-dimensional filter that is going over one embedding dimension per time for the whole sequence, that is, over \(12 \times 1\) but with 300 channels for all embedding dimensions. If we want the output to be of the same length as the input, that is, 12, we need to pad the input to adjust for the filter length.</p> <p>In our implementation, we shall try the second strategy. Remember that the input channels will all be added together so before having our 300 embedding dimensions shrunk to one, we shall define an appropriate output channel size for the convolution. In other words, we’ll use as many as 100 filters for example to create an output of 100 dimensions for each respective input token.</p> <p>After having defined our model, we need to implement the training loop with a fitting method. To understand our training process we also need an evaluation mechanism. Finally, we need another method for making predictions from the trained classifier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN_Classifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    CNN for sentiment classification with 6 classes, consisting of an embedding 
    layer, two convolutional layers with different filter sizes, different 
    pooling sizes, as well as one linear output layer.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># We can implement embeddings as a simple lookup-table for given word 
</span>        <span class="c1"># indices
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_vocab_size</span><span class="p">(),</span> <span class="mi">300</span><span class="p">)</span>

        <span class="c1"># One-dimensional convolution-layer with 300 input channels, and 100  
</span>        <span class="c1"># output channels as well as kernel size of 3; note that the
</span>        <span class="c1"># one-dimensional convolutional layer has 3 dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with with a one-dimensional sliding window of length 3, 
</span>        <span class="c1"># reducing in this fashion the sequence length 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># The input will be the reduced number of maximum picks from the
</span>        <span class="c1"># previous operation; the dimension of those picks is the same as the
</span>        <span class="c1"># output channel size from self.conv_1. We apply a different filter of 
</span>        <span class="c1"># size 5.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with window size of 5
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># Final fully connected linear layer from the 50 output channels to the
</span>        <span class="c1"># 6 sentiment categories 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Defining the forward pass of an input batch x.

        Args:
            x, tensor: The input is a batch of tweets from the data.
        Returns:
            y, float: The output are the logits from the final layer.
        </span><span class="sh">"""</span>
        <span class="c1"># x will correspond here to a batch; therefore, the input dimensions of 
</span>        <span class="c1"># the embedding will be by PyTorch convention as follows:
</span>        <span class="c1"># [batch_size, seq_len, emb_dim]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Unfortunately the embedding tensor does not correspond to the shape 
</span>        <span class="c1"># that is needed for nn.Conv1d(); for this reason, we must switch its 
</span>        <span class="c1"># order to [batch_size, emb_dim, seq_len] for PyTorch
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We can wrap the ReLu activation function around our convolution layer
</span>        <span class="c1"># The output tensor will have the following shape: 
</span>        <span class="c1"># [batch_size, 100, seq_len]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Applying max pooling of size 3 means that the output length of the 
</span>        <span class="c1"># sequence is shrunk to seq_len//3
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Output of the following layer: [batch_size, 50, seq_len//3]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Shrinking the sequence length by 5
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>
        <span class="c1"># At this point we have a tensor with 3 dimensions; however, the final layer 
</span>        <span class="c1"># requires an input of size [batch_size x 50]. To get this value we can 
</span>        <span class="c1"># aggregate the values and continue only with their mean
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># In this fasion, the linear layer can be used to make predictions
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Gradient based fitting method with Adam optimization and automatic 
        evaluation (F1 score) for each epoch.

        Args:
            train_instances, list: List of instance tuples.
            val_instances, list: List of instance tuples.
            epochs, int: Number of training epochs.
            batch_size, int: Number of batch size.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">train_batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span>
                <span class="n">train_instances</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_batches</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            
            <span class="n">train_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">val_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">val_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> train F1 score: </span><span class="si">{</span><span class="n">train_f1</span><span class="si">}</span><span class="s">, validation F1 score: </span><span class="si">{</span><span class="n">val_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make inferences from the model.

        Args:
            input, tensor: Single instance.
        Returns:
            int: Integer for most probable class.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make evaluations against the gold standard (true labels) from the 
        data.

        Args:
            instances, list: List of instance tuples.
            batch_size, int: Batch size.
        Returns:
            float: Macro F1 score for given instances.
        </span><span class="sh">"""</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">true</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
            <span class="n">true</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">pred</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

        <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="sh">"</span><span class="s">macro</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can now train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="nc">CNN_Classifier</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">26</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">37.29</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.6349625340555586</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.5885997748733948</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">27</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">36.16</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">2</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9181883927660527</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8391332797173209</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">31</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">31.86</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">3</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9548322090243025</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8665508014136801</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">30</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">32.63</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">4</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9715161826479329</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.851259672136001</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">33</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">30.13</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">5</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9830196392925995</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8649424900902641</span>
</code></pre></div></div> <h2 id="section-4-evaluation-of-results">Section 4) Evaluation of results</h2> <p>After we have trained our model, we can use our test set to see how well it predicts sentiments for unseen examples.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f1_test</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">test_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">F1 score for test set: </span><span class="si">{</span><span class="n">f1_test</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">F1 score for test set: 0.8403978699590415</code></p> <h4 id="hyperparameters">Hyperparameters</h4> <p>There are many hyperparameters in the entire modeling process that we can adjust to achieve even better results. These include different layer sizes, filters, poolings, and more efficient optimization techniques. Typically, dropout is applied during the training of such models to prevent overfitting. It is also worthwhile to explore the possibility of using pre-trained embedding representations.</p> <p>The <strong>Jupyter notebook</strong> for this project can be found here: <a href="https://github.com/omseeth/cnn_sentiment_analysis">https://github.com/omseeth/cnn_sentiment_analysis</a></p> <h2 id="references">References</h2> <p>Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biol. Cybernetics 36</em>, pages 193–202.</p> <p>Gage, P. (1994). A new algorithm for data compression. <em>C Users J.</em>, 12(2):23–38.</p> <p>LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. (1989). Handwritten digit recognition with a back-propagation network. In Touretzky, D., editor, <em>Advances in Neural Information Processing Systems</em>, volume 2. Morgan-Kaufmann</p> <p>Saravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y.-S. (2018). CARER: Contextualized affect representations for emotion recognition. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 3687–3697, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Sebastian, R., Yuxi, L., and Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Erk, K. and Smith, N. A., editors, <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics</p> <p>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. and Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. In <em>IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37</em>, no. 3, pages 328-339.</p>]]></content><author><name></name></author><category term="NLP"/><category term="CNN"/><category term="sentiment"/><category term="analysis"/><category term="text"/><category term="classification"/><category term="PyTorch"/><category term="Deep"/><category term="Learning"/><category term="tutorial"/><summary type="html"><![CDATA[Tutorial with full code]]></summary></entry><entry><title type="html">AI ethics, a primer</title><link href="https://omseeth.github.io/blog/2024/AI_ethics_primer/" rel="alternate" type="text/html" title="AI ethics, a primer"/><published>2024-11-23T10:00:00+00:00</published><updated>2024-11-23T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/AI_ethics_primer</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/AI_ethics_primer/"><![CDATA[<p>Many of us have heard of ethics in AI before. But what does ethics actually mean? And what could we learn from the different ethical traditions, some of which that date more than two thousand years back, when dealing with AI? We will see that it is worthy to learn from the classics of philosophy to reflect upon AI.</p> <p>In this post, we will touch on three aspects:</p> <ul> <li>Deep dive: traditions in ethics</li> <li>Inspiration: philosophy of technology</li> <li>Application: challenges when using AI ethically</li> </ul> <h2 id="deep-dive-traditions-in-ethics">Deep dive: traditions in ethics</h2> <p><strong>Ethics and morality:</strong> Ethics is a sub-discipline of philosophy, theology, psychology, sociology, education, political science, or anthropology. It is often helpful to divide ethics into three areas: 1) <strong>descriptive ethics</strong> dealing with the empirical question which moral rules and norms exist in each community, 2) <strong>normative ethics</strong> which reflects on reasons for moral systems, and 3) <strong>metaethics</strong> questioning the language we use in moral discourse. For example, from a descriptive point of view, we could investigate what psychological factors affect our moral decision making. The predominant ethical traditions, however, adhere to a normative approach.</p> <p><strong>The three traditions of classical ethics:</strong> Throughout the history of ethics, three traditions have influenced the field. Today’s ethicist can often still be traced to one of these traditions. The first tradition which many have heard before is often referred to as <strong>utilitarianism</strong>. A utilitarian would try to maximize the utility for the greatest number of people. It is sometimes also described as the greatest happiness principle. The second tradition in ethics is closely tied to the German philosopher, Immanuel Kant, who introduced the <strong>categorial imperative</strong> as man’s and woman’s best guide for moral behavior. The imperative says: “Act only according to that maxim whereby you can at the same time will that it should become a universal law.” (Kant 1993, p. 30) In other words, Kant provides us with a tool with which we can check whether any action is moral. The third tradition is the oldest. It is linked to the Ancient Greek philosopher Aristotle. In contrast to the other two traditions, Aristotle’s ethics consists in multiple virtues (e.g. prudence and temperance) that he and many of his followers believe to be leading to <strong>the good life</strong>.</p> <p><strong>Three different perspectives:</strong> The three dominant traditions in ethics can also be divided by what aspect they focus on. For example, utilitarianism considers only the results of actions. What counts is the aftermath! A Kantian ethicist, in contrast, sets her eyes on each particular action. She wonders whether it is ethically licensed by the categorical imperative. A virtue ethicist is rather interested in what virtues lead us to a particular action.</p> <h2 id="inspiration-philosophy-of-technology">Inspiration: philosophy of technology</h2> <p><strong>I, technology, and the world:</strong> The philosopher Don Ihde (1990) introduces a simple, but powerful, <strong>abstraction</strong> that can help us reflect on technologies. Consider yourself looking through a microscope, seeing the wonders of the world from a different angle. As you observe bacteria on a little petri dish, you forget everything around yourself. In fact, you have morphed with the microscope. It has become part of your vision and even of yourself. Ihde (1990) describes this relation with the technology with this short formula:</p> <p><code class="language-plaintext highlighter-rouge">(I – Technology) – World</code></p> <p>Imagine you are in a warm room and the rain is pouring down on your window. As you are looking at the thermostat on your wall, you are seeing that it is almost freezing outside. When you become aware of the temperature, you are not feeling the cold. It is transmitted to you by the thermostat. Ihde (1990) describes this constellation as follows:</p> <p><code class="language-plaintext highlighter-rouge">I – (Technology – World)</code></p> <p>Differently put, the temperature of the outside world in such situations is only indirectly accessible to you through the measuring device on your wall.</p> <h2 id="application-challenges-when-using-ai-ethically">Application: challenges when using AI ethically</h2> <p><strong>Technological relations and moral decisions:</strong> When making decisions involving AI, it is helpful to understand how the technology is being used. For instance, are we merging with a driving assistant that is powered with AI? Or do we confront reality, as it is mediated to us by AI powered algorithms; for instance, when we use social media platforms that present a particular representation of what a pleasant vacation ought to entail? Realizing our <strong>position with which we relate to AI</strong> can then help us with understanding how AI would mediate and even change our actions. For instance, AI could help us with achieving the best outcome for the most people by ameliorating our capacity to react quickly in critical situations. But it could also deceive us when we would ask a chatbot whether something would be acceptable as a universal moral law. For an ethically desirable way of developing and using AI, we need to be aware of how it affects <strong>our actions</strong>.</p> <p><strong>The Other:</strong> Finally, Ihde also considers situations where we find ourselves confronted with technology that we perceive as another being. He describes this sort of relation as follows (Ihde 1990):</p> <p><code class="language-plaintext highlighter-rouge">I – Technology – World</code></p> <p>And as AI advances, we might be more inclined to also consider AI as such a being that should deserve more attention. But whether we will ever see this one day is yet to be seen!</p> <p>In this post, you have learned a basic understanding of ethics. You saw an abstraction from the philosophy of technology to help position yourself when using AI. With this knowledge, I hope you feel motivated to use AI ethically.</p> <h2 id="bibliography">Bibliography</h2> <p>Ihde, D. (1990). Technology and the Lifeworld – From Garden to Earth. Indiana University Press.</p> <p>Kant, I. (1993). Groundwork of the Metaphysics of Morals. Translated by Ellington, James W. (3rd ed.). Hackett.</p>]]></content><author><name></name></author><category term="AI"/><category term="ethics"/><category term="utilitarianism"/><category term="Kant"/><category term="virtue"/><category term="ethics"/><category term="Don"/><category term="Ihde"/><category term="philosophy"/><category term="of"/><category term="technology"/><summary type="html"><![CDATA[Introduction to AI ethics that’s less than 1000 words long]]></summary></entry><entry><title type="html">Responsibility Protraction with Remotely Controlled Combat Vehicles</title><link href="https://omseeth.github.io/blog/2024/responspibility_protraction/" rel="alternate" type="text/html" title="Responsibility Protraction with Remotely Controlled Combat Vehicles"/><published>2024-10-19T10:00:00+00:00</published><updated>2024-10-19T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/responspibility_protraction</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/responspibility_protraction/"><![CDATA[<p><strong>Abstract</strong> This paper deals with the phenomenon that there has been little to no prosecution of misconduct with remotely controlled combat vehicles (RCCVs), such as armed drones, although it is likely that RCCVs have been deployed unjustly. While responsibility gaps are considered as one reason for this apparent lack of prosecution, it is argued that there are no such gaps because no realistic cases of unintended, uncontrolled, physical outcomes, unbeknownst to their operators, are conceivable. Instead, lack of prosecution is explained with responsibility protractions: The involvement of many stakeholders makes it difficult to pin down one responsible individual, errors can slip in unbeknownst to all parties involved, and “surgical attacks” with drones may have been considered to be less significant as bombing by aircraft and therefore largely neglected. To address possible misconduct with RCCVs, this paper finally considers the following measures: (a) that RCCVs should be deployed as little as possible, (b) more effort must be spent on ascribing responsibilities, (c) conscientious engagement should be promoted among RCCV operators, and (d) a novel military convention ought to mandate that pilots be stationed in countries where RCCVs are being deployed, whenever there is no official declaration of war against these respective countries.\(^1\)</p> <h2 id="introduction">Introduction</h2> <p>One recurring theme in robotics and AI is responsibility gaps (Matthias 2004; Coeckelbergh 2020). It follows from the thought that we can build machines for whose action neither the machine nor its developer(s) can be fully held responsible. Consider a machine that has been trained from vast amounts of data and been let to run on its own, causing consequences that could not be predicted. Since the machine’s developer(s) would not be the physical source of its actions and could not fully predict its performance, they would be only responsible for the machine in a reduced sense. But the machine could neither be held responsible when we agree that a machine lacks the necessary capacities to regulate and adapt its output, based on a moral understanding of the situation and a propensity to foresee any consequences (Köhler 2020, 3127). Furthermore, if we consider responsibility to be a form of accountability, a machine would also be incapable of answering or demonstrating accountability for its output, as would a human being, because a machine is not an appropriate recipient for praise or punishment (Sparrow 2007, 71; Danaher 2016). These points, when considered together, indicate that autonomous machine actions cause a gap in the assignment of responsibility.</p> <p>In military ethics, responsibility gaps have been discussed by considering autonomous military robots, also known as lethal autonomous weapons systems (LAWS) (Sparrow 2007; Lokhorst and van den Hoven 2012; Leveringhaus 2016; Müller 2016; Oimann 2023). In these discussions, it is assumed that their operators are “on” or “out” of the loop. If they are on the loop, it means that the systems run autonomously but the operators can intervene, if necessary. If the operators are out of the loop, the deployed systems are fully autonomous and follow their own programs and data. In both respects, the human-robot constellations that were discussed in this fashion have anticipated scenarios that are still rare because no fully automated military robots are being deployed as of today. The Kargu-2 drone by the Turkish company STM appears to be a new exception.</p> <p>In contrast, responsibility gaps with existing remotely controlled combat vehicles (RCCVs), such as the MQ-9 Reaper drone, have not been considered to a similar degree.\(^2\) For it is assumed that their pilots are “in” the loop, that is, that they control the system and everything it produces. Hence, most people think that the operators who deploy and steer RCCVs, such as aerial drones, can be held responsible for the consequences.</p> <p>Since drones from the MQ series are still being used by various countries (e.g., by the U.S., the U.K., or the Netherlands) and there is little literature on responsibility gaps with RCCVs, another consideration of responsibility seems justified. But even more interesting is the fact that, to my knowledge, no one has been held liable for drone strikes that were done without just cause while ground troops are periodically sentenced for war crimes. In the United States, for example, court cases have resulted in no convictions. What is also striking is that all of them were brought to court by civil parties only. Some cases on excessive force with RCCVs were eventually dismissed by referring to the “political question doctrine”, assuming that drone strikes must first be politically addressed (Kaufman and Diakun 2017, 120). A path to the merits has also been barred by state-secrets privileges.\(^3\) Conversely, a notable war crime that ended in prison time were the “thrill killings” by U.S. infantry soldier Pvt 1st Class Andrew Holmes in Kandahar, Afghanistan, in 2011 (Fallon 2011). The mention of these examples, and legal liability more generally, may help us in what follows to better understand the apparent problem with RCCVs, as liability is also rooted in the attribution of responsibility.</p> <p>Another reason to discuss responsibility allocation with RCCVs is to gain a better understanding of who would be the source for possible misconduct with RCCVs. In military ethics, it is generally accepted that no excessive force should be used to achieve military goals (Lazar 2016). However, one would need to be able to call out those who have created more harm and pain than can be justified, that is, those who are responsible, if this standard were to have any force. Therefore, considering responsibility regarding RCCVs would also be important if we are to maintain military standards.</p> <p>Having now introduced some of the thoughts that lie behind this paper, let me briefly sum up what follows: I discuss responsibility gaps with Alex Leveringhaus’s Ethics and Autonomous Weapons (2016). Leveringhaus provides us with useful criteria that lead me to the conclusion that there are no responsibility gaps with RCCVs; for no realistic cases of unintended, uncontrolled, physical outcomes occur unbeknownst to their operators with contemporary RCCVs. However, I argue in the next section that the legal disinterest regarding misconduct with RCCVs could still be explained by considering responsibility. For the complexity of RCCV deployment leads to responsibility protraction. Responsibility is protracted when too many stakeholders are involved so that it is very difficult to pin down one responsible individual. Furthermore, accumulative actions might also create undesirable outcomes by allowing errors to slip in unbeknownst to all parties involved. Ultimately, “surgical attacks” may appear less significant in comparison to heavy bombing by aircraft and have been therefore mostly ignored.</p> <p>In the last section, I discuss several measures that could help with reducing possible misconduct: (a) RCCVs should be deployed as little as possible, (b) more effort must be spent on ascribing responsibilities, (c) conscientious engagement should be promoted among RCCV operators, with the aim to incorporate their views into decision making processes, and (d) the introduction of a new military convention should require operators to be stationed in those countries where RCCVs are being deployed, provided that their states have not officially declared war against the respective country where they carry out their missions.</p> <h2 id="responsibility-gaps-with-remotely-controlled-combat-vehicles">Responsibility Gaps with Remotely Controlled Combat Vehicles</h2> <p>There are many theories of responsibility, but there is no uniquely accepted definition of what it means to be responsible. For example, David Miller (2001) maintains that there are roughly four types of responsibility. According to Miller, we can be causally (1) and morally (2) responsible, as well as responsible due to certain capacities that we might have (3), or because of our affiliation with a community (4). Briefly summarized: 1) If I damage your bike, by kicking it, I am causally responsible for the damage. 2) If your bike gets stolen because I have forgotten to lock it after having it borrowed, I would be morally responsible for your loss: I am not the actual source of the larceny, so I am not causally responsible, but I have neglected my duty to secure your possession while it had been under my custody. Moral responsibility also implies for Miller the possibility of appraisal or blame. Causal responsibility, in turn, might not incur any blame or legal action because things can be done without intention. For example, I could damage someone’s property while having a seizure. 3) Moreover, Miller argues that people can be responsible because of their capacities. Take into consideration doctors who may be responsible for helping the sick due to their knowledge and training. 4) Finally, our affiliation with a group might imply certain responsibilities that are owed to fellow members, such as an obligation to pay taxes because one shares a political community with them.</p> <p>However, Miller’s variations on responsibility are not sufficient for addressing responsibility gaps in human-machine interaction. His types focus mainly on concepts that we can apply to human action, but which are ignorant of whether humans act with or without machines, or within or without complex machine systems. Therefore, it makes sense to enrich the concept of responsibility with another account that is better attuned to the human-machine-action that we find in RCCV’s deployments.</p> <p>In this regard, it is worthy to consider Alex Leveringhaus’s <em>Ethics and Autonomous Weapons</em> (2016). Leveringhaus suggests three points that must be fulfilled to allocate responsibility when machines are being used. Machines’ operators are responsible for any physical output: (1) if it is the result of the operator’s intent, (2) the machine action happens to her knowledge, and (3) when it is under her control (Leveringhaus 2016, 78-79). For these points to hold, it is generally assumed that the machines are tools, extending or enhancing their operators’ capacities. Where (1)-(3) do not apply, and we observe physical output, it is reasonable to speak of a situation where neither the operator behind the machine is responsible nor the machine itself for it is but a tool. In other words, in such a situation we would be confronted with a responsibility gap.</p> <p>At the outset, it is important to note that accidents might not be responsibility gaps in the sense I am trying to discuss here.\(^4\) If an operator loses control of her machine because of unforeseeable weather conditions, and if her machine crashes into a densely populated area, she might not be responsible for this outcome, or only in a reduced manner, even though some action had occurred with devastating results. This is so because it was arguably not in her power to alter the weather, nor the crash of her machine. We would excuse her, if she had spent due care on preparing the deployment of the machine, by assuring its functionality, by looking at weather reports, by following conventional security measures etc. And, in doing so, we would reach the conclusion that no one is ascribed responsibility, at least in the sense of culpability.</p> <p>There are further scenarios where machines could produce outcome without violating Leveringhaus’s principles. For example, a machine could evade its operators control and run against her intention when it is hacked or even repurposed by a third party. (Leveringhaus 2016, 78) In such a scenario, we would not speak of any responsibility gap because the machine would follow someone else’s intention and control. For example, the Australian company DroneShield Limited develops “Counter-Unmanned Aircraft Systems”, such as its DroneGun MK4, with the aim to disrupt signals to and from third party vehicles, in the hopes that the disrupted vehicle either lands on the spot or returns to its operator by default. In short, we may have situations where machines run against their operator’s intentions and go out of control, but we can still find a way to avoid talking about responsibility gaps in these situations.</p> <p>To press the issue, we need to consider whether RCCVs systematically produce unintended, uncontrolled, physical outputs, unbeknownst and unforeseeable to their operators. In this respect, Leveringhaus is not sure whether all three conditions need to be fulfilled to speak of responsibility gaps, or whether two of them might already be sufficient. However, he thinks that the first criterion, intent, is necessary: Only if RCCVs, such as armed drones, systematically run against their operators’ intention, we could think of gaps when trying to ascribe responsibility. For example, if we have a machine that nudges its operators to always shoot at innocent people against her will.</p> <p>Are there any situations where responsibility gaps realistically occur with armed drones? Lack of intent (1) seems unlikely. If drone missiles kept hitting the wrong targets, this might be the result of a bug or malicious programming. They would be reprogrammed to fully serve the commands of their operators.\(^5\) Likewise, military drones would not simply fly in the opposite direction as commanded by the operator. If that happens, it could be a sign of interference from a third party. In any case, drones would ideally be stopped before more unintended action would follow.</p> <p>Let us examine the knowledge-condition (2). We might want to ask: Do contemporary RCCVs lead to indiscriminate killings unbeknownst to their operators? This seems to be unlikely as well. It has not been the case that drones created a surge of massacres without the knowledge of those who have steered them, as they have already been used for more than a decade. As a result, we can conclude that drones, as far as we know, do not involve any unforeseeable physical actions.</p> <p>That said, two arguments that are discussed regarding unmanned warfare seem to be pressing this issue. The first is the so-called Threshold Argument. According to this argument, military technologies, such as armed drones, might lower the threshold for states to enter war because their deployment is less costly in terms of soldiers’ lives and money (Sparrow 2012; Galliott 2015). While I cannot go into a more detailed discussion of this argument, I would like to emphasize that it is possible to say that the deployment of armed drones has consequences that are not directly anticipated by their operators. In a similar fashion, it has been argued that asymmetric warfare can create unethical responses, such as guerrilla tactics or terror, because the technologically inferior side might have no other means to retaliate (Killmister 2008). Again, one can argue that RCCV deployment produces these outcomes in the long run, which would not be easily predicted.</p> <p>One might thus conclude that RCCV deployment violates Leveringhaus’s third criterion, that is, RCCVs could systematically produce long-term effects unbeknownst to their operators. However, it is far more difficult to prove for each RCCV deployment respectively that these vehicles are the primary causal source of such effects. If a drone is flown to a terrorist outpost with the goal to eliminate it, the possible outcomes that arise seem to be calculable. Additionally, pilots could introduce sufficient risk analyses, evaluating chances of failure. They could also include rather unlikely events to estimate whether their drone deployment is justified. Eventually, the operator would know whether the attack succeeds in damaging the terrorist’s infrastructure or even in eliminating the terrorist if the drone’s telecommunication works properly. If not, she could always adapt its action. It appears that such a drone mission would not lead to any outcomes that could not be anticipated after careful consideration.</p> <p>When considering Leveringhaus’s last criterion, we observe that contemporary military drones are not sophisticated enough to undermine their operators’ control. If connection is lost and they run out of energy, they would land on the spot or crash. Some models are also equipped with the capacity to automatically return to the point from where they were started whenever telecommunication with the operator is interrupted. As with many other machines, they require the operator to be always in the loop. Hence, everything contemporary RCCVs produce is controlled by their operators, unless auto-pilots and other automated control mechanism are being used extensively. But in this paper, I want to focus on those RCCVs that are not automated yet. A discussion of automated military robots would entail many more issues concerning control, intention, and anticipation on the operator’s side.</p> <p>For the reasons given, it is difficult to claim that contemporary RCCVs systematically lead to responsibility gaps. If unintended, uncontrolled action occurs with RCCVs, unbeknownst to their operators, it would be likely due to accidents or due to insufficient foresight or control from their operators or from those who deploy them, thus making the operators or deployers responsible. Putting such exceptions aside, action by RCCVs can indeed be linked to the decisions, control, and knowledge of their operators. If that is the case, namely, if there is no responsibility gap with contemporary RCCVs, one is further justified to conclude that their operators would be responsible for whatever consequences the RCCVs might cause.</p> <h2 id="responsibility-protractions-with-remotely-controlled-combat-vehicles">Responsibility Protractions with Remotely Controlled Combat Vehicles</h2> <p>Given the number of missions and lethal strikes that have taken place with RCCVs, predominantly with armed drones, chances are high that abuses have occurred. Why has there been no conviction? One explanation could be that the military suppresses any misapplication of drones. There might have been abuses and unjust drone strikes, but the misconduct has not been addressed within the military apparatus. One recent investigation by the New York Times, How the U.S. Hid an Airstrike That Killed Dozens of Civilians in Syria, points into this direction (Philipps and Schmitt 2021). While the article focuses on aircraft, it would make sense that similar attempts of hiding and neglecting misconduct in the military has happened with armed drones. But why, on the other hand, are infantry soldiers, such as Andrew Holmes, prosecuted and convicted for war crimes? Another explanation would be that states may knowingly and willfully engage in unjust and abusive conduct, overriding international law, to exercise their power without regard for legal consequences. The justifications for dismissing litigations involving armed drones in the U.S. would suggest such a position. Yet there are states that could be said to subscribe to this stance, and which neither ratify the Rome Statute of the International Criminal Court, like the United States of America, but which still hold some of their soldiers responsible for excessive and unlawful force. Given these circumstances, I think it is possible to provide a third explanation for the lack of prosecution and conviction for unjust and abusive drone strikes.</p> <p>Another possible explanation could be that armed drones result in a phenomenon I would like to refer to as responsibility protraction, which can muddy the waters of our ethical assessments and comprehensive legal proceedings. The idea of protraction is not novel. For example, it is extremely difficult to find the right kind of addressees in large corporations when the business has been involved in criminal action (Nucci and de Sio 2016). The complex way of how responsibilities are shared within large institutions and how the minute acts of each employee that add up to what the institution eventually “does” make it difficult to pin down one person who is responsible for the outcome. This difficulty has also been called the Problem of Many Hands (Thompson 1980; van de Poel et al. 2012). Although one would assume that politicians or managers are the kind of persons in charge, examples of corporate scandals have shown that they can successfully exempt themselves from taking responsibility, by claiming that things were done unbeknownst to them. On a similar note, we can assume that the deployment of armed drones happens within a complex institution, involving many people: ground personnel fueling and repairing drones, pilots steering and targeting, officers and intelligence service agents discussing who will be killed next. If Jo Becker and Scott Shane (2012) are right, there are up to 100 individuals involved in such debates, including generals, a minister of defense, or even the president, who might sign the final verdict to kill suspects with drones. In fact, there might be so many people involved that it becomes almost impossible to evenly distribute responsibilities among all stakeholders. Arguably, this is already the case with every military mission and even more so, as more sophisticated technologies are involved. But the crucial point is to be aware that RCCVs, such as armed drones, would add a further layer to this situation.</p> <p>Due to this complexity, we may not blame a drone pilot for unjust strikes in the same sense as a combat soldier. While one can assume that eventually the pilot would be causally responsible for the strikes, which he sets off by clicking or pressing a button, as he is the last in the chain of command and hence physically involved in the action of the machine, we can say with Miller (2001) that he is not fully morally responsible for the killing to a similar degree as a soldier would be by shooting her rifle. For the drone pilot has had less leeway in applying his own interpretations to the combat. We have seen that the decision who is going to be killed entails in contemporary drone warfare the involvement of many agents, whereby the final verdict to attack a target is put out of the hands of the pilots. To put it more bluntly, pilots might be just the final cog within the military complex, executing orders without much chance for considering situations based on their own application of military standards.</p> <p>Infantry soldiers, in contrast, seem to have more leeway as to what they will do and whom they will kill, if they deem it justified. Consider Sam Mendes’s film 1917. Two British soldiers, Lance Corporal William “Will” Schofield (George MacKay) and Lance Corporal Thomas “Tom” Blake (Dean-Charles Chapman), are sent to the front to warn their troops of a German ambush. After having passed the first trench, Will and Tom reach a small, abandoned farm behind the lines. While they search for German outposts, they observe an airplane battle that ends with a German plane crashing into the barn. Tom decides to help the severely wounded pilot, holds him in his hands, and feeds him water from his own bottle. While Will continues his search, the next scene reveals how the German stabs Tom. The film presents us with how differently soldiers can treat the enemy. While Tom shows mercy, the pilot decides to take another life with his own. Finally, after Will discovers what happened, he shoots the German.</p> <p>What is also worthy keeping in mind is that Tom and Will could not only spare the German’s life, but they would also have the chance to take him prisoner, which was likely Tom’s intention when he decided to help him. Drone operators, on the other hand, do not have as many options as infantry soldiers have. All that they could do is to fully disobey their orders, by not flying and shooting with their drone. It is likely these individual circumstances that make it easier to identify infantry soldiers as the fully responsible source of outcomes, and why they are therefore more often put on trial for excessive force than RCCV pilots.</p> <p>A further explanation for the muddying of responsibility allocation is that the complexity of RCCV deployment also increases the possibilities for errors. There might be situations where errors slip into the process of deciding which suspect should be killed with RCCVs, as well as into the execution of this decision, unbeknownst to all parties involved. Whether we should understand these cases as accidents for which no one could be taken neither responsible, as with unforeseeable weather, or whether such cases hint at a systematic responsibility gap in RCCVs is open to debate. On the one hand, if such a situation were to happen, responsibility might be distributed in terms of moral responsibility because it could be argued that the drone’s operators failed to detect the error and therefore neglected their duty to oversee and care for the RCCV’s deployment. On the other hand, I am inclined to say, if RCCVs repeatedly imply errors, which remain unknown, Leveringhaus second criterion for responsibility gaps is fulfilled here. That is, RCCVs could potentially do something because of undetected errors, which were unforeseeable to their operators.</p> <p>Finally, the lack of prosecution of wrong drone strikes could also be explained by looking at the bigger picture of wars. Armed drones allow “surgical” and small-scale strikes which do not appear as grave and obvious, as, for example, erroneous and unjust bombings by aircraft. RCCVs can fly at lower altitudes than aircraft and, thus, much closer to their targets. This provides their operators with the opportunity to attack with precision and less collateral damage. In contrast, consider when the German Colonel Georg Klein mistakenly ordered the bombing of two fuel trucks in Afghanistan in 2009, and the incident led to approximately 100 deaths of innocent civilians. This resulted in an investigation by public prosecution, although charges were eventually dropped. Compared with such damage, it is possible that drone strikes have not been adequately addressed. If considered firmly, however, (legal) responsibility for wrong drone operations could and, of course, should be allocated.</p> <p>To sum up, in this section I have argued that the deployment of RCCVs is prone to responsibility protractions. This is so because they are highly sophisticated technologies whose deployment requires many hands. For this reason, killings perpetrated with RCCVs appear to be qualitatively different from those which are perpetrated with conventional rifles by infantry soldiers. The complexity of remotely controlled machines seems to deprive military personnel from an individual application of their own ethical standards. Additionally, errors can happen without individual responsibility because they could slip in undetected. While surgical effectiveness of RCCVs is arguably an advantage, this aspect might be another reason as to why unjust drone strikes have rarely been prosecuted and not let to any conviction.</p> <h2 id="conclusion-and-measures-to-address-possible-rccv-misconduct">Conclusion and Measures to Address Possible RCCV Misconduct</h2> <p>The previous analysis agrees with the established view that armed drones do not create responsibility gaps\(^6\); for there seem to be no realistic cases of unintended, uncontrolled, physical outcomes, unbeknownst to their operators, with contemporary RCCVs. However, there is evidence suggesting that RCCVs create other problems with responsibility, by protracting its allocation. If that is the case, RCCV deployments pose a problem for our ethical standards.</p> <p>Just War principles and established military conventions, such as The Convention on Certain Conventional Weapons, are meant to prevent excessive force. But if unjust harm is being done with armed drones, and no one seems to be fully responsible for it, it would be difficult to hold someone accountable. Who should it be? The pilot? The officer or the intelligence service agent who discuss suspects? The general? The head of state? This is an ethically undesirable situation because the responsible use of machines is not only achieved through good will on the operators’ side, but also by sanctions for misconduct that can only be applied if it is clear who has been responsible for the machine’s output. If, however, no one can be held responsible for RCCVs, then there is no way for sanctions and further legal action; and the deployment of RCCVs eventually invites excessive and unjust use.</p> <p>How can this situation be ameliorated? While it seems that one should as much as possible avoid technologies, such as RCCVs, that diffuse responsibility allocations, this answer will only partly suffice because RCCVs are already being deployed by many states. The chances that this will change are slim. Therefore, one amelioration would be to spell out at least as clearly as possible in what way and to what extent agents are responsible for RCCV strikes. This paper has already started with pointing out the complexity of responsibility that arises with RCCVs, but further research, especially empirical research, is needed. We need to discuss in what way military institutions prosecute misconduct, how RCCV sorties are being internally monitored, whether one pilot suffices for steering RCCVs, and whether pilots or superiors duly take responsibility for their deeds. It would require the military of many states to step up their internal monitoring.</p> <p>At the same time, I believe that considering RCCVs on their own will reach its limits. If RCCVs are seen as being part of a complex institution, a view that mainly focuses on individual responsibility is too narrow. In this regard, Luciano Floridi (2016) suggests one way of reversing our classical understanding of responsibility that deserves further attention, but upon which I can only concentrate briefly. Considering actions, we should, says Floridi, focus less on agents and their intentions and more on outcomes. The underlying idea is that if a network of human, artificial, and hybrid agents produces effects that are morally undesirable, then one way of addressing the issue could be by introducing systematic changes, especially when it is unclear who or what has led to the outcome of the network. For Floridi, this is best achieved through “soft legislation, rules and codes of conducts, nudging, incentives and disincentives” (Floridi 2016, 7). In other words, Floridi’s approach is trying to circumvent the scavenger hunt for individual responsibility by varying different systematic stimuli until the morally desired outcome for a network of agents is achieved.</p> <p>Considering RCCVs, it would be hence reasonable to complement our strategy of assigning responsibility with additional strategies to disincentivize misconduct. We therefore need to ask ourselves: What systematic measures could be useful to reduce possible unjust RCCV deployment?</p> <p>One opportunity to reduce unjust RCCV strikes is to strengthen soldiers’ and pilots’ possibilities for “conscientious refusal to fight”, as Jeff McMahan suggests (McMahan 2009, 97). This could be achieved by giving pilots a greater leeway as to how they also apply their own moral standards, for example by being involved in the process of deciding which target is being hit. Because RCCV missions can often be prepared, questioning operators whether killing such prospective targets is also consistent with their moral views does not seem to conflict with other practical requirements that must be met for a successful mission. In this way, operators, especially the pilots, would also have more grounds to object if they thought the killing of people was morally wrong. Ultimately, we would want to strive for a situation where the operators are as morally responsible for their action with RCCVs as infantry fighters are in combats, for whom it appears easier for us to assign responsibility. If this is achieved, RCCV operators could take a greater share of responsibility, as when they are only cogs in the military machinery.</p> <p>Another problem with contemporary RCCV missions is that RCCVs are often deployed within the context of terrorist hunts that are not in any way part of a justified war. This then points to an issue with RCCVs which we need to address, namely, the fact that they invite military action outside of our classical understanding of just and unjust wars, for example, by breaching sovereignty of states where terrorists hide. Therefore, we may try to directly re-enforce the principle of state sovereignty that has been weakened with RCCV deployments. We should require clear agreements between the deploying countries and those countries where RCCV missions take place.</p> <p>To undermine quick and dirty missions, we could further demand that RCCV pilots must be stationed in the country where they deploy their systems, provided that there is no official declaration of war against the country where they carry out their missions. For it is one thing, to quickly fly through a foreign and often militarily inferior country, but another to station active military personnel abroad. Physically locating pilots in a foreign country where they undertake military strikes would also necessitate that such a country allows for their presence, helping to reinforce state sovereignty. Additionally, this requirement would mean that disagreeing countries could more easily prevent deployments by, for example, expelling RCCV pilots from their territory. In the traditional scenario of defending against a belligerent state, it appears neither feasible nor ethically imperative to deploy RCCV personnel abroad, as missions are likely to be accompanied by additional military action against the adversary and can be justified to effectively halt their attacks. For instance, a nation defending itself may employ drone strikes to impede the adversary’s military infrastructure.</p> <p>Such an approach to promoting a specific outcome may prove effective if it were to become a mandatory requirement that is accepted by all nations in the spirit of war conventions. If the international community agrees that RCCVs lead to ethically undesirable situations, they might be willing to cede certain RCCV missions in the hopes of ameliorating the overall legal and moral standards with which they can be deployed. At the same time, it would allow RCCVs being deployed for defensive purposes. Having states agree on the use of certain weapons in this sense does not seem to be impossible if we think of The Convention on Certain Conventional Weapons that prohibits or restricts the use of certain weapons, such as booby traps or incendiary weapons. Even in the absence of a formal agreement, states may wish to station their pilots in the areas where they deploy RCCVs to ensure a more responsible approach to these systems.</p> <p>In sum, I am suggesting several measures to address possible RCCV misconduct: (a) the military should deploy RCCVs as little as possible to avoid responsibility protractions, (b) if RCCVs are being deployed, the military must assure that for each RCCV mission responsibility can be allocated. To heighten the standards with which RCCVs are being deployed, (c) conscientious engagement among RCCV operators should be promoted, allowing them a greater leeway to apply their own moral interpretations. Finally (d), states should station pilots in countries where their RCCV mission takes place whenever there is no official declaration of war against these respective countries.</p> <h2 id="footnotes">Footnotes</h2> <p>\(^1\) The paper is an ameliorated and extended version of Chapter 3.3. from my master’s thesis Drone Ethics: Duties and Responsibilities for Unmanned Aerial Combat Vehicles from 2020, which is accessible at the library of the University of Vienna, Austria.</p> <p>\(^2\) One notable exception is Jail Galliott’s nineth chapter on responsibility gaps in his Military Robots. Mapping the Moral Landscape (2015).</p> <p>\(^3\) Al-Aulaqi v. Obama, in which these privileges played a role, was ultimately dismissed on different grounds (U.S. Congressional Research Service 2022, 27)</p> <p>\(^4\) Thomas Simpsons and Vincent Müller (2016) as well as Sebastian Köhler (2020, 3137) also defend this view.</p> <p>\(^5\) The rise of amateur drone deployments (as was the case in Ukraine against the Russian military in 2022) increases the chance of bugs and flawed designs. But do amateur models lead to systematic responsibility gaps? To address this question, further information concerning actual deployments is required that is not yet available as of 2023. This paper focuses on RCCVs that have been produced by licensed companies, undergone professional testing, and which are maintained by professional mechanics.</p> <p>\(^6\) That is, there are no responsibility gaps when we analyze armed drones based on Alex Leveringhaus’s criteria (2016). Ibo van de Poel et al. (2012) speak of the Problem with Many Hands also in terms of responsibility “gaps”.</p> <h2 id="bibliography">Bibliography</h2> <p>Becker, Joe and Scott Shane. 2012. Secret ‘Kill List’ Proves a Test of Obama’s Principles and Will. The New York Times. https://www.nytimes.com/2012/05/29/world/ obamas-leadership-in-war-on-al-qaeda.html (accessed May 6, 2023).</p> <p>Coeckelbergh, Mark. 2020. AI Ethics. Cambridge, MA: MIT Press.</p> <p>Danaher, John. 2016. Robots, law and the retribution gap. Ethics and Information Technology, 18(4): 299–309.</p> <p>Fallon, Amy. 2011. US soldier jailed for seven years over murders of Afghan civilians. The Guardian. https://www.theguardian.com/world/2011/sep/24/us-soldier-jailed-afghan-civilians (accessed June 17, 2023)</p> <p>Floridi, Luciano. 2016. Faultless responsibility: on the nature and allocation of moral responsibility for distributed moral actions. Philosophical Transactions of the Royal Society, vol. 374 (A): 1-13.</p> <p>Galliott, Jai. 2015. Military Robots. Mapping the Moral Landscape. London: Routledge.</p> <p>Kaufman, Brett Max, and Anna Diakun. 2017. United States Targeted Killing Litigation Report. In Litigating Drone Strikes: Challenging the Global Network of Remote Killing, eds. Andreas Schüller and Wolfgang Kaleck. Berlin: European Center for Constitutional and Human Rights (ECCHR): 118-133.</p> <p>Killmister, Suzy. 2008. Remote Weaponry: The Ethical Implications. Journal of Applied Philosophy, vol. 25, no. 2: 121-133.</p> <p>Köhler, Sebastian. 2020. Instrumental Robots. Science and Engineering Ethics, (2020) 26: 3121–3141.</p> <p>Lazar, Seth. 2016. War. The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta. https://plato.stanford.edu/entries/war/ (accessed May 6, 2023).</p> <p>Leveringhaus, Alex. 2016. Ethics and Autonomous Weapons. London: Palgrave Macmillan.</p> <p>Lokhorst, Get-Jan and Jeroen van den Hoven. 2012. Responsibility for Military Robots. In Robot Ethics. The Ethical and Social Implications of Robotics, eds. Patrick Lin, Keith Abney,and George Bekey. Cambridge, MA: MIT Press: 145-156.</p> <p>Matthias, Andreas. 2004. The responsibility gap: Ascribing responsibility for the actions of learning automata. Ethics and Information Technologies, vol. 6: 175-183.</p> <p>McMahan, Jeff. 2009. Killing in War. Oxford: Oxford University Press.</p> <p>Miller, David. 2001. Distributing Responsibilities. The Journal of Political Philosophy, vol. 9, no. 4: 453-471.</p> <p>Müller, Vincent C. 2016. Autonomous Killer Robots are Probably Good News. In Drones and Responsibility. Legal, Philosophical and Socio-Technical Perspectives on Remotely Controlled Weapons, eds. Ezio Di Nucci and Filippo Santoni de Sio. London: Routledge: 67-81.</p> <p>Nucci, Ezio Di and Filippo Santoni de Sio. 2016. Drones and Responsibility. Mapping the Field. In Drones and Responsibility. Legal, Philosophical and Socio-Technical Perspectives on Remotely Controlled Weapons, eds. Ezio Di Nucci and Filippo Santoni de Sio. London: Routledge: 1-14.</p> <p>Oimann, Ann-Katrien. 2023. The Responsibility Gap and LAWS: a Critical Mapping of the Debate. Philosophy &amp; Technology (2023): 36:3.</p> <p>The Organization for Security and Co-operation in Europe. 1994. Code of Conduct on Politico-Military Aspects of Security. https://www.osce.org/files/f/documents/5/7/ 41355.pdf (accessed May 6, 2023).</p> <p>Phillips, Dave, and Eric Schmitt. 2021. How the U.S. Hid an Airstrike That Killed Dozens of Civilians in Syria. The New York Times. https://www.nytimes.com/2021/11/13/us/us airstrikes-civilian-deaths.html (accessed May 6, 2023).</p> <p>Simpson, Thomas W., and Vincent C. Müller. 2016. Just War and Robot’s Killings. The Philosophical Quarterly, vol. 66, No. 263.</p> <p>Sparrow, Robert. 2007. Killer Robots. Journal of Applied Philosophy, vol. 24, no. 1: 62-77.</p> <p>Sparrow, Robert. 2012. ”Just Say No” to Drones. IEEE Technology and Society Magazine, Spring: 56-63.</p> <p>Thompson, Dennis F. 1980. Moral Responsibility of Public Officials: The Problem of Many Hands. The American Political Science Review, vol. 74, no. 4 (Dec.): 905-916.</p> <p>U.S. Congressional Research Service. 2022. The State Secrets Privilege: National Security Information in Civil Litigation (R47081, April 28, 2022) by Jennifer K. Elsea and Edward C. Liu. ProQuest® Congressional Research Digital Collection (accessed June 17, 2023).</p> <p>van de Poel, Ibo, Jessica Nihlén Fahlquist, Neelke Doorn, Sjoerd Zwart, and Lambèr Royakkers. 2012. The Problem of Many Hands: Climate Change as an Example. Science and Engineering Ethics, vol. 18: 49-67.</p>]]></content><author><name></name></author><category term="drone"/><category term="autonomous"/><category term="combat"/><category term="vehicles"/><category term="ethics"/><category term="responsibility"/><category term="protraction"/><category term="gap"/><category term="just"/><category term="war"/><category term="theory"/><summary type="html"><![CDATA[This paper examines drone warfare from an ethical perspective]]></summary></entry><entry><title type="html">Using LLMs in Syntactic Research</title><link href="https://omseeth.github.io/blog/2024/llms_syntactic_research/" rel="alternate" type="text/html" title="Using LLMs in Syntactic Research"/><published>2024-10-17T10:05:00+00:00</published><updated>2024-10-17T10:05:00+00:00</updated><id>https://omseeth.github.io/blog/2024/llms_syntactic_research</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/llms_syntactic_research/"><![CDATA[<p><strong>Abstract</strong> Large language models (LLMs) have revolutionized natural language processing. This study investigates their potential for replicating and contributing to syntactic research. I leverage GPT-3.5 Turbo to reproduce Experiment 2 from Salzmann et al. (2022), which explored Principle C (coreference restrictions) in German. The LLM successfully replicated human behavior in confirming Principle C’s influence. However, unlike human participants, GPT-3.5 Turbo did not exhibit sensitivity to syntactic movement, but to argument and adjunct positions, suggesting a potential difference in underlying processing mechanisms. These findings highlight the potential of LLMs for probing syntactic principles as well as linguistic phenomena more generally but also raise questions about their ability to mirror human language processing. I discuss the methodological implications of using LLMs for linguistic inquiry and the potential for uncovering insights into their inner workings.</p> <h2 id="1-introduction">1 Introduction</h2> <p>The advent of large language models (LLMs) through the Transformer architecture (Vaswani et al., 2017) has led to models with unprecedented capabilities. This contribution attempts to harness these capabilities to reproduce an empirical experiment from Salzmann et al. (2022) with one Transformer based model, GPT 3.5 Turbo developed by OpenAI.</p> <p>The background to the experiment is an investigation of Principle C (also called Condition C). According to syntactic theory, Principle C governs the possibilities of coreference between pronouns and R-expressions (i.e., proper names and definite determiner phrases). However, how the principle works remains a matter of debate (Adger et al., 2017; Bruening and Khalaf, 2018; Salzmann et al., 2022; Stockwell et al., 2021). For that matter, Salzmann et al. (2022) developed several experiments to explore the principle in German contexts. This paper focuses on their second experiment, Experiment 2, where the authors attempted with a 2 x 2 x 2 experimental design to investigate whether participants reconstructed the syntactic hierarchy of given sample sentences when asked if different interpretations were possible (Salzmann et al., 2022, 601).</p> <p>To reproduce Experiment 2, GPT 3.5 Turbo was queried with the exact same items that were used in the original work by Salzmann et al. (2022). I employed diverse prompt types, including zero-shot and one-shot, and varied sizes of iterations. In order to match the identical quantity of data points collected from 32 participants in Salzmann et al. (2022), I finally chose the outcomes of a configuration of four iterations with a zero-shot prompt to query GPT 3.5 Turbo for fitting a generalized linear mixed model. With this model, I tested the same three hypotheses from Experiment 2 in Salzmann et al. (2022).</p> <p>I report the following results: In line with human participants, the LLM qualified sentences with referring expressions in such a fashion that the influence of Principle C for respective cases with R-expressions and pronouns is attested. In contrast to the human proportion of answers, GPT 3.5 Turbo’s output did not indicate a significant influence of movement when interpreting the test item sentences. If movement is understood as a variable for reconstruction, it appears that the LLM does not reconstruct the pre-moved positions of the phrases. Finally, a significant influence of position of R-expressions in either arguments or adjuncts can be shown with GPT 3.5 Turbo. Overall, GPT 3.5 Turbo’s judgments of sentences differ from those of humans in more ambiguous cases. However, whether movement and reconstruction play a role in human interpretations is neither fully clear from Experiment 2, as Salzmann et al. (2022) conclude in their discussion of the original experiment.</p> <p>The novel possibility to reproduce an empirical experiment in linguistics with LLMs also raises many methodological questions. Are LLMs like GPT 3.5 Turbo computational accounts of how humans process language (Blank, 2023)? Can they be deployed to test hypotheses in syntax as well as other linguistic fields? Even if these questions remain open to debate, querying LLMs with test items from linguistics would also provide us with valuable insight into the ”metacognition” of these models (Begus et al., 2023). Knowing how the models respond can help engineers with improving their performance.</p> <p>In <strong>Section 2</strong>, I will introduce Principle C and the syntactic considerations that led to Experiment 2 in Salzmann et al. (2022). The reports of the original experiment will be reported in <strong>Section 3</strong>. I will discuss in <strong>Section 4</strong> the prompts and details used to query GPT 3.5 Turbo with the items from Salzmann et al. (2022). I will summarize the LLM’s results in <strong>Section 4.2</strong>. Finally, I will provide a short discussion of possible implications of using LLMs in linguistic research in <strong>Section 5</strong>.</p> <h2 id="2-syntactic-principle-for-referring-expressions">2 Syntactic principle for referring expressions</h2> <p>Several foundational assumptions hold in generative syntax. Generally, its goal is to analyze language by assuming that rules generate well-formed patterns. Some of these rules, also called principles, are considered to be universal, others only apply to specific languages. It is further assumed that each language is structured hierarchically where the constituents of a syntactically formed phrase can be moved within its hierarchy to accommodate the large variety of linguistic possibilities. When a constituent is being moved, analysis states that it is generally assumed that it leaves a trace behind. However, many movements are also restricted, for example by the depth of embedding of each constituent within the hierarchy.</p> <p>Among the syntactic principles, generative syntax has identified three types that restrict in particular co-reference of noun phrases (R-expressions, pronouns, anaphors). These types are called Principle A, B, and C. Principle C will be of interest to us in this contribution.</p> <p>Principle C states that a pronoun cannot refer to an R-expression that it c-commands. R-expressions are proper names or definite determiner phrases, such as ”Mary” or ”the dog”. C-command is a syntactic constraint where a constituent within a phrase’s hierarchy would occupy a structurally higher position than another one from which syntactic restrictions as in Principle C follow. For example:</p> <blockquote> <p>(1) \(^*\)He\(_{i}\) thinks that <strong>Peter</strong>\(_{i}\) is the happiest.</p> </blockquote> <p>is ungrammatical according to Principle C when it is assumed that the pronoun should refer to the noun (see indices). It occupies a structurally higher position than the R-expression Peter. For the same reasons, a sentence like</p> <blockquote> <p>(2) \(^*\)[Which of <strong>Lara</strong>\(_{i}\)’s sweaters]\(_{1}\) do you think she\(_{i}\) likes __\(_{1}\)?</p> </blockquote> <p>is considered ungrammatical, even if the R-expression Lara appears on the surface before the pronoun. This can be explained with reconstructing to the trace of the movement of the wh-phrase (consider index 1 and square brackets) which occupies a lower position than the pronoun before its movement. This is also called an A’-movement since the movement does not affect the type of phrase.</p> <p>In their paper, ”Condition C in German A’-movement: Tackling challenges in experimental research on reconstruction”, Salzmann et al. (2022) analyze Principle C (also called Condition C) within particular contexts. Their research follows a shift in syntax where principles that were suggested by syntactitians based on introspective judgements (sometimes described as armchair syntax) are checked through experimental setups. If principles can be confirmed in a variety of linguistic examples by a sufficient size of native speakers, the empirical evidence provides an additional justification for them. If a pattern is equivocally observed, it might not apply in a principled sense.</p> <p>The aim of Salzmann et al. (2022) is to examine the workings of Principle C within three contexts. I will explain them briefly. <em>Context 1)</em> When the R-expression is part of an adjunct, as opposed to when it is part of an argument, the established view states that there is no Principle C reconstruction (Lebeaux, 1991; Salzmann et al., 2022). <em>Context 2)</em> Principle C is always violated whenever the R-expression is contained within a predicate. The containment of an R-expression within an argument does not always lead to a violation, contrary to the assumption in Context 1 (Huang, 1993; Salzmann et al., 2022). <em>Context 3)</em> Condition C is less strict with relative clauses than with wh-movement (Citko, 1993; Salzmann et al., 2022).</p> <p>Salzmann et al. (2022) conducted three experiments in German to examine the previously mentioned syntactic phenomena. This paper focuses on their second experiment (Salzmann et al., 2022, pp. 601-609, the experiment was also registered at <a href="https://osf.io/mjgpz]">https://osf.io/mjgpz</a>).</p> <p>In Experiment 2, Salzmann et al. (2022) investigate coreference between pronoun and R-expression in embedded clauses. The clauses have either moved or in situ wh-phrases, serving either as objects or as subjects. Additionally, it is investigated whether the position of an R-expression as part of an argument or and adjunct makes a difference. The following two examples from Salzmann et al. (2022, 603) illustrate the conditions in Experiment 2:</p> <blockquote> <p>(3)… [welche Geschichte im Buch über <strong>Hanna</strong>] <em>sie</em> __ ärgerlich fand.</p> <blockquote> <p>’which story in the book about Hanna she found upsetting.’</p> </blockquote> </blockquote> <blockquote> <p>(4)… [welche Geschichte über <strong>Hanna</strong>] __ <em>sie</em> verärgert hat.</p> <blockquote> <p>’which story about Hanna upset her.’</p> </blockquote> </blockquote> <p>Note in (3) that the object is moved and the R-expression Hanna serves as an adjunct to ”im Buch”. If Principle C were to have its assumed force in all instances, one would assume for (3) that the pronoun cannot refer to the R-expression. (Though the established view has noticed that the adjunct position indicates a less strict application of Principle C.) In (4), the wh-phrase serves as the subject which is moved and the R-expression is part of an argument. One would assume in contrast to the previous example that the pronoun can refer to the R-Expression. Also consider the for the depth of embedding and hence the structural hierarchy in both examples for the assumptions.</p> <p>The aim of Experiment 2 in (Salzmann et al., 2022) is to create a statistical baseline for syntactically well-formed clauses (no violation of Principle C) through positive responses as well as a percentage of responses for those examples where Principle C would be violated if movement is assumed. If no violation in terms of negative responses is reported for ungrammatical examples, Salzmann et al. (2022) consider surface factors as another influence for the acceptability of coreference between R-expression and pronoun in such cases.</p> <p>Salzmann et al. (2022, 603-604) test three hypotheses in their experiment to consider the effect of Principle C (also called Condition C) in the previously mentioned variety of contexts:</p> <ul> <li><strong>H1</strong> Condition C hypothesis: R-expressions cannot be coreferential with a c-commanding expression.</li> <li><strong>H2(a)</strong> Reconstruction hypothesis: the base position of moved phrases matters for Condition C.</li> <li><strong>H2(b)</strong> Surface hypothesis: the surface position of moved phrases matters for Condition C.</li> <li><strong>H3</strong> Argument/adjunct asymmetry hypothesis: in contrast to argument po- sitions, there is no reconstruction if the R-expression is part of an adjunct.</li> </ul> <p>H1 assumes that phrases with an in situ subject receive more positive responses for the subject than for the object, as coreference with the object would clearly violate Principle C. Consider the following example:</p> <blockquote> <p>(5) \(^*\)Jim erklärt, dass <em>er</em> die neue Geschichte über <strong>Mert</strong> ärgerlich fand.</p> <blockquote> <p>’Jim explains that he found the new story about Mert upsetting.’</p> </blockquote> </blockquote> <p>That the pronoun ”er” refers to <strong>Mert</strong> contradicts our syntactic intuition.</p> <p>If H1 is confirmed, the authors investigate with H2 if more positive responses for a moved subject can be observed than with a moved object, that is, whether Principle C also holds with movement (H2(a)). If there is a significant difference between H2(a) and H1, then it is assumed that H2(b) would be confirmed, that is, surface positions influence the force of Principle C in case that the surface effects differ for objects and subjects. Finally, H3 investigates the difference of responses with instances where the R-expression is positioned as an adjunct compared to those within argument structures.</p> <h2 id="3-experimental-results-from-32-participants">3 Experimental results from 32 participants</h2> <p>In this section, I will briefly describe the experimental setting of Experiment 2 from (Salzmann et al., 2022) and report their results.</p> <h3 id="31-experimental-setting">3.1 Experimental setting</h3> <p>To test their hypotheses, Salzmann et al. (2022) developed a 2 x 2 x 2 design where the first factor was Phrase (containment of R-expression either within the subject or the object), the second Position (R-expression either as argument or adjunct) and the third Movement (in situ or moved). 32 participants were recruited via www.prolific.co. They were given 78 stimuli which comprised 32 critical items, 44 fillers, and 2 additional items for exploration.</p> <p>The general set-up of each experimental round consisted in a shown example sentence with two R-expressions and a pronoun. The sentence was followed by two binary questions. Before the start of the experiment, the participants received an instruction with an example that deliberately allowed two interpretations for resolving the references of the example sentence. The participants were told to answer ”Yes” and ”Yes” in such cases. Generally, a sentence such as (5) was accompanied with two questions of the following type: Q1) ”Can this sentence be interpreted that Jim found a story upsetting?” (asking about the subject from the matrix clause ’Jim explains that…’) and Q2) ”Can this sentence be interpreted that Mert found a story upsetting?” (asking about the subject and/or object, depending on the respective condition, from the embedded clause). To avoid the order of Q1 and Q2 as a confound, the questions were randomly switched. Salzmann et al. (2022) collected the respective ”Yes” and ”No” answers for all stimuli. On average, the authors report that the experiments took 24 minutes until completion.</p> <h3 id="32-results-from-32-participants">3.2 Results from 32 participants</h3> <p>For their 32 participants, Salzmann et al. (2022) report the results for all conditions as shown in Table 1.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_1-480.webp 480w,/assets/img/using_llms/table_1-800.webp 800w,/assets/img/using_llms/table_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_1.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 1:</strong> Results of Experiment 2 with proportion of positive answers</p> <p>Noticeably, the interpretations of questions of type Q1 with respect to the refer- ring R-expression were accepted in nearly all stimuli. This comes as no surprise as its intention was to establish a baseline for syntactically well-formed patterns where the subject from the matrix clause occupies the structurally highest position. Given that some noise is considerably part of every empirical evaluation, 100% positive answers for the well-formed examples would be more concerning. A greater difference is observable for those readings that were elicited with Q2. Q2 received a proportion of more than 50% positive answers where the R-expression was part of a subject phrase. To a lesser degree, Q2 was accepted when it was contained within an object phrase, almost never when the R-expression stayed in situ (consider example (5)).</p> <p>With their findings, the authors fit two generalized linear mixed models, following the recommendations of Bates et al. (2015), where the items are considered as random effects. For both models the baseline of <em>Phrase</em> was ’object’ and for <em>Position</em> ’argument’. To test all hypotheses two contrast encodings were necessary so that <em>Movement</em> received ’in situ’ as its baseline in <strong>Model 1</strong> and ’moved’ in <strong>Model 2</strong>. I report their results in Table 2 and 3 where <em>Position</em> was originally called ”arg/adj”.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_2-480.webp 480w,/assets/img/using_llms/table_2-800.webp 800w,/assets/img/using_llms/table_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_2.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 2:</strong> Generalized linear mixed model results from Experiment 2 (<strong>Model 1</strong>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_3-480.webp 480w,/assets/img/using_llms/table_3-800.webp 800w,/assets/img/using_llms/table_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_3.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 3:</strong> Generalized linear mixed model results from Experiment 2 (<strong>Model 2</strong>)</p> <p>Salzmann et al. (2022, 606) use <strong>Model 1</strong> and <strong>2</strong> to validate or reject their hypotheses. They report an effect of Phrase for the levels ’in situ’ and ’argument’ with respect to the other factors: z = 8.226, p &lt; 0.001 in <strong>Model 1</strong>, so H1 is being confirmed. The reasoning behind this is that for sentences with subject, in situ, argument, such as in <strong>Condition e</strong>:</p> <blockquote> <p>(6) Jim erklärt, dass die neue Geschichte über <strong>Mert</strong> <em>ihn</em> verärgert.</p> <blockquote> <p>’Jim explains that the new story about Mert upsets him.’</p> </blockquote> </blockquote> <p>there is no c-command for the pronoun over the R-expression in the embedded clause, compared with sentences with object, in situ, argument <strong>Condition a</strong>, as in:</p> <blockquote> <p>(7) Jim erklärt, dass <em>e</em>r* die neue Geschichte über <strong>Mert</strong> ärgerlich fand.</p> <blockquote> <p>’Jim explains that he found the new story about Mert upsetting.’</p> </blockquote> </blockquote> <p>An effect of Phrase was also noticed within the levels of ’moved’ and ’argument’ with respect to the other factors: z = 2.391, p = 0.017 in <strong>Model 2</strong>. This confirms H2(a) of their hypotheses: that the base position of moved phrases matters for Principle C. Compare <strong>Condition c</strong> (not the principle):</p> <blockquote> <p>(8) Lisa erzählt, [welche Geschichte über <strong>Hanna</strong>] <em>sie</em> __ ärgerlich fand.</p> <blockquote> <p>’Lisa tells which story about Hanna she found upsetting.’</p> </blockquote> </blockquote> <p>with <strong>Condition g</strong>:</p> <blockquote> <p>(9) Lisa erzählt, [welche Geschichte über <strong>Hanna</strong>] __ <em>sie</em> verärgert hat.</p> <blockquote> <p>’Lisa tells which story about Hanna upset her.’</p> </blockquote> </blockquote> <p>Salzmann et al. (2022) also explain in their study pre-registration: ”If there is reconstruction for Principle C, we should see the same effect in the conditions with wh-movement as in the conditions without wh-movement, irrespective of the differences in surface structure.”</p> <p>The authors further confirm hypothesis H2(b) (the relevance of surface positions) because an interaction between <em>Movement</em> and <em>Phrase</em> is found in the level ’argument’ of the other factor: |z| = 5.596, p &lt; 0.001 in both models. The reasoning is that Principle C would be reduced in contexts with movement compared to those without because on the surface the moved phrase would not look like a violation of the principle.</p> <p>Finally, hypothesis H3 (argument/adjunct asymmetry) is not confirmed as there is overall no effect of <em>Position</em> and neither an interaction between <em>Phrase</em> and <em>Position</em> (arg/adj) within the level of ’moved’ (z = 0.087,p = 0.931 in <strong>Model 2</strong>) nor between <em>Movement</em> and <em>Position</em> (arg/adj) within ’object’ (|z| = 0.209, p = 0.834 in <strong>Model 1</strong> and <strong>2</strong>). Salzmann et al. (2022) argue that H3 can also only hold if H2(a) is borne out.</p> <h2 id="4-experimental-results-from-llm">4 Experimental results from LLM</h2> <p>In section 4, I will discuss the model and prompts used in the experiment and report its results.</p> <h3 id="41-experimental-setting">4.1 Experimental setting</h3> <p>The experiment was executed with the updated GPT 3.5 Turbo (gpt-3.5-turbo- 0125) large language model from <a href="www.openai.com">www.openai.com</a>. The model’s training data dates up to September 2021. For the experiment, GPT 3.5 Turbo is queried twice with an adjusted prompt for each test sentence with Q1 in the first round and Q2 in the second. This process was repeated for two and four iterations, leading to an overall data with 512 and 1024 data points for each type of questions (Q1 or Q2). Four iterations led to the same amount of data points as in Experiment 2 from Salzmann et al. (2022). The temperature of the model, which renders its answers more deterministic, was set to 0. The role – an option by OpenAI to further customize the prompts – used for the queries was the default ”user”.</p> <p>Prompt P1 was chosen for the task after some manual experimentation. Since Experiment 2 from (Salzmann et al., 2022) was conducted in German, German was also used for the model instructions in the prompt (Consider the Appendix for the original prompts):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P1-480.webp 480w,/assets/img/using_llms/P1-800.webp 800w,/assets/img/using_llms/P1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P1.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The variables ’content’ and ’question’ in the prompt referred to the different conditioned sentences that belonged to the experimental items from Salzmann et al. (2022).</p> <p>Since the participants in Experiment 2 received instructions, a second prompt P2 with a similar instruction for the large language model was used.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P2-480.webp 480w,/assets/img/using_llms/P2-800.webp 800w,/assets/img/using_llms/P2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P2.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>P2 was an instance of what is commonly known as few shot learning because GPT 3.5 Turbo received one example with an ideal solution.</p> <h3 id="42-results-from-gpt-35-turbo">4.2 Results from GPT 3.5 Turbo</h3> <p>The closest overall results compared to the baseline of human answers can be derived from a zero shot configuration of GPT 3.5 Turbo (Table 4). In the configuration with four iterations and a zero shot prompt, Q1 received a great share of positive answers in all conditions. Q2 was almost never positively answered with phrases that contained the R-expression as an object (<strong>Condition a-d</strong>). The adjunct position led to some greater variability though (21.1% and 35.9% positive answers). For Q2, GPT 3.5 Turbo provided mostly more positive answers than humans when queried with subject phrases, with one exception (<strong>Condition e</strong>). For subjects, GPT 3.5 Turbo’s proportions of answers differ to those from humans up to 9,4%. The four times iterated one shot configuration led to a lot of positive answers for both Q1 and Q2 except for those cases where the R-expression was contained in an argument within an object phrase (<strong>Condition a</strong> and <strong>b</strong>). Again, <strong>Condition e</strong> was slightly less positively answered than the other subject conditions. The configurations with only two iterations followed the trend of both previously mentioned configurations. The configuration with a zero shot prompt and two iterations led to results which are even closer to the human baseline than the same prompt four times iterated. The greatest difference to the human baseline for the zero shot configurations can be noticed in <strong>Condition b</strong> with a difference of 35.1% and 34.3%.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_4-480.webp 480w,/assets/img/using_llms/table_4-800.webp 800w,/assets/img/using_llms/table_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_4.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 4:</strong> Proportion of GPT 3.5 Turbo’s positive answers, using zero and one shot prompts with 4 and 2 iterations of all items from Experiment 2 (the conditions are the same as those in Table 1)</p> <p>Following Salzmann et al. (2022), I fit a generalized linear mixed model with the data from the zero shot configuration after four iterations. I chose this configuration because its amount of data points resembles the amount from Experiment 2 in Salzmann et al. (2022). The items were again considered as random effects. As with the models from Salzmann et al. (2022), the baseline of <em>Phrase</em> was ’object’ and for <em>Position</em> ’argument’. Similarly, two baselines for <em>Movement</em> were used to account for the differences between ’in situ’ (<strong>Model 3</strong>) and ’moved’ (<strong>Model 4</strong>). I report the results in Table 5 and 6.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_5-480.webp 480w,/assets/img/using_llms/table_5-800.webp 800w,/assets/img/using_llms/table_5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_5.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 5:</strong> Generalized linear mixed model results from the zero shot configuration with four iterations (<strong>Model 3</strong>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_6-480.webp 480w,/assets/img/using_llms/table_6-800.webp 800w,/assets/img/using_llms/table_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_6.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 6:</strong> Generalized linear mixed model results from the zero shot configuration with four iterations (<strong>Model 4</strong>)</p> <p>With <strong>Model 3</strong>, an effect of <em>Phrase</em> can be observed for the levels ’in situ’ and ’argument’ from <em>Movement</em> and <em>Position</em> with z = 5.844, p &lt; 0.001 confirming H1 from Experiment 2. The same effect is observed with <strong>Model 4</strong> where <em>Phrase</em> leads to z = 6.075, p &lt; 0.001. If the interpretation of the LLM results were to follow Salzmann et al. (2022), the effect of <em>Phrase</em> in either generalized linear mixed model with ’in situ’ as well as ’moved’ as baseline for Movement would imply the acceptance of H2(a). However, no effect of <em>Movement</em> nor an interaction of <em>Movement</em> in the other levels is observed in <strong>Model 3</strong> and <strong>4</strong>, rejecting H2(b). As H2(a) appears to be dependent on <em>Movement</em>, it is questionable whether H2(a) can be accepted when no noticeable effect of Movement is given. Salzmann et al. (2022) predicted an effect of Position if H2(a) is rejected. Precisely, this effect is observed in <strong>Model 3/4</strong> with z = 3.705,p = 0.001 and z = 4.697,p &lt; 0.001, that is, where the appearance of the R-expression in either an argument or an adjunct position makes a difference. Position also interacts with Phrase in both models.</p> <h2 id="5-discussion-of-llms-in-linguistic-research">5 Discussion of LLMs in linguistic research</h2> <p>The implications of the results as well as the methodical approach presented in this contribution appear to be so extensive that I will only provide a few thoughts in this section.</p> <h3 id="51-interpretation-of-results">5.1 Interpretation of results</h3> <p>To begin with, the results from the experiment with GPT 3.5 Turbo as well as with human participants clearly indicate the presence of Principle C (hypothesis H1). Generally, questions of type Q2 were answered in such a fashion that the pronoun in the given example was only referring to the corresponding R-expression from the main clause whenever the second R-expression was part of an embedded object. An increase of positive proportions for Q2 is noticed when the second R-expression is part of an embedded subject phrase.</p> <p>The disagreement between the LLM and human responses with respect to hypothesis H2(a&amp;b) aggravates the question of whether reconstruction is part of processing sentences with Principle C. While the results from 32 participants indicate an effect of movement which Salzmann et al. (2022) interpret as possible evidence for reconstruction, that is, the application of Principle C in Syntax under the assumption that the pre-moved position is the key to its force, the large language model responses do not indicate such an effect. This raises multiple questions: Does a language model operate differently when applying Principle C than human beings? Is movement the right variable for investigating reconstruction? Do human beings really reconstruct? Salzmann et al. (2022, 607) also question their results from Experiment 2: ”How can the finding that the base position plays a role (pointing toward reconstruction) be reconciled with the finding that the surface position also matters (speaking against reconstruction)?” In other words, an effect of movement is attested, but it is difficult to reconcile with the fact that the surface position of the moved phrase also influences the participants’ interpretations. Given that the proportions of answers from their experiment does not lead to clear numbers, and that the authors attempt to avoid a logic of arbitrary thresholds (e.g., share of proportions below 20% should be considered as noise), they reject strong reconstruction with Principle C as the governing factor of coreference in German A ́-movements. The LLM’s results appear to confirm this conclusion because the LLM’s responses were not considerably influenced by movement.</p> <p>The effect of position (hypothesis H3) is confirmed with the LLM. The results from 32 participants indicated that the adjunct and argument position were not treated differently and for that matter H3 was rejected in Salzmann et al. (2022). However, the LLM corroborates the established view (Lebeaux, 1991) that there is an asymmetry between the argument and adjunct position as <em>Position</em> led to a significant effect in the generalized linear mixed model that was fit with the LLM’s responses.</p> <h3 id="52-the-use-of-llms-in-linguistic-research">5.2 The use of LLMs in linguistic research</h3> <p>The idea of using models to simulate responses in linguistic experiments has been previously explored with smaller language models based on recurrent neural networks (RNNs), as seen in Linzen et al. (2016) and Futrell et al. (2019). This work culminated in the development of <a href="https://syntaxgym.org/">SyntaxGym</a>, a platform led by Jennifer Hu, Jon Gauthier, Ethan Wilcox, Peng Qian, and Roger Levy at the MIT Computational Psycholinguistics Laboratory. SyntaxGym’s objective is to evaluate how well deep neural network models align with human intuitions about grammatical phenomena. Transformer-based models were later integrated into the platform. Additionally, research comparing neural language models with human responses in psycholinguistic studies is presented in Wilcox et al. (2021), while Li et al. (2024) examine the alignment between humans and large language models in processing garden-path sentences. Haider (2023) offers a qualitative perspective on potential applications of large language models in linguistic research. Nevertheless, the scientific validity of using language models in this context remains a topic of debate.</p> <p>At the outset, we might wonder: ”Do large language models (LLMs) constitute a computational account of how humans process language?” (Blank, 2023, 987) For Blank, this question is inherently linked to how the mind is understood: Does it work on symbolic or distributed structures? I cannot discuss this question in more detail here. But I would like to suggest a pragmatic affirmation of the initial question: Let us assume that current LLMs can serve as an account of how humans process language. As it appears to me, we would then find ourselves in a situation that might be abstracted as follows:</p> <blockquote> <p>Syntax Theory ← Human Language → Large Language Model</p> </blockquote> <p>where both, syntax theories and LLMs, approximate the workings and patterns of human language. The former does so based on symbolic rules, whereas the latter is running on ”distributed patterns of activation across hidden units within LLMs (i.e., the ‘neurons’ of the LLM)” (Blank, 2023, 987). If we do research in linguistics, for example syntax, by using LLMs in experiments, I believe that we would look at the following situation:</p> <blockquote> <p>Syntax Theory ← Large Language Model ← Human Language</p> </blockquote> <p>This line of research seems acceptable, if we agree that the LLM is good enough in approximating human language, although it would certainly have substantial differences compared with human participants (e.g., the LLM’s linguistic capacity is not grounded in a ”human-like trajectory of language acquisition” (Blank, 2023, 988)).</p> <p>The application of large language models (LLMs) in empirical syntax, as proposed in Chapter 4, serves as further evidence that they can enhance theory testing. In general, this research approach aims not only to simulate human language utterances using LLMs for specific research purposes but also to generate linguistic judgments (e.g., by prompting ”Yes” or ”No” responses from an LLM). The data produced by LLMs could then be used to support linguistic theories, such as providing evidence for the existence of Principle C in German.</p> <p>If this approach proves successful and scientifically valid, using LLM simulations could significantly streamline linguistic research. Researchers would no longer need to recruit participants, create filler items for distraction, or rely on laboratory settings for experiments. Instead, a syntactician could simply return to her armchair, testing theories with a computer and proper access to an LLM.</p> <p>However, a significant dependence on the underlying LLMs would also ensue. If the LLM does not represent human language appropriately, its errors would be propagated and syntactic theories might be accepted or rejected based on the LLM’s incomplete representation of human language. The decision to split up the questions Q1 and Q2 for separate queries to reduce the complexity of the task (also consider the one-shot prompt that rather ”confused” the LLM) indicates that GPT 3.5 Turbo is still processing language differently than human beings. This suggests, if an LLM is used in syntactic research, we must consider its limits and capacities thoroughly before we deploy it.</p> <h3 id="53-the-implication-of-linguistic-experiments-for-llms">5.3 The implication of linguistic experiments for LLMs</h3> <p>Deploying LLMs in linguistic research would also help with understanding and possibly improving LLMs. Begus et al. (2023) argue that ”testing complex metalinguistic abilities of large language models is now a possible line of inquiry” and ”that the results of such tests can provide valuable insights into LLMs’ general metacognitive abilities.” (Begus et al., 2023) For example, Begus et al. (2023) created several tests to specifically check how GPT 4 provides formal and theoretical answers to syntactic, semantic, and phonological questions. In similar vain, Wilson et al. (2023) used linguistically inspired tests to investigate how LLMs process argument structures. In particular, they investigated if LLMs can generalize from argument patterns that are recognizable in different contexts. They report that RoBERTa, BERT, and DistilBERT fail some of their tests because they observed that these models are prone to errors ”based on surface level properties like the relative linear order of corresponding elements.” (Wilson et al., 2023, 1391) (This could be a reason why GPT 3.5 Turbo did not show signs of reconstruction. ) Therefore, such experiments would not only help us with understanding the LLMs’ limits, but could also be used for improving future models.</p> <p>The deployment of GPT 3.5 Turbo for pronoun resolution in contexts that are covered by Principle C has shown that few shot learning does not improve the model’s answers. The ambiguous example given in the instruction rather nudged the LLM into accepting more syntactically incorrect interpretations than without. This suggests that few-shot learning may not always be helpful for LLMs if ambiguous phrases are at stake. Furthermore, GPT 3.5 Turbo was generally sensitive to the prompt. An even shorter wording for the task led in manual experimentation to diminished results. This finding corroborates the recent discussion of LLM’s fickleness by Fourrier et al. (2024).</p> <h2 id="project-scripts">Project scripts</h2> <p>The scripts used for the experiments can be found here: <a href="https://github.com/omseeth/using_llms_for_syntactic_research">https://github.com/omseeth/using_llms_for_syntactic_research</a></p> <h2 id="appendix">Appendix</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P_ger-480.webp 480w,/assets/img/using_llms/P_ger-800.webp 800w,/assets/img/using_llms/P_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P_ger.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I would like to thank Martin Salzmann for providing me with the materials and experimental data from Salzmann et al. (2022).</p> <h2 id="bibliography">Bibliography</h2> <p>Adger, D., Drummond, A., Hall, D., and van urk, C. (2017). Is there condition c reconstruction? In Lamont, A. and Tetzloff, K., editors, Nels 47: Proceedings of the forty-seventh annual meeting of the North East Linguistic Society, volume 1, pages 21–30. GLSA.</p> <p>Bates, D. M., Kliegl, R., Vasishth, S., and Baayen, H. (2015). Parsimonious mixed models. arXiv: Methodology.</p> <p>Begus, G., Dabkowski, M., and Rhodes, R. (2023). Large linguistic models: An- alyzing theoretical linguistic abilities of llms. arXiv: Computation and Language.</p> <p>Blank, I. A. (2023). What are large language models supposed to model? Trends in Cognitive Sciences, 27(11):987–989.</p> <p>Bruening, B. and Khalaf, E. A. (2018). No argument–adjunct asymmetry in reconstruction for binding condition c. Journal of Linguistics, 55:247–276. Citko, B. (1993). Deletion Under Identity in Relative Clauses. Proceedings of the North Eastern Linguistic Society (NELS), 31:131–145.</p> <p>Fourrier, C., Louf, R., and Kurt, W. (2024). Improving prompt consistency with structured generations. https://huggingface.co/blog/evaluation-structured-outputs.</p> <p>Futrell, R., Wilcox, E., Morita, T., Qian, P., Ballesteros, M., and Levy, R. (2019). Neural language models as psycholinguistic subjects: Representations of syntactic state. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 32–42, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Haider, H. (2023). Is chat-gpt a grammatically competent informant? ling-buzz/007285.</p> <p>Huang, J. (1993). Reconstruction and the Structure of VP: Some Theoretical Consequences. Linguistic Inquiry, 24(1):103–138.</p> <p>Lebeaux, D. (1991). Relative Clauses, Licensing, and the Nature of the Derivation. Perspectives on Phrase Structure: Heads and Licensing, 25:209–239.</p> <p>Li, A., Feng, X., Narang, S., Peng, A., Cai, T., Shah, R. S., and Varma, S. (2024). Incremental comprehension of garden-path sentences by large language models: Semantic interpretation, syntactic re-analysis, and attention.</p> <p>Linzen, T., Dupoux, E., and Goldberg, Y. (2016). Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535.</p> <p>Salzmann, M., Wierzba, M., and Georgi, D. (2022). Condition C in German A-movement: Tackling challenges in experimental research on reconstruction. Journal of Linguistics, 59(3):577–622.</p> <p>Stockwell, R., Meltzer-Asscher, A., and Sportiche, D. (2021). There is reconstruction for condition c in english questions. In Farinelly, A. and Hil, A., editors, Nels 51: Proceedings of the fifty-first annual meeting of the North East Linguistic Society, volume 2, pages 205–214. GLSA.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p> <p>Wilcox, E., Vani, P., and Levy, R. (2021). A targeted assessment of incremental processing in neural language models and humans. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 939–952, Online. Association for Computational Linguistics.</p> <p>Wilson, M., Petty, J., and Frank, R. (2023). How abstract is linguistic generalization in large language models? experiments with argument structure. Transactions of the Association for Computational Linguistics, 11:1377–1395.</p>]]></content><author><name></name></author><category term="LLM"/><category term="linguistic"/><category term="experiment"/><category term="empirical"/><category term="syntax"/><category term="referring"/><category term="expression"/><category term="Principle-C"/><summary type="html"><![CDATA[Unpublished paper exploring how to use LLMs for empirical studies of syntax]]></summary></entry><entry><title type="html">Was ist Argument Mining?</title><link href="https://omseeth.github.io/blog/2024/argument_mining/" rel="alternate" type="text/html" title="Was ist Argument Mining?"/><published>2024-10-06T10:05:00+00:00</published><updated>2024-10-06T10:05:00+00:00</updated><id>https://omseeth.github.io/blog/2024/argument_mining</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/argument_mining/"><![CDATA[<p>In Abschnitt <strong>1</strong> stelle ich eine Definition für Argumente vor und gehe auf das Argumentieren als kommunikatives Phänomen ein. In dem zweiten Abschnitt <strong>2</strong> erläutere ich Modellierungen von Argumenten. Abschließend skizziere ich in <strong>3</strong> eine Übersicht zu Argument Mining als Aufgabe der natürlichen Sprachverarbeitung.</p> <h2 id="1-argumentieren-als-sprachliches-und-kommunikatives-phänomen">1 Argumentieren als sprachliches und kommunikatives Phänomen</h2> <p>Ein Argument kann nach der <em>Stanford Encyclopedia of Philosophy</em> wie folgt definiert werden: “as a complex symbolic structure where some parts, known as the premises, offer support to another part, the conclusion” (Dutilh Novaes, 2022). Ein Argument ist also ein Gebilde, das mindestens aus zwei Teilen besteht – wobei wir die Definition noch um ein Gebilde aus <em>Propositionen</em> (d.h. wahrheitsfähigen Aussagen) erweitern sollten. In einem Argument stehen nach der Definition mindestens zwei Propositionen insoweit zueinander, sodass eine der Propositionen in einer stützenden Beziehung zu einer anderen steht. Die letztere wird als Konklusion bezeichnet. Geläufig sind aber auch Bezeichnungen wie ‘Behauptung’ oder ‘Standpunkt’.</p> <p>Die Stützkraft einer Prämisse kann unterschiedlich definiert werden (Dutilh Novaes, 2022). Prämissen können die Wahrheit einer Konklusion garantieren; sie können sie wahrscheinlicher oder akzeptabler machen. Auf der Grundlage dieser Einsicht können Argumente auch als deduktiv, induktiv, und abduktiv typisiert werden. Die erste umfangreiche Untersuchung zum deduktiven und induktiven Argumentieren wurde bereits in der Antike von Aristoteles (2007) vorgelegt. Deduktive Argumente führen zu einer Konklusion, die durch die Wahrheitswerte der Prämissen garantiert werden kann. Bei induktiven Argumenten werden Prämissen meist in Form von Beobachtungen zur Regularitäten vorgelegt, die in die Zukunft gerichtete Konklusionen wahrscheinlich machen. Der Philosoph Charles Sanders Peirce führt in seinen Vorlesungen in den 1860er Jahren schließlich das abduktive Argument (1982) ein, das sich von einem induktiven insofern unterscheidet, als das bereits einige Beobachtungen eine Konklusion möglicherweise erklären können und sie auf diese Weise akzeptabel erscheinen lassen.</p> <p>Der Austausch von Argumenten kann als Argumentieren bezeichnet werden. Es ist eine dialogische Praxis, bei der meist die Behauptung einer Aussage zur Folge hat, dass diese weiter explizit mit Prämissen gestützt werden muss, um potentiell von einem Dialogpartner akzeptiert zu werden. Während einer Argumentation lässt sich beobachten, dass Prämissen auch genutzt werden, um Behauptungen zu untergraben. Allgemein wird beim Argumentieren vorausgesetzt, dass alle DialogpartnerInnen rational und ernsthaft an einem Austausch interessiert sind. Gleichzeitig ist die Sprache der Argumente nach Stede und Schneider (2019) oft subjektiv geprägt. (Dies mag verwundern, wenn zumindest der Anspruch eines ernsthaften Argumentierens eine inter-subjektive Übereinkunft ermöglichen sollte.)</p> <p>Für Dutilh Novaes (2022) gibt es drei Ziele des Argumentierens. Zum einen können Meinungsverschiedenheiten durch Argumente aufgelöst werden, das heißt das Argumentieren dient dazu, einen Konsens herbeizuführen (1). Es kann jedoch zum anderen in einer rein adversarialen Auseinandersetzung verhaftet bleiben, zum Beispiel wenn politische KandidatInnen in öffentlichen Debatten ausschließlich eine Zuhörerschaft von ihrem Standpunkt überzeugen wollen (2). Nach Dutilh Novaes (2022) wird in den Wissenschaften das Argumentieren auch als eine epistemische Praxis eingesetzt, mit der Absicht, das Wissen aller Beteiligten zu erweitern (3).</p> <p>Der Verweis auf das tatsächliche Argumentieren deutet in eine Richtung, bei der sich klassische Konzepte der Argumentation von der kommunikativen Praxis unterscheiden. Argumente sind keinesfalls immer gut durchdacht und erfolgreich in ihrer Zielsetzung, ein Gegenüber zu überzeugen. Ein Argument, das weder wahrheitserhaltend noch einen Standpunkt wahrscheinlich oder akzeptabel macht, wird als ‘Fehlschluss’ bezeichnet. In ihrer <em>Introduction to Logic</em> stellen Copi und Cohen (2005, S. 126-7) mindestens 17 Typen solcher Fehlschlüsse vor. Zum Beispiel wird häufig versucht, das Gegenüber durch das Hervorrufen von Mitleid von einer Behauptung zu überzeugen.</p> <h2 id="2-modellierung-von-argumenten">2 Modellierung von Argumenten</h2> <p>Zur Modellierung von Argumenten haben sich im 20. Jahrhundert drei Ansätze etabliert, die jeweils unterschiedliche Schwerpunkte setzen: das Toulmin-Modell, die <em>Neue Rhetorik</em>, und eine Modellstruktur nach Dung (1995).</p> <p>Das Toulmin-Modell (Toulmin, 2003) teilt Argumentkomponenten in sechs mögliche Typen auf: <em>Claim</em>, <em>Data</em>, <em>Warrant</em>, <em>Backing, *Qualifier</em>, <em>Rebuttal</em>. Dabei besteht jedes Argument aus mindestens einer Behauptung (<em>Claim</em>), die durch Annahmen (<em>Data</em>) gestützt wird, und zwar auf der Grundlage einer Art Garantie (<em>Warrant</em>), die Annahmen und Behauptung miteinander verbindet. Die Garantie kann noch durch zusätzliche Aussagen gestützt werden (<em>Backing</em>). Das Toulmin-Modell lässt auch widerlegende Einschübe (<em>Rebuttal</em>) und Qualifizierungen der Komponenten (<em>Qualifier</em>) zu. Der Fokus des Toulmin-Modells liegt nicht auf einen Schlusstyp (sowohl induktive als auch deduktive Argumente können mit dem Schema modelliert werden). Es begrenzt sich allerdings auf die interne Struktur <em>eines</em> Argumentes.</p> <p>Die <em>Neue Rhetorik</em> nach Perelman und Olbrechts-Tyteca (1991) legt ihren Schwerpunkt auf die Überzeugungskraft von Argumenten. Entscheidender Faktor des Argumentierens sind für die AutorInnen die Zuhörerschaft und wie diese für einen Standpunkt rhetorisch gewonnen werden kann. Deshalb sollten Argumente der Rhetorik nach insbesondere anhand ihrer perlokutionären Beschaffenheit verstanden und bewertet werden.</p> <p>Dung (1995) hingegen modelliert nicht die interne Struktur eines Argumentes, sondern wie mehrere Argumente zueinander stehen. Im Fokus seines Ansatzes sind Angriffsbeziehungen zwischen Argumenten. Dung (1995) führt auch eine neue Abstraktionsebene ein: Jedes Argument kann als Knoten eines Graphen interpretiert werden. Die Kanten des Graphen entsprechen Angriffen, wobei die Richtung der Kante zwischen zwei Argumenten auf das angegriffene Argument zeigt.</p> <p>Eine offene Frage bleibt an dieser Stelle, inwieweit Argumente als Bäume abstrahiert werden können. Ein Baum ist ein spezieller Graph, bei dem alle Knoten zusammenhängen und die Kanten keine Kreise bilden. Ansätze zur Modellierung von Argumenten (Palau und Moens, 2009; Peldszus und Stede, 2013; Musi et al., 2018; Hewett et al., 2019), die beispielsweise auch Schnittmengen mit der <em>Rhetorical Structure Theory</em> (Mann und Thompson, 1988) haben, interpretieren Argumente oft als gewurzelte Bäume, bei denen die Konklusion der Wurzel entspricht und die übrigen Komponenten auf diese mit gerichteten Kanten verweisen. In einer Korpusanalyse zu Argumenten geben (Niculae et al., 2017, S. 986) wiederum an, dass um die 20\% ihrer analysierten Argumentstrukturen nicht durch Bäume dargestellt werden können. Auch Pietron et al. (2024) stellen hierarchische Modellierungen von Argumenten mit Bäumen infrage, da Argumente in der kommunikativen Praxis vielfältigere Strukturen aufweisen würden.</p> <h2 id="3-argument-mining-in-der-natürlichen-sprachverarbeitung">3 Argument Mining in der natürlichen Sprachverarbeitung</h2> <p>Das <em>Argument Mining</em> hat sich in der natürichen Sprachverarbeitung zu Beginn des 21. Jahrhunderts, insbesondere ab den 2010er Jahren, als ein Forschungsbereich entwickelt, bei dem die automatische Identifikation und Extraktion von Argumenten und deren Strukturen verfolgt wird. Die Datengrundlage bildet meist geschriebene Sprache, in der Argumente präsentiert werden, wobei weniger literarische Texte genutzt werden, sondern vornehmlich Essays zu kontroversiellen Themen oder Kommentare aus dem Internet zu meist politischen Themen.</p> <p>Eine allgemein anerkannte Definition des <em>Argument Minings</em> gibt es in der Wissenschaftsgemeinschaft nicht. Palau und Moens (2009), Peldszus und Stede (2013), Persing und Ng (2016), Eger et al. (2017), Stede und Schneider (2019), oder Lawrence und Reed (2019) präsentieren jeweils unterschiedliche Definitionen. Ein sehr detaillierter Ansatz wird von \citeStede und Schneider (2019) vorgelegt. Demnach lässt sich <em>Argument Mining</em> in folgende sieben Aufgaben aufteilen (gekürzte Übersetzung aus Stede und Schneider, 2019, S.6-7):</p> <ul> <li>Identifizierung von argumentativem Text</li> <li>Segmentiertung des Textes in argumentative Diskurseinheiten</li> <li>Identifizierung der zentralen Behauptung</li> <li>Identifizierung der Rolle oder Funktion der Einheiten</li> <li>Identifizierung von Relationen zwischen den Einheiten</li> <li>Aufbau einer strukturellen Repräsentation</li> <li>Identifizierung von Typen und der Qualität der Argumentation</li> </ul> <p>Eine prägnantere Definition legen Palau und Moens (2009, S. 5) vor. Ihrer Definition nach lässt sich <em>Argument Mining</em> in drei Aufgaben teilen: (1) Identifizierung von Argumenten im Text, (2) Identifizierung der internen Struktur der Argumente, und (3) Identifizierung von Interaktionen zwischen Argumenten. Während Stede und Schneider (2019) den Schwerpunkt ihrer Definition eher auf die Mikrostruktur eines Argumentes legen (Aufgaben 2.-5.), gehört für Palau und Moens (2009) die Makrostruktur zwischen mehreren Argumenten zu gleichen Teilen zum Aufgabenbereich des <em>Argument Minings</em>. Palau und Moens (2009) lassen wiederum eine qualitative Bewertung von Argumenten außen vor.</p> <p>Es gibt auch keine Einheitlichkeit bei der Frage, was die ‘Bausteine’ des ‘Argument Minings’ sind. Bei den möglichen Argumentkomponenten gibt es sowohl Ansätze mit unterschiedlicher Granularität als auch welche, die die Rollen der Argumentkomponenten umdeuten. Palau und Moens (2009) und Feng und Hirst (2011) nutzen zum Beispiel den <em>Araucaria</em>-Korpus (Reed, 2006), bei dem Argumentkomponenten als Prämissen und Konklusionen repräsentiert werden. Peldszus (2014), der den vorläufigen <em>Microtext</em>-Korpus entwickelt, weist Argumentkomponenten noch besondere Rollen zu, die als Opponent und Proponent bezeichnet werden, und sich aus der dialogischen Form des Argumentierens mit einem Für und Wider ableiten. Stab und Gurevych (2014a) nutzen den <em>Persuasive Essay</em>-Korpus, nach welchem Argumente aus Prämissen, Behauptungen, und zentralen Behauptungen (<em>Major Claim</em>) bestehen. Für Rinott et al. (2015) bestehen argumentative Passagen aus einem Thema, einer Behauptung sowie kontextabhängiger Evidenz. Niculae et al. (2017) nutzen den CDCP-Korpus, der keine argumentativen Komponenten von anderen Propositionen unterscheidet, sondern die letzteren in fünf Klassen (<em>Fact</em>, <em>Policy</em>, <em>Reference</em>, <em>Testimony</em>, <em>Value</em>) aufteilt und unter diesen Beziehungen herstellt. (Dass Wertaussagen (<em>Policy</em>, <em>Value</em>) auch als Propositionen, das heißt Aussagen mit Wahrheitsgehalt gewertet werden, wird vorausgesetzt.) Ein hybrider Ansatz wird schließlich von Schaefer et al. (2023) vorgestellt, wonach Behauptungen weiter als <em>Fact</em>, <em>Policy</em>, und <em>Value</em>, und Prämissen als <em>Testimony</em>, <em>Statistics</em>, <em>Hypthetical Instance</em>, <em>Real-example</em> und <em>Common-ground</em> typisiert werden können.</p> <p>Ein ebenfalls heterogenes Bild gibt es mit Hinsicht möglicher Relationen zwischen Argumentkomponenten. Palau und Moens (2009) geben keine konkreten Relationen an, sondern stützen sich auf eine Vielfalt von möglichen Strukturen nach Walton et al. (2008) (z.B. <em>fulfillment</em> oder <em>causal</em> Relationen, oder verschiedene Angriffsrelationen wie <em>Rebuttal</em> oder <em>Undercutter</em>), die sie implizit mit Regeln einer kontextfreien Grammatik einzufangen versuchen. Peldszus (2014) folgt in Teilen dem Toulmin-Modell und definiert fünf mögliche Argumentrelationen: <em>Support</em>, <em>Example</em>, <em>Rebuttal</em>, <em>Undercutter</em>, <em>Linked</em>. Für Stab und Gurevych (2014a) gibt es nur <em>Support</em> und <em>Attack</em>. Rinott et al. (2015) beschränken wiederum mögliche Relationen zwischen argumentativen Propositionen auf rein unterstützende. Ähnlich verfahren auch Park und Cardie (2018), für die es bei der <em>Support</em>-Relation noch eine Unterteilung in <em>Reason</em> und <em>Evidence</em> gibt.</p> <p>In dem weiteren Verlauf werde ich für die drei Bereiche des <em>Argument Minings</em> nach der kompakteren Definition von Palau und Moens (2009) beispielhaft einige Ansätze vorstellen.</p> <p>Die Identifizierung von Argumenten in einem Text kann über die Segmentierung von Texteinheiten in argumentative oder nicht-argumentative Passagen realisiert werden. Für dieses Ziel nutzen Ajjour et al. (2017) einen Ansatz, bei dem sie für jeden möglichen Token eines Textes ein entsprechendes Label ermitteln. Für ihren Ansatz verwenden sie Texte aus drei Korpora unterschiedlicher Domänen (Essays, Nachrichten, online Kommentare). Die Modelle ihrer Experimente unterscheiden sich, inwieweit sie umliegende Tokens für die Klassifikation mit berücksichtigen: Eine lineare <em>Support Vector Machine</em> mit <em>Features</em> klassifiziert nur jeweils einen Token für sich, ein <em>Conditional Random Field</em> für Sequenzen berücksichtigt hingegen umliegende Tokens innerhalb eines Fensters. Als drittes lassen Ajjour et al. (2017) in ihrem bidirektionalen <em>Long Short-Term Memory</em>-Modell die Informationen aller vorangegangen und nachfolgenden Tokens mit in die Labelprädiktion eines jeden Tokens einfließen.</p> <p>Die Analyse der internen Struktur eines Argumentes kann über die Klassifikation von Argumentkomponenten- und relationen erreicht werden. Auf der Grundlage des <em>Persuasive Essay</em>-Korpus nutzen Stab und Gurevych (2014b) ebenfalls ein Modell basierend auf einer <em>Support Vector Machine</em>. Stab und Gurevych (2014b) entwickeln dafür umfangreiche <em>Feature</em>-Sets, die auf der Grundlage struktureller Merkmale wie Interpunktion oder Satzlänge, lexikalischer Merkmale wie <em>n-grams</em>, von Adverbien, syntaktischer Merkmale, einschlägiger Diskursmarker, oder kontextueller Merkmale wie umliegenden Sätze oder Anzahl von Nebensätzen eine Prädiktion machen. Im Falle der Komponenten klassifizieren sie, ob es sich um Prämisse, Behauptung, oder zentrale Behauptung, und im Falle der Relationen, ob es sich um <em>Support</em>-Relationen handelt.</p> <p>Schließlich kann das Verhältnis von Argumenten zueinander aus einer Makroperspektive wie bei Dung (1995) untersucht werden. Carstens und Toni (2015) nehmen dafür eine Minimaldefinition eines Argumentes an, wonach bereits einzelne Sätze argumentativen Gehalt haben und damit als Argumente behandelt werden können. Unter dieser Annahme klassifizieren sie ausschließlich Relationen (<em>Support</em> und <em>Attack</em>) zwischen mehreren Argumenten. Dafür nutzen Carstens und Toni (2015) ein <em>Random Forest</em>-Modell mit einem selbst entwickelten Korpus. Carstens und Toni (2015) vertreten die Annahme, dass sich aus der Bestimmung einer Relation automatisch ableiten lässt, welche Sätze auch Argumente sind.</p> <h2 id="zusätzliche-resourcen">Zusätzliche Resourcen</h2> <p>Ich klassifiziere Propositionen und deren argumentative Relationen mit BERT im folgenden Projekt: <a href="https://github.com/omseeth/AM_BERT_classification">Argument Mining with BERT classification</a></p> <h2 id="bibliographie">Bibliographie</h2> <p>Ajjour, Y., Chen, W.-F., Kiesel, J., Wachsmuth, H., und Stein, B. (2017). Unit Segmentation of Argumentative Texts. In Habernal, I., Gurevych, I., Ashley, K., Cardie, C., Green, N., Litman, D., Petasis, G., Reed, C., Slonim, N., und Walker, V., Herausgeber, Proceedings of the 4th Workshop on Argument Mining, Seiten 118–128, Copenhagen, Denmark. Association for Computational Linguistics.</p> <p>Aristoteles (2007). BAND 3/I.1 Analytica priora. Buch I. Akademie Verlag, Berlin.</p> <p>Copi, I. und Cohen, C. (2005). Introduction to Logic. Pearson/Prentice Hall.</p> <p>Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence, 77(2):321–357</p> <p>Dutilh Novaes, C. (2022). Argument and Argumentation. In Zalta, E. N. und Nodelman, U., Herausgeber, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2022. Auflage.</p> <p>Eger, S., Daxenberger, J., und Gurevych, I. (2017). Neural End-to-End Learning for Computational Argumentation Mining. In Barzilay, R. und Kan, M.-Y., Herausgeber, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Seiten 11–22, Vancouver, Canada. Association for Computational Linguistics.</p> <p>Feng, V. W. und Hirst, G. (2011). Classifying arguments by scheme. In Lin, D., Matsumoto, Y., und Mihalcea, R., Herausgeber, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Seiten 987–996, Portland, Oregon, USA. Association for Computational Linguistics.</p> <p>Hewett, F., Prakash Rane, R., Harlacher, N., und Stede, M. (2019). The Utility of Discourse Parsing Features for Predicting Argumentation Structure. In Stein, B. und Wachsmuth, H., Herausgeber, Proceedings of the 6th Workshop on Argument Mining, Seiten 98–103, Florence, Italy. Association for Computational Linguistics.</p> <p>Lawrence, J. und Reed, C. (2019). Argument Mining: A Survey. Computational Linguistics, 45(4):765–818.</p> <p>Mann, W. C. und Thompson, S. A. (1988). Rhetorical Structure Theory: Toward a functional theory of text organization. Text - Interdisciplinary Journal for the Study of Discourse, 8(3):243–281.</p> <p>Musi, E., Alhindi, T., Stede, M., Kriese, L., Muresan, S., und Rocci, A. (2018). A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations. In International Conference on Language Resources and Evaluation.</p> <p>Niculae, V., Park, J., und Cardie, C. (2017). Argument Mining with Structured SVMs and RNNs. In Barzilay, R. und Kan, M.-Y., Herausgeber, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Seiten 985–995, Vancouver, Canada. Association for Computational Linguistics.</p> <p>Palau, R. M. und Moens, M.-F. (2009). Argumentation mining: the detection, classification and structure of arguments in text. In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09, Seite 98–107, New York, NY, USA. Association for Computing Machinery.</p> <p>Park, J. und Cardie, C. (2018). A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments. In Calzolari, N., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Hasida, K., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S., und Tokunaga, T., Herausgeber, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p> <p>Peirce, C. S. (1982). Writings of Charles S. Peirce: A Chronological Edition, Volume 5: 1884-1886. Indiana University Press</p> <p>Peldszus, A. (2014). Towards segment-based recognition of argumentation structure in short texts. In Green, N., Ashley, K., Litman, D., Reed, C., und Walker, V., Herausgeber, Proceedings of the First Workshop on Argumentation Mining, Seiten 88–97, Baltimore, Maryland. Association for Computational Linguistics.</p> <p>Peldszus, A. und Stede, M. (2013). From Argument Diagrams to Argumentation Mining in Texts: A Survey. Int. J. Cogn. Inform. Nat. Intell., 7(1):1–31.</p> <p>Pietron, M., Olszowski, R., und Gomu lka, J. (2024). Efficient argument classification with compact language models and ChatGPT-4 refinements.</p> <p>Perelman, C. und Olbrechts-Tyteca, L. (1991). The New Rhetoric: A Treatise on Argumentation. University of Notre Dame Press.</p> <p>Persing, I. und Ng, V. (2016). End-to-End Argumentation Mining in Student Essays. In Knight, K., Nenkova, A., und Rambow, O., Herausgeber, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seiten 1384–1394, San Diego, California. Association for Computational Linguistics.</p> <p>Reed, C. (2006). Preliminary results from an argument corpus. In Berm´udez, E. M. und Miyares, L. R., Herausgeber, Linguistics in the Twenty-first Century, Seiten 185––196. Cambridge Scholars Press.</p> <p>Rinott, R., Dankin, L., Alzate Perez, C., Khapra, M. M., Aharoni, E., und Slonim, N. (2015). Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection. In M`arquez, L., Callison-Burch, C., und Su, J., Herausgeber, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Seiten 440–450, Lisbon, Portugal. Association for Computational Linguistics.</p> <p>Schaefer, R., Knaebel, R., und Stede, M. (2023). Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays. In Alshomary, M., Chen, C.-C., Muresan, S., Park, J., und Romberg, J., Herausgeber, Proceedings of the 10th Workshop on Argument Mining, Seiten 76–88, Singapore. Association for Computational Linguistics.</p> <p>Stab, C. und Gurevych, I. (2014a). Annotating Argument Components and Relations in Persuasive Essays. In Tsujii, J. und Hajic, J., Herausgeber, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, Seiten 1501–1510, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.</p> <p>Stab, C. und Gurevych, I. (2014b). Identifying Argumentative Discourse Structures in Persuasive Essays. In Moschitti, A., Pang, B., und Daelemans, W., Herausgeber, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seiten 46–56, Doha, Qatar. Association for Computational Linguistics.</p> <p>Stede, M. und Schneider, J. (2019). Argumentation Mining. Morgan Claypool, San Rafael (CA).</p> <p>Toulmin, S. E. (2003). The Uses of Argument. Cambridge University Press, 2. Auflage.</p> <p>Walton, D., Reed, C., und Macagno, F. (2008). Argumentation Schemes. Cambridge University Press.</p>]]></content><author><name></name></author><category term="argument"/><category term="mining"/><category term="NLP"/><category term="proposition"/><category term="conclusion"/><summary type="html"><![CDATA[Eine Einführung zum Forschungsfeld des Argument Minings]]></summary></entry><entry><title type="html">Questioning Recursion as a Distinctive Feature of Human Language</title><link href="https://omseeth.github.io/blog/2024/recursive_language/" rel="alternate" type="text/html" title="Questioning Recursion as a Distinctive Feature of Human Language"/><published>2024-10-03T10:00:00+00:00</published><updated>2024-10-03T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/recursive_language</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/recursive_language/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>How can we define language? What difference is there between a string of random sounds and a meaningful combination of signs, communicating information between two beings? A first approximation toward a definition of language could be by postulating that language is a way of representing the world in terms of material entities, such as sounds. We might call these entities signs with meaning. While this is a definition of language broadly speaking, we might want to add that such signs can also be systematically combined with each other, resulting in more complex expressions. By adding this aspect, we arrive at a narrower definition that language is a system of signs with combinatory rules.</p> <p>According to Noam Chomsky, this narrow definition is the basis for what makes human language unique. In more detail, Chomsky argues with Marc Hauser and W. Tecumseh Fitch (2002, abbreviated henceforth as HCF) that humans cannot only combine expressions, but they can do so endlessly by embedding phrases within other phrases. They call this aspect ‘recursion’. According to HCF, this is the distinctive feature that distinguishes human language from communicative systems of animals.</p> <p>The goal of this essay is to discuss HCF’s claim that recursion is the distinctive property of human language. I begin with outlining recursion and explain in what sense recursion appears to be unique to human language. In the second part of this essay, I present ornithological findings. I introduce how starlings demonstrate recursive understanding by recognizing different grammar types. Finally, I propose the hypothesis that the difference between human language and communicative systems used by animals, such as birds, is rather a matter of quantity than quality.</p> <h2 id="an-overview-of-recursion">An Overview of Recursion</h2> <p>Recursive structures are omnipresent in our lives: be it in terms of counting, be it in terms of speaking. Roughly put, recursion can be understood as a function calling upon itself repeatedly. As a first attempt, we might formalize this idea as follows: \(R → R\). But what is more important, this function can even be repeated when additional information is being added. Consider: \(R → R+a\). Such a function provides us with one explanation of how we count. For instance, the rules \(S → 0C\), \(C → C+1\), \(C → ε\) will lead to any natural number, depending on how many times \(C\) is being repeated, until the loop is being terminated with \(ε\). In a similar vein, natural language appears to make extensive use of recursion. This aspect of language is also prominently evident in children’s memory games, such as the ‘My mother went to market’ game where each player must add an item of free choice to the initial phrase, including everything that has previously been said by other players. More complex examples of recursion in language include sentences within sentences. Technically, we can endlessly embed sentences, consisting of subclauses that have additional subclauses and so on. Recursion is one fundamental aspect of how we count or speak, and it is, consequently, part of every other activity or technology that is based on counting and speaking.</p> <p>According to Michael Corballis (2007), it is important to note that recursion is not mere repetition. He believes that recursion, as the principle is evident in how we speak, allows us to freely choose how things might be combined and embedded within each other. That is, recursive rules do not only allow us to repeatedly add another instance. We also can easily jump to higher numbers, skip intermediate additions, multiply terms. The children’s game that has previously been mentioned might not be the best example illustrating recursion because recursion allows us to add whole sentences, several words, or no element at all.</p> <p>While this first sketch of recursion explains a central feature of counting and speaking, it is worthy to keep in mind that human beings only use recursion in a limited sense. By observing our day-to-day activities, we see that we will stop counting at some point. Considering language, each layer that is being added to a sentence through recursion makes it harder to follow what has previously been said. This is so because we have cognitive limits. In other words, we can “overstretch [our] working memory”, says Corballis (2007, p. 244).</p> <h2 id="recursion-as-a-distinctive-feature-of-human-language">Recursion as a Distinctive Feature of Human Language</h2> <p>After comparative research in animal communication, HCF (2002) believe that recursion is the distinctive property of human natural language. While different animal species can communicate with each other in a myriad of ways, through alarm calls, by mobbing predators, by asking for help, or by sharing information about valuable food sources, HCF argue that they “lack the rich expressive and open-ended power of human language” (2002, p. 1570). HCF even grant that animals have conceptual representations of the world. However, these concepts appear closely linked to specific functions. Human concepts, on the other hand, are enmeshed in a broader system, without any “straightforward word-thing relationship”, and “can be linked to virtually any [other] concept that humans [] entertain” (Hauser et al., 2002, p. 1576). This difference brings HCF to the conclusion that we “must entertain the possibility of an independently evolved mechanism” (2002, p. 1576). The mechanism that explains the richness and open-ended structure of human natural language lies for the authors in recursion.</p> <p>With respect to the discussion of recursion in human versus animal language, avian communication is often discussed in more detail (Corballis, 2007; Hauser et al., 2002). For it appears that birds have something like recursion because of their variety of sounds. However, it is argued that birds only have a finite state grammar, that is, a set of limited rules that underlies their communication. The idea of a finite state grammar (also known as regular grammar) consists in restricted combinations of the sort that each rule is determined by the fixed pattern: \(R → aB\) and / or \(R → a\). However, context free or context sensible grammars, which are each more complex respectively, allow either for variations on the right side or variations on both sides of the rule. For this reason, context free grammars can generate expressions of the type \(R → A^n B^n\). It is assumed that this truly allows for the richness of arbitrary combinations that characterizes human language. For the structure of such a grammar can account for center-embedded phrases, such as: ‘I am looking research I need for the essay I must hand in up.’ These constructions would not be possible with a finite state grammar that is confined to a fixed production of binary patterns (see for a visualization of the difference Figure 1).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grammar_differences-480.webp 480w,/assets/img/grammar_differences-800.webp 800w,/assets/img/grammar_differences-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/grammar_differences.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> While the finite state grammar would only allow for a fixed pattern of binary addition, the context free grammar allows for center-embedding. This is an adoption of Gentner’s et al.’s figure (2006, p. 1204).</p> <h2 id="ornithological-findings-concerning-recursion-and-finite-state-grammars">Ornithological Findings Concerning Recursion and Finite State Grammars</h2> <p>There are at least two ways of how one can question the argument that the most developed animals only master a finite state grammar, whereas human beings are capable of processing more complex grammar structures. The first strategy would be to prove that animals can master a context free grammar, too. The second one could probe the power of finite state grammars, which may also account for some degree of recursion.</p> <p>As to the first strategy, Timothy Gentner and his colleagues (2006) created an experiment with the aim to train starlings recognizing different sound motifs, rattles and warbles, that they had recorded and replayed in front of the birds. The motifs were patterned to resemble either a finite state grammar, creating a string of binary sounds, or a context free grammar with center-embedding. The result from simple tests has been that the starlings could learn to recognize and to differently respond to the different sounds sequences, representing each grammar type. To corroborate their results, they further enlarged the sequences by \(n=3\) and \(n=4\) for their finite \((AB)^n\) and context free \(A^nB^n\) sound structures. Again, the birds were capable of producing different reactions to each of the structures. Gentner and his colleagues also assured that the starlings had no similar reactions to arbitrary patterns. While they leave open the possibility that the birds might have used other cognitive heuristics, such as approximating context free patterns through finite state rules, Gentner and his colleagues conclude from their experiment that “starlings can recognize syntactically well-formed strings, including those that use a recursive centre-embedding rule” (2007, p. 1206).</p> <p>The caveat to Gentner’s findings points at the second strategy of questioning the limitations of animal communication. At the outset, we should note that recursion appears in finite state grammars because these grammars can reiteratively produce an infinite string of symbols. This has already been recognized by Chomsky (1956; Chomsky &amp; Miller, 1958) in his early papers on finite languages. So the point is not that human natural language is defined by reiterative rules, but by some context free or higher grammar, allowing for greater varieties of recursion. But what if center-embedding could be achieved with the means of a finite state grammar, too? This still is an ongoing research question where syntacticians (Roche &amp; Schabes, 1997; Dacjuk &amp; van Noord, 2002) attempt to represent a more complex grammar through the means of a stricter one. A detailed discussion of this topic would go beyond this essay. However, it is worthy to keep such a possibility in mind when discussing the properties of language(s).</p> <h2 id="final-discussion">Final Discussion</h2> <p>To conclude, recursion as a reiterative rule cannot be the distinctive property characterizing human natural language because it is possible to loop through finite state grammar rules that have been attributed to animals. Although Chomsky, Hauser, and Fitch (2002) as well as Corballis (2007) speak of ‘recursion’ when singling human language out, it has been evident from the discussion that they have something slightly different in mind that goes beyond the minimal definition of recursion as a self-applicative rule. For them, recursion implies the possibility of grammatically complex structures, allowing center-embedding. For this reason, the essay has additionally discussed the possibility of context free grammars in avian communication by presenting research on starlings. Here, Gentner et al. (2006) offer empirical evidence that starlings can master complex recursive patterns, as in \(R → A^n B^n\) combinations. Therefore, it appears that recursion, neither in its simplest form, nor in its more complex applications, is the distinctive property of human language.</p> <p>While Gentner et al. (2006) believe that humans are not the only ones having recursion, they also acknowledge that there is a difference between communicative systems of birds and human natural language. Obviously, the number of words and syntactical structures over which birds have command, and which they can process, is more restricted than those of humans. We can assume that the most advanced birds use about 100-200 signals (Ballentine &amp; Hyman, 2021) and might have some basic ordering rules, of which some are even recursive. Therefore, it is fair to say that the difference between species is “quantitative rather than qualitative”, especially with respect to “distinctions in cognitive mechanisms” (Gentner et al., 2006, p. 1206). This conclusion also fits Ray Jackendoff’s and Steven Pinker’s (2005) critique of HCF’s claim that the distinction between animals’ and human beings’ communication systems should rather be gradual than categorical.</p> <p>When quantitative differences between human language and animal communication are undeniable, we should not forget that human memory is limited, too. In fact, it has been argued by Fred Karlsson (2007) that actual embedding in spoken language is almost absent and that it appears in written discourse only up to three times. While it is common to assume that a certain quantitative difference ought to be explained with a different quality, or with “an independently evolved mechanism”– as HCF (2002, p. 1576) argue – memory limits in humans makes one wonder where we should draw the line between animal language and human language. While human grammars allow for endless center-embedding in principle, it is rarely used. What if bird grammars also allow for center-embedding principally, but we just have not observed it so far?</p> <p>Another strategy to question recursion in terms of center-embedding as a distinctive property of human language would be to grant that birds only use finite state grammars. However, it would be worthwhile discussing whether these grammars could approximate recursive patterns, such as center-embedding. While there still is need for research on this topic, part of the idea is also driven by the fact that humans’ cognitive capacities are finite. So human natural language might be less context sensible or even less context free in some respects and in that less recursive, as assumed.</p> <h2 id="bibliography">Bibliography</h2> <p>Ballentine, B. &amp; Hyman, J. (2021). Bird Talk: An Exploration of Avian Communication. Cornell University Press.</p> <p>Chomsky, N. (1956). Three models for the description of language. The Institute of Radio Engineers, Inc I.R.E. transactions on information theory, 2 (3), pp. 113-124. https://doi.org/10.1109/TIT.1956.1056813</p> <p>Chomsky, N. &amp; Miller, G. (1958). Finite State Languages. Information and control, 1 (2), pp. 91-112. https://doi.org/10.1016/S0019-9958(58)90082-2</p> <p>Corballis, M. (2007). The Uniqueness of Human Recursive Thinking. American Scientist, 95 (3, May/Jun 2007), pp. 240, 242-248. https://doi.org/10.1511/2007.65.240</p> <p>Daciuk, J. &amp; van Noord, G. (2002). Finite Automata for Compact Representation of Language Models in NLP. In Watson, B.W., Wood, D. (Eds.) Implementation and Application of Automata. pp. 65-73. CIAA 2001. Lecture Notes in Computer Science, 2494. https://doi.org/10.1007/3-540-36390-4_6</p> <p>Gentner, T., Fenn, K., Margoliash, D. &amp; Nusbaum, H. (2006). Recursive syntactic pattern learning by songbirds. Nature, 440 (7088), pp. 1204–1207. https://doi.org/10.1038/nature04675</p> <p>Hauser, M., Chomsky, N. &amp; Fitch, T. (2002). The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?. Science, 298 (5598), pp. 1569-1579. https://doi.org/10.1126/science.298.5598.1569</p> <p>Jackendoff, R. &amp; Pinker, S. (2005). The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky). Cognition, 97 (2005), pp. 211-225. https://doi.org/10.1016/j.cognition.2005.04.006</p> <p>Kallmayer, L. (2010). Parsing Beyond Context Free Grammars. Springer.</p> <p>Karlsson, F. (2007). Constraints on Multiple Center-Embedding of Clauses. Journal of Linguistics, 43 (2), pp. 365-392. https://doi.org/10.1017/S0022226707004616</p> <p>Roche, E. &amp; Schabes, Y. (1997). Finite-State Language Processing. MIT Press.</p>]]></content><author><name></name></author><category term="recursion"/><category term="language"/><category term="animal"/><category term="linguistics"/><category term="finite"/><category term="state"/><category term="grammar"/><category term="context"/><category term="free"/><category term="grammar"/><summary type="html"><![CDATA[Putting human language into perspective with animal linguistics]]></summary></entry></feed>