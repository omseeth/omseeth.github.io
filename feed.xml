<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://omseeth.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://omseeth.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-04T15:20:20+00:00</updated><id>https://omseeth.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Questioning Recursion as a Distinctive Feature of Human Language</title><link href="https://omseeth.github.io/blog/2024/recursive_language/" rel="alternate" type="text/html" title="Questioning Recursion as a Distinctive Feature of Human Language"/><published>2024-10-03T10:00:00+00:00</published><updated>2024-10-03T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/recursive_language</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/recursive_language/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>How can we define language? What difference is there between a string of random sounds and a meaningful combination of signs, communicating information between two beings? A first approximation toward a definition of language could be by postulating that language is a way of representing the world in terms of material entities, such as sounds. We might call these entities signs with meaning. While this is a definition of language broadly speaking, we might want to add that such signs can also be systematically combined with each other, resulting in more complex expressions. By adding this aspect, we arrive at a narrower definition that language is a system of signs with combinatory rules.</p> <p>According to Noam Chomsky, this narrow definition is the basis for what makes human language unique. In more detail, Chomsky argues with Marc Hauser and W. Tecumseh Fitch (2002, abbreviated henceforth as HCF) that humans cannot only combine expressions, but they can do so endlessly by embedding phrases within other phrases. They call this aspect ‘recursion’. According to HCF, this is the distinctive feature that distinguishes human language from communicative systems of animals.</p> <p>The goal of this essay is to discuss HCF’s claim that recursion is the distinctive property of human language. I begin with outlining recursion and explain in what sense recursion appears to be unique to human language. In the second part of this essay, I present ornithological findings. I introduce how starlings demonstrate recursive understanding by recognizing different grammar types. Finally, I propose the hypothesis that the difference between human language and communicative systems used by animals, such as birds, is rather a matter of quantity than quality.</p> <h2 id="an-overview-of-recursion">An Overview of Recursion</h2> <p>Recursive structures are omnipresent in our lives: be it in terms of counting, be it in terms of speaking. Roughly put, recursion can be understood as a function calling upon itself repeatedly. As a first attempt, we might formalize this idea as follows: \(R → R\). But what is more important, this function can even be repeated when additional information is being added. Consider: \(R → R+a\). Such a function provides us with one explanation of how we count. For instance, the rules \(S → 0C\), \(C → C+1\), \(C → ε\) will lead to any natural number, depending on how many times \(C\) is being repeated, until the loop is being terminated with \(ε\). In a similar vein, natural language appears to make extensive use of recursion. This aspect of language is also prominently evident in children’s memory games, such as the ‘My mother went to market’ game where each player must add an item of free choice to the initial phrase, including everything that has previously been said by other players. More complex examples of recursion in language include sentences within sentences. Technically, we can endlessly embed sentences, consisting of subclauses that have additional subclauses and so on. Recursion is one fundamental aspect of how we count or speak, and it is, consequently, part of every other activity or technology that is based on counting and speaking.</p> <p>According to Michael Corballis (2007), it is important to note that recursion is not mere repetition. He believes that recursion, as the principle is evident in how we speak, allows us to freely choose how things might be combined and embedded within each other. That is, recursive rules do not only allow us to repeatedly add another instance. We also can easily jump to higher numbers, skip intermediate additions, multiply terms. The children’s game that has previously been mentioned might not be the best example illustrating recursion because recursion allows us to add whole sentences, several words, or no element at all.</p> <p>While this first sketch of recursion explains a central feature of counting and speaking, it is worthy to keep in mind that human beings only use recursion in a limited sense. By observing our day-to-day activities, we see that we will stop counting at some point. Considering language, each layer that is being added to a sentence through recursion makes it harder to follow what has previously been said. This is so because we have cognitive limits. In other words, we can “overstretch [our] working memory”, says Corballis (2007, p. 244).</p> <h2 id="recursion-as-a-distinctive-feature-of-human-language">Recursion as a Distinctive Feature of Human Language</h2> <p>After comparative research in animal communication, HCF (2002) believe that recursion is the distinctive property of human natural language. While different animal species can communicate with each other in a myriad of ways, through alarm calls, by mobbing predators, by asking for help, or by sharing information about valuable food sources, HCF argue that they “lack the rich expressive and open-ended power of human language” (2002, p. 1570). HCF even grant that animals have conceptual representations of the world. However, these concepts appear closely linked to specific functions. Human concepts, on the other hand, are enmeshed in a broader system, without any “straightforward word-thing relationship”, and “can be linked to virtually any [other] concept that humans [] entertain” (Hauser et al., 2002, p. 1576). This difference brings HCF to the conclusion that we “must entertain the possibility of an independently evolved mechanism” (2002, p. 1576). The mechanism that explains the richness and open-ended structure of human natural language lies for the authors in recursion.</p> <p>With respect to the discussion of recursion in human versus animal language, avian communication is often discussed in more detail (Corballis, 2007; Hauser et al., 2002). For it appears that birds have something like recursion because of their variety of sounds. However, it is argued that birds only have a finite state grammar, that is, a set of limited rules that underlies their communication. The idea of a finite state grammar (also known as regular grammar) consists in restricted combinations of the sort that each rule is determined by the fixed pattern: \(R → aB\) and / or \(R → a\). However, context free or context sensible grammars, which are each more complex respectively, allow either for variations on the right side or variations on both sides of the rule. For this reason, context free grammars can generate expressions of the type \(R → A^n B^n\). It is assumed that this truly allows for the richness of arbitrary combinations that characterizes human language. For the structure of such a grammar can account for center-embedded phrases, such as: ‘I am looking research I need for the essay I must hand in up.’ These constructions would not be possible with a finite state grammar that is confined to a fixed production of binary patterns (see for a visualization of the difference Figure 1).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grammar_differences-480.webp 480w,/assets/img/grammar_differences-800.webp 800w,/assets/img/grammar_differences-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/grammar_differences.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> While the finite state grammar would only allow for a fixed pattern of binary addition, the context free grammar allows for center-embedding. This is an adoption of Gentner’s et al.’s figure (2006, p. 1204).</p> <h2 id="ornithological-findings-concerning-recursion-and-finite-state-grammars">Ornithological Findings Concerning Recursion and Finite State Grammars</h2> <p>There are at least two ways of how one can question the argument that the most developed animals only master a finite state grammar, whereas human beings are capable of processing more complex grammar structures. The first strategy would be to prove that animals can master a context free grammar, too. The second one could probe the power of finite state grammars, which may also account for some degree of recursion.</p> <p>As to the first strategy, Timothy Gentner and his colleagues (2006) created an experiment with the aim to train starlings recognizing different sound motifs, rattles and warbles, that they had recorded and replayed in front of the birds. The motifs were patterned to resemble either a finite state grammar, creating a string of binary sounds, or a context free grammar with center-embedding. The result from simple tests has been that the starlings could learn to recognize and to differently respond to the different sounds sequences, representing each grammar type. To corroborate their results, they further enlarged the sequences by \(n=3\) and \(n=4\) for their finite \((AB)^n\) and context free \(A^nB^n\) sound structures. Again, the birds were capable of producing different reactions to each of the structures. Gentner and his colleagues also assured that the starlings had no similar reactions to arbitrary patterns. While they leave open the possibility that the birds might have used other cognitive heuristics, such as approximating context free patterns through finite state rules, Gentner and his colleagues conclude from their experiment that “starlings can recognize syntactically well-formed strings, including those that use a recursive centre-embedding rule” (2007, p. 1206).</p> <p>The caveat to Gentner’s findings points at the second strategy of questioning the limitations of animal communication. At the outset, we should note that recursion appears in finite state grammars because these grammars can reiteratively produce an infinite string of symbols. This has already been recognized by Chomsky (1956; Chomsky &amp; Miller, 1958) in his early papers on finite languages. So the point is not that human natural language is defined by reiterative rules, but by some context free or higher grammar, allowing for greater varieties of recursion. But what if center-embedding could be achieved with the means of a finite state grammar, too? This still is an ongoing research question where syntacticians (Roche &amp; Schabes, 1997; Dacjuk &amp; van Noord, 2002) attempt to represent a more complex grammar through the means of a stricter one. A detailed discussion of this topic would go beyond this essay. However, it is worthy to keep such a possibility in mind when discussing the properties of language(s).</p> <h2 id="final-discussion">Final Discussion</h2> <p>To conclude, recursion as a reiterative rule cannot be the distinctive property characterizing human natural language because it is possible to loop through finite state grammar rules that have been attributed to animals. Although Chomsky, Hauser, and Fitch (2002) as well as Corballis (2007) speak of ‘recursion’ when singling human language out, it has been evident from the discussion that they have something slightly different in mind that goes beyond the minimal definition of recursion as a self-applicative rule. For them, recursion implies the possibility of grammatically complex structures, allowing center-embedding. For this reason, the essay has additionally discussed the possibility of context free grammars in avian communication by presenting research on starlings. Here, Gentner et al. (2006) offer empirical evidence that starlings can master complex recursive patterns, as in \(R → A^n B^n\) combinations. Therefore, it appears that recursion, neither in its simplest form, nor in its more complex applications, is the distinctive property of human language.</p> <p>While Gentner et al. (2006) believe that humans are not the only ones having recursion, they also acknowledge that there is a difference between communicative systems of birds and human natural language. Obviously, the number of words and syntactical structures over which birds have command, and which they can process, is more restricted than those of humans. We can assume that the most advanced birds use about 100-200 signals (Ballentine &amp; Hyman, 2021) and might have some basic ordering rules, of which some are even recursive. Therefore, it is fair to say that the difference between species is “quantitative rather than qualitative”, especially with respect to “distinctions in cognitive mechanisms” (Gentner et al., 2006, p. 1206). This conclusion also fits Ray Jackendoff’s and Steven Pinker’s (2005) critique of HCF’s claim that the distinction between animals’ and human beings’ communication systems should rather be gradual than categorical.</p> <p>When quantitative differences between human language and animal communication are undeniable, we should not forget that human memory is limited, too. In fact, it has been argued by Fred Karlsson (2007) that actual embedding in spoken language is almost absent and that it appears in written discourse only up to three times. While it is common to assume that a certain quantitative difference ought to be explained with a different quality, or with “an independently evolved mechanism”– as HCF (2002, p. 1576) argue – memory limits in humans makes one wonder where we should draw the line between animal language and human language. While human grammars allow for endless center-embedding in principle, it is rarely used. What if bird grammars also allow for center-embedding principally, but we just have not observed it so far?</p> <p>Another strategy to question recursion in terms of center-embedding as a distinctive property of human language would be to grant that birds only use finite state grammars. However, it would be worthwhile discussing whether these grammars could approximate recursive patterns, such as center-embedding. While there still is need for research on this topic, part of the idea is also driven by the fact that humans’ cognitive capacities are finite. So human natural language might be less context sensible or even less context free in some respects and in that less recursive, as assumed.</p> <h2 id="bibliography">Bibliography</h2> <p>Ballentine, B. &amp; Hyman, J. (2021). Bird Talk: An Exploration of Avian Communication. Cornell University Press.</p> <p>Chomsky, N. (1956). Three models for the description of language. The Institute of Radio Engineers, Inc I.R.E. transactions on information theory, 2 (3), pp. 113-124. https://doi.org/10.1109/TIT.1956.1056813</p> <p>Chomsky, N. &amp; Miller, G. (1958). Finite State Languages. Information and control, 1 (2), pp. 91-112. https://doi.org/10.1016/S0019-9958(58)90082-2</p> <p>Corballis, M. (2007). The Uniqueness of Human Recursive Thinking. American Scientist, 95 (3, May/Jun 2007), pp. 240, 242-248. https://doi.org/10.1511/2007.65.240</p> <p>Daciuk, J. &amp; van Noord, G. (2002). Finite Automata for Compact Representation of Language Models in NLP. In Watson, B.W., Wood, D. (Eds.) Implementation and Application of Automata. pp. 65-73. CIAA 2001. Lecture Notes in Computer Science, 2494. https://doi.org/10.1007/3-540-36390-4_6</p> <p>Gentner, T., Fenn, K., Margoliash, D. &amp; Nusbaum, H. (2006). Recursive syntactic pattern learning by songbirds. Nature, 440 (7088), pp. 1204–1207. https://doi.org/10.1038/nature04675</p> <p>Hauser, M., Chomsky, N. &amp; Fitch, T. (2002). The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?. Science, 298 (5598), pp. 1569-1579. https://doi.org/10.1126/science.298.5598.1569</p> <p>Jackendoff, R. &amp; Pinker, S. (2005). The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky). Cognition, 97 (2005), pp. 211-225. https://doi.org/10.1016/j.cognition.2005.04.006</p> <p>Kallmayer, L. (2010). Parsing Beyond Context Free Grammars. Springer.</p> <p>Karlsson, F. (2007). Constraints on Multiple Center-Embedding of Clauses. Journal of Linguistics, 43 (2), pp. 365-392. https://doi.org/10.1017/S0022226707004616</p> <p>Roche, E. &amp; Schabes, Y. (1997). Finite-State Language Processing. MIT Press.</p>]]></content><author><name></name></author><category term="recursion"/><category term="language"/><category term="animal"/><category term="linguistics"/><category term="finite"/><category term="state"/><category term="grammar"/><category term="context"/><category term="free"/><category term="grammar"/><summary type="html"><![CDATA[Putting human language into perspective with animal linguistics]]></summary></entry><entry><title type="html">Transformer-Modelle wie GPT und BERT erklärt (German version)</title><link href="https://omseeth.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="Transformer-Modelle wie GPT und BERT erklärt (German version)"/><published>2024-09-13T16:40:16+00:00</published><updated>2024-09-13T16:40:16+00:00</updated><id>https://omseeth.github.io/blog/2024/transformer</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/transformer/"><![CDATA[<p>Um die theoretische Grundlage zum Verständnis von Modellen wie GPT und BERT zu schaffen, umreiße ich in diesem Blogbeitrag einige Konzepte der Transformerarchitektur. Dazu diskutiere ich in <strong>1</strong> <em>feedforward</em> neuronale Netze. In Abschnitt <strong>2</strong> beschreibe ich rekurrente neuronale Netze mit einer Enkodierer-Dekodierer-Architektur und dem ersten Aufmerksamkeitsmechanismus. In <strong>3</strong> führe ich alle Elemente zusammen, um ein Transformer-Modell zu beschreiben. Abschließend gehe ich in <strong>4</strong> auf die Besonderheiten der GPT-Modelle und BERTs ein und führe das Konzept des Transferlernens ein.</p> <p>Dieser Beitrag verfolgt zwei Ziele. Zum einen sollen Transformer-Modelle über ihre historische Genese erklärt werden, deshalb empfehle ich die Abschnitte <strong>1</strong> und <strong>2</strong> nachzuvollziehen, wobei das Augenmerk hier auf der Verarbeitung von Sequenzen mit einer Enkodierer-Dekodierer-Struktur liegen sollte. Zum anderen liegt den Transformer-Modellen ein ‘neuer’ (d.h. für das Jahr 2017 neuer) Selbstaufmerksamkeitsmechanismus zugrunde, bei welchem sich auch ein mathematisches Verständnis lohnt. Einmal mehr wird es hoffentlich für die Leserschaft nicht schwierig sein, diesen zu verstehen, sobald ein Gespür für vorangegangene Aufmerksamkeitsmechanismen da ist. Dieser Blogeintrag soll eine Hilfe sein, um Transformer-Modell zu verstehen. Nichtsdestotrotz empfehle ich die zitierten Veröffentlichungen ebenfalls zu lesen. Eine chronologische Lektüre ist dazu sinnvoll.</p> <h2 id="1-feedforward-neuronale-netze">1 <em>Feedforward</em> neuronale Netze</h2> <h4 id="11-definition-eines-neuronalen-netzes">1.1 Definition eines neuronalen Netzes</h4> <p>Abstrakt betrachtet ist ein neuronales Netz zunächst ein Ansatz, um eine Funktion \(f(X; \theta)\) zu approximieren, durch die mit den Parametern \(\theta\) die Eingabewerte \(X\) abgebildet werden können (Goodfellow et al. 2016). Als Klassifikator würde ein Netz zum Beispiel mit den richtigen Parametern \(f(x)=y\) vorhersagen, wobei \(y\) einem Klassenlabel entspricht (Goodfellow et al. 2016).</p> <p>Neuronale Netze bestehen aus einer Mehrzahl von verknüpften Funktionen, die mit Hilfe eines gerichteten kreisfreien Graphen eine Eingabe bis zur Ausgabe verarbeiten. Die jeweiligen Funktionen können auch als Schichten (<em>layers</em>) \(h_{i}\) mit \(i \in N\) und \(N\) als entsprechende Tiefe des Netzes bezeichnet werden. Die letzte Schicht wird als Ausgabeschicht \(a\) bezeichnet. Anstatt dass man jede Funktion einer Schicht nur als eine Abbildung eines Vektors auf einen Vektor betrachtet, sollte man die Funktionen eher als Kompositionen von Einheiten verstehen, die zusammen Vektoren auf ein Skalar abbilden (Goodfellow et al. 2016). Das Konvolut an verknüpften Funktionen eines neuronalen Netztes umfasst auch nicht-lineare Funktionen (Aktivierungsfunktionen). Wir können die Schichten wie folgt definieren: \begin{equation} h_{linear} := W \cdot x + b \end{equation} \begin{equation} h_{non-linear} := \sigma(W \cdot x + b) \end{equation} wobei \(W\) eine Matrix mit Gewichten, \(x\) die Eingabe, \(b\) zusätzliche Biases, und \(\sigma\) eine Aktivierungsfunktion (z.B: <em>Sigmoid</em>, <em>Tanh</em>, <em>Softmax</em>) sind. Ein neuronales Netz wird als <em>feedforward</em> bezeichnet, wenn von der Eingabe bis zur Ausgabe des Informationsflusses keine Form von Feedback berücksichtigt wird (Goodfellow et al. 2016).</p> <h4 id="12-training-eines-neuronalen-netzes">1.2 Training eines neuronalen Netzes</h4> <p>Die Approximation der Parameter \(\theta\) (damit sind die Gewichtungen und Biases des Netzes gemeint) folgt dem typischen Schema des maschinellen Lernens aus drei Schritten für jede Trainingsinstanz.</p> <ol> <li>Für \(x \in X\) sagt das Netz einen Wert \(y\) vorher.</li> <li>Dieser Wert wird mit einer weiteren Funktion, einer Kostenfunktion, ‘bewertet’. Die Kosten geben eine Information darüber ab, inwieweit sich die Vorhersage des Netzes von einem Soll-Wert unterscheidet (typische Kostenfunktionen sind z.B. <em>Mean Squared Error</em> oder <em>Binary Cross Entropy</em>). Durch den Einbezug eines Zielwertes (manchmal auch als <em>Ground Truth</em> bezeichnet) kann hier auch vom überwachten Lernen gesprochen werden.</li> <li>Um zukünftige Kosten zu senken, werden mit einem Optimierungsalgorithmus alle Parameter des Netzes mit Hinblick auf die Kostenfunktion angepasst. Dieser Algorithmus versucht die Kostenfunktion zu minimieren, indem das globale Minimum der Funktion durch Anpassung der Modellparameter in Bezug auf die Kosten für eine Eingabe angenähert wird. Der letzte Schritt involviert das Berechnen der Gradienten der Parameter, mit denen das Modell für die nächste Lernrunde entsprechend der Steigung verändert werden kann (z.B. mit dem <em>Gradient-Descent</em>-Algorithmus). In einem mehrschichtigen neuronalen Netz müssen dafür partielle Ableitungen für alle Parameter der verketteten Funktionen gefunden werden. Die Berechnung kann mit dem <em>Backpropagation</em>-Verfahren durchgeführt werden, das rekursiv die Kettenregel innerhalb eines Berechnungsgraphen ausführt und so die Ableitungen aller Gradienten findet.</li> </ol> <h2 id="2-rekurrente-netze-mit-einer-enkodierer-dekodierer-architektur">2 Rekurrente Netze mit einer Enkodierer-Dekodierer-Architektur</h2> <h4 id="21-definition-eines-rekurrenten-neuronalen-netzes">2.1 Definition eines rekurrenten neuronalen Netzes</h4> <p>Im Unterschied zu den <em>feedforward</em> Netzen werden bei rekurrenten neuronalen Netzen Informationen innerhalb einer Schicht mit den Zuständen \(h_{i}^{t}\) (auch <em>hidden states</em> bzw. verborgene Zustände genannt) ausgetauscht. Jeder Zustand zum Zeitpunkt \(t\) erhält nicht nur Informationen aus der Eingabe, sondern auch aus den Zuständen \(t-1\), also aus \(h_{i}^{t-1}\) (vgl. <strong>Fig. 1</strong>). Die Rekurrenz kann dabei prinzipiell ebenfalls von der Ausgabe zu jeder Zwischenschicht oder nur bei jeder Ausgabe vorgenommen werden.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rnn-480.webp 480w,/assets/img/rnn-800.webp 800w,/assets/img/rnn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rnn.png" class="img-fluid mx-auto d-block" width="40%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> Status eines neuronalen Netzes ohne (<em>feedforward</em>) und mit Feed-back (rekurrent) sowie jeweils einer Eingabeschicht \(x\), einer verborgenen Schicht \(h1\) und einer Ausgabe \(a\)</p> <p>Der Vorteil rekurrenter neuronaler Netze wie dem <em>Long Short-Term Memory</em>-Modell (kurz LSTM in Hochreiter und Schmidhuber, 1997) liegt darin, insbesondere sequenzielle Daten wie zum Beispiel Sprache sehr gut modellieren zu können. Wie man mit Ferdinand de Saussure inspiriert pointieren kann: Die Bedeutung eines Wortes leitet sich aus dem Zusammenspiel der Differenzen der umliegenden Wörter ab (de Saussure, 1931). So kann auch ein neuronales Netz wenig Sinn aus einer isolierten Betrachtung eines jeden Wortes ableiten. Werden hingegen die Bedeutungen der umliegenden Worteingaben mit in einer Schicht eines rekurrenten Netzes einbezogen, das heißt insgesamt eine Sequenz, können dadurch komplexere Zusammenhänge abgebildet werden.</p> <h4 id="22-ein-auto-regressives-sprachmodell">2.2 Ein auto-regressives Sprachmodell</h4> <p>Mit diesen Werkzeugen können wir bereits ein einfaches Sprachmodell zur Vorhersage von Sequenzen entwickeln. Angenommen wir wollen ein Modell, das eine Sequenz \(w\) mit \(w=(w_{1}, w_{2}, w_{3}, ..., w_{n})\) generiert, wobei \(n\) der Länge der Wörter entspricht, die zu der Sequenz, zum Beispiel einem Satz, gehören. Ein viel genutzter Ansatz in der natürlichen Sprachverarbeitung ist, die Vorhersage jedes Wortes durch alle vorangegangenen Wörter der Sequenz abzuleiten. Diese Idee können wir mit der Kettenregel für Wahrscheinlichkeiten wie folgt darstellen: \begin{equation} p(w) = p(w_{1}) \cdot p(w_{2}|w_{1}) \cdot p(w_{3}|w_{1}, w_{2}) \cdot … \cdot p(w_{n}|w_{1}, …, w_{n-1}) \end{equation} An dieser Stelle ist es bereits sinnvoll, nachzuvollziehen, dass wir hier eine auto-regressive Vorhersage der Sequenz verfolgen, bei der jedes Wort als abhängig von allen vorherigen Wörtern behandelt wird.</p> <p>Auf das maschinelle Lernen anhand der Daten \(w\) übertragen folgt aus der vorgeschlagenen Sprachmodellierung, dass wir folgende Funktion approximieren wollen: \begin{equation} p(w; \theta) \end{equation} d.h. wir suchen die besten Parameter \(\theta\) mit vielen Sprachdaten (auch Korpora genannt) für unser Modell, mit denen wir eine Vorhersage für eine Sequenz von Wörtern \(w\) erreichen können, die den genutzten Daten entspricht. Die Approximation können wir durch ein Training sowohl mit einem einfachen <em>feedforward</em> neuronalen Netz als auch einem rekurrenten realisieren. Das rekurrente hat den Vorteil, dass es mit der Weiterreichung von zusätzlichen Informationen durch die Status innerhalb jeder seiner Schichten besser die vorangegangenen Wörter mit einbezieht.</p> <h4 id="23-enkodierer-dekodierer-modelle-zur-maschinellen-übersetzung">2.3 Enkodierer-Dekodierer-Modelle zur maschinellen Übersetzung</h4> <p>Mit LSTM-Modellen entwickeln Sutskever et al. (2014) eine Sequenz-zu-Sequenz-Architektur zur maschinellen Übersetzung. Bei ihrem Ansatz kommen zwei wichtige Ideen zusammen. Zum einen (a) soll eine Übersetzung durch die Ursprungssprache bedingt werden, d.h. ein übersetzter Satz \(A\) (Ausgabe) hängt von seinem Ursprungssatz \(E\) (Eingabe) ab. Zum anderen (b) können Übersetzungen nicht immer wortwörtlich vollzogen werden. Aus diesem Grund ist es sinnvoll, dass ein Modell den ganzen Ursprungssatz berücksichtigt, bevor es eine potenzielle Übersetzung vorhersagt.</p> <p>Die erste Idee (a) führt zu den <em>bedingten</em> Sprachmodellen: \begin{equation} p(w | c; \theta) \end{equation} Bei diesen Modellen hängt die Vorhersage der Wortsequenz nicht nur von jedem vorangegangen Wort ab, sondern wird auch durch den für die Übersetzung wichtigen Ursprungssatz \(c\) bedingt. Prinzipiell kann es sich aber auch um andere Informationen handeln, die so in die Vorhersage mit einfließen würden.</p> <p>Die zweite Idee (b) setzen Sutskever et al. (2014) um, indem sie eine Architektur aus zwei Teilen, einem Enkodierer und einem Dekodierer (siehe <strong>Fig. 2</strong>), entwickeln (vgl. auch Cho et al. 2014). Wobei der Enkodierer den Ursprungssatz in eine feste Repräsentation \(c\) zusammenfasst und dann diese dem Dekodierer zum Vorhersagen der Übersetzung in der Zielsprache übergibt.</p> <p>Für den Enkodierer nutzen Sutskever et al. (2014) ein LSTM-Modell, dem Vektorrepräsentationen (auch <em>Embeddings</em> genannt) für die Wörter einer Eingabesequenz aus der Ursprungssprache zugeführt werden. Es werden <em>Embeddings</em> aus dem einfachen Grund verwendet, da neuronale Netze nur mit Zahlen und nicht mit Buchstaben operieren können. Die verborgenen Status dieser Eingaben werden daraufhin durch das Modell zu einem finalen Zustand \(c\) zusammengeführt: \begin{equation} c = q({h^{1},…,h^{T}}) \end{equation} wobei \(q\) dem LSTM-Modell entspricht und \(T\) der Länge der Eingabesequenz. Der Zustand \(c\) wird dem Dekodierer übergeben.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/seq2seq_ger-480.webp 480w,/assets/img/seq2seq_ger-800.webp 800w,/assets/img/seq2seq_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/seq2seq_ger.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2:</strong> Sequenz-zu-Sequenz-Architektur mit Enkodierer und Dekodierer</p> <p>Der Dekodierer besteht auch aus einem LSTM-Modell, welches auf der Grundlage des übergebenen Zustandes Wort für Wort eine Übersetzung in der Zielsprache vorhersagt. Dabei werden jedes übersetzte Wort und der Enkodierer-Endzustand der ursprünglichen Eingabe \(c\) regressiv dem Dekodierer so lange zugeführt, bis das Modell die Übersetzung abschließt: \begin{equation} p(w)= \prod_{t=1}^{T}p(w_{t}|{w_{1},…,w_{t-1}},c) \end{equation} Es schließt die Übersetzung ab, sobald es den Token <strong>&lt;eos&gt;</strong> vorhersagt. Mit diesem besonderen Token zeigen wir dem Modell bereits während des Trainings, an welcher Stelle Sequenzen beginnen und enden. In seinen Prädiktionen wird das Modell deshalb auch im besten Fall am Ende einer Sequenz noch diesen Token vorhersagen und den Inferenzprozess dadurch selbst terminieren.</p> <p>Abschließend noch ein weiteres Wort zum Training eines Sequenz-zu-Sequenz-Modells: während des Trainings werden dem Enkodierer des Modells Sätze aus der Ursprungssprache und dessen Dekodierer deren Übersetzungen entsprechend eines Hyperparameters (z.B. mit <em>Professor Forcing</em> (Goyal et al., 2016)) gezeigt, wodurch die Gewichte \(\theta\) beider Kodierer stets zusammen gelernt und aufeinander abgestimmt werden können.</p> <h4 id="24-der-erste-aufmerksamkeitsmechanismus">2.4 Der erste Aufmerksamkeitsmechanismus</h4> <p>Um die Qualität der Übersetzungen, insbesondere für lange Sequenzen, zu verbessern, führen Bahdanau et al. (2014) einen Aufmerksamkeitsmechanismus ein. Die Schwäche der Architektur nach Sutskever et al. (2014) liegt darin, dass die zu übersetzende Eingabe in eine einzige Repräsentation \(c\) gezwängt wird, mit welcher der Dekodierer eine Übersetzung finden muss. Allerdings spielen für eine Übersetzung nicht alle Wörter eines Satzes eine gleich große Rolle und auch kann die Beziehung unter den Wörtern variieren. Ob der Artikel in ‘the annoying man’ für ‘l’homme ennuyeux’ mit ‘le’ oder ‘l´’ übersetzt wird, hängt im Französischen beispielsweise davon ab, ob auf den Artikel ein Vokal folgt, gegebenenfalls mit einem stummen ‘h’ davor (Bahdanau et al. 2014). Bahdanau et al. (2014) entwickeln deshalb einen Mechanismus, der diesen Nuancen besser gerecht wird (vgl. <strong>Fig. 3</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_seq_ger-480.webp 480w,/assets/img/attention_seq_ger-800.webp 800w,/assets/img/attention_seq_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention_seq_ger.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3:</strong> Aufmerksamkeitsgewichte für eine Eingabe mit Bezug auf die Ausgabe an der Position <em>i=2</em></p> <p>Die um Aufmerksamkeit erweiterte Architektur übermittelt dem Dekodierer statt \(c\) für jede Eingabe kontextabhängige Zustände \(c_{i}\): \begin{equation} c_{i} = \sum_{t=1}^{T}a_{it}h^{t} \end{equation} Das Gewicht \(a_{it}\) für jeden Zustand \(h^{t}\) (in (Bahdanau et al. 2014) auch ‘Annotation’ genannt), wird wie folgt ermittelt: \begin{equation} a_{it} = \frac{\exp(e_{it})}{\sum_{k=1}^{T}\exp(e_{ik})} \end{equation} wobei \(a_{it}\) eine Normalisierung (<em>Softmax</em>-Funktion) für das Modell \(e_{it}\) ist. Dieses Modell ist wiederum ein <em>Feedforward</em>-Netz mit einer einzelnen Schicht, das bewertet, wie gut die Eingabe zum Zeitpunkt \(t\) mit der Ausgabe an Position \(i\) übereinstimmt. Damit erhält insgesamt jede Eingabe \(x^{1}...x^{T}\) eine eigene Menge an Aufmerksamkeitsgewichten, die in \(c_{i}\) resultieren, einem Kontextvektor, der dem Dekodierer hilft für jede Eingabe die passende Ausgabe (z.B. ‘l’homme’) zu bestimmen.</p> <h2 id="3-transformer-modelle-mit-selbstaufmerksamkeit">3 Transformer-Modelle mit Selbstaufmerksamkeit</h2> <h4 id="31-die-struktur-eines-transformer-modells">3.1 Die Struktur eines Transformer-Modells</h4> <p>Die Transformerarchitektur (Vaswani et al., 2017) führt einige der zuvor genannten Elemente zusammen. Die Architektur verleiht dabei dem Aufmerksamkeitsmechanismus eine wesentlich größere Rolle und verzichtet auf rekurrente Strukturen.</p> <p>Der Enkodierer der Transformerarchitektur besteht aus Ebenen mit jeweils zwei Komponenten, durch die die eingehenden Informationen verarbeitet werden (vgl. <strong>Fig. 4</strong>). Eingaben werden als erstes einer Schicht mit einem Selbstaufmerksamkeitsmechanismus <strong>parallel</strong> zugeführt, der in Vaswani et al. (2017) vorgestellt wird. Nachdem dieser Mechanismus angewendet wurde, werden die Informationen normalisiert und daraufhin einer weiteren Schicht mit einem <em>feedforward</em> neuronalen Netz übergeben. Die Verarbeitung der Eingaben findet auf dieser Schicht wiederum <strong>einzeln</strong> statt. Für den Zwischenschritt der Normalisierung werden Mittelwerte und Standardabweichungen nach dem Prinzip der <em>Layer Normalization</em> berechnet (Ba et al. 2016; es gibt auch <em>Root Square Layer Normalization</em> von Zhang &amp; Sennrich (2019), die z.B. in Llama 2 angewendet wurde). Zusätzlich wird die normalisierte Ausgabe mit der Ausgabe der vorangegangenen Schicht addiert. Dies wird auch als ‘<em>Residual Connection</em>’ bezeichnet und ist eine Methode, um dem Problem verschwindender Gradienten während der <em>Backpropagation</em> etwas entgegenzuwirken.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_encoder_ger-480.webp 480w,/assets/img/transformer_encoder_ger-800.webp 800w,/assets/img/transformer_encoder_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_encoder_ger.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4:</strong> Enkodierer eines Transformer-Modells</p> <h4 id="32-embeddings-mit-positioneller-enkodierung">3.2 <em>Embeddings</em> mit positioneller Enkodierung</h4> <p>Wie bei den vorigen Sequenz-zu-Sequenz-Architekturen wird die Eingabe zuerst in <em>Embeddings</em> überführt. Die <em>Embeddings</em> werden aber zusätzlich mit einer Positionsenkodierung versehen, die über eine Frequenzdarstellung (Sinus- und Kosinusfunktionen) realisiert wird. Dies begründet sich wie folgt. Im Gegensatz zu den rekurrenten Ansätzen verarbeitet die Aufmerksamkeitsschicht eines Transformerkodierers eine Eingabesequenz auf einmal und zum Beispiel im Falle einer Übersetzung nicht Wort für Wort. Ohne eine zusätzliche Information zur Position einer jeden Eingabe innerhalb einer Sequenz würden den Kodierern die wichtige Information fehlen, wie die einzelnen Wörter aufeinander folgen.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embeddings_ger-480.webp 480w,/assets/img/embeddings_ger-800.webp 800w,/assets/img/embeddings_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embeddings_ger.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 5:</strong> Beispielsequenz, deren Tokens in <em>Embeddings</em> mit \(d=4\) überführt werden</p> <p>Für die Verarbeitung der Eingabe hält die Transformerarchitektur eine <em>Embedding</em>-Matrix für alle Vokabeln der Daten bereit, die in das Training eines Tranformer-Modells einfließen. Die Größe der Matrix entspricht der Anzahl der Vokabeln (sonst als Tokens bezeichnet, zu denen auch bspw. Satzzeichen zählen) Kreuz einer gewählten Dimension (also n x d), mit der jedem Eingabetoken genau eine Zeile innerhalb der Matrix zugeordnet werden kann (vgl. <strong>Fig. 5</strong>). Die Anzahl der Spalten entspricht der gewählten Dimension. Die Matrixwerte für die Tokentypen werden während der Initialisierung eines Transformer-Modells zufällig ausgewählt. Es sind die Zeilen dieser Matrix, die auch als <em>Embeddings</em> bezeichnet werden. Man kann auch sagen, dass jeder Tokentyp eine Vektorrepräsentation besitzt. Wobei diese Vektoren in einem weiteren Schritt mit der positionalen Enkodierung addiert so der Eingabe eine einzigartige Darstellung verleihen. Entscheidend ist, dass die <em>Embeddings</em> der Transformerarchitektur sich im Laufe eines Trainings auch verändern, d.h. entsprechend der Daten angepasst, also ‘gelernt’ werden können. (Anmerkung: der erste Schritt ist, die Eingabetokens in eine <em>One-hot</em> Enkodierung zu überführen, mit der jedem Token eine Zeile in der <em>Embedding</em>-Matrix zugeordnet werden kann.)</p> <p>Zur Veranschaulichung der Positionsenkodierung ist es hilfreich, zunächst ein stark vereinfachtes Beispiel zu betrachten. Angenommen man teilt jedem <em>Embedding</em> einfach die Positionen (<em>pos</em>) des jeweiligen Tokens in Form von ganzen Zahlen mit \(n \in N\), wobei \(N\) der Länge der Eingabetokens inklusive <strong>&lt;eos&gt;</strong>-Token entspricht, zu. Wenn wir den Token “heiße” und dessen <em>Embedding</em> [0.11, 0.45, 0.23, 0.77] (ausgedachte Werte) auswählen, dann ließe sich für den Token innerhalb der Sequenz “&lt;eos&gt; ich heiße max &lt;eos&gt;” auf diese Weise eine Positionsenkodierung von [2, 2, 2, 2] ermitteln. Der Vektor hätte diese Werte, weil wir die zweite Position der Sequenz (Sequenz beginnt bei 0) und eine <em>Embedding</em>-Dimension von \(d=4\) ausgewählt haben. Der Transformer-Architektur entsprechend könnten wir diesen Vektor anschließend auf das <em>Embedding</em> des Tokens addieren [2.11, 2.45, 2.23, 2.77] und hätten diesem auf diese Weise zusätzliche Informationen hinzugefügt. Dieser Ansatz würde jedoch zu mehreren Problemen führen. Zu nennen sind hier beispielsweise große Werte für lange Sequenzen, deren Positionsenkodierung die Werte der <em>Embeddings</em> stark beeinflussen würden, und es würde ein relativer Bezug wiederkehrender Positionsmuster fehlen.</p> <p>Vaswani et al. (2017, S. 6) stellen deshalb eine Positionsenkodierung vor, durch die jedem Token-<em>Embedding</em> über das Bogenmaß der trigonometrischen Funktionen zusätzliche Informationen zu der Position des Tokens innerhalb einer Sequenz zugeführt werden. Die Vorteile dieses Ansatzes sind unter anderem, dass die Positionsenkodierungswerte auf ein Intervall von \([-1,1]\) beschränkt werden können, und die Periodizität der trigonometrischen Funktionen auch erlaubt, wiederkehrende Muster abzubilden. Denn bestimmte Abstände zwischen Positionen werden ähnliche Werte erzeugen. Dadurch kann das Modell leichter lernen, dass sich bestimmte Muster oder Abstände zwischen Tokens in unterschiedlichen Bereichen einer Sequenz wiederholen, unabhängig von der genauen Position in der Sequenz.</p> <p>Die Positionen der Tokens werden nach Vaswani et al. (2017) wie folgt berechnet: \begin{equation} PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} \begin{equation} PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} <em>pos</em> entspricht der absoluten Position innerhalb einer Eingabesequenz der Länge <strong>N</strong>, der Wert \(10000\) ist eine gewählte Konstante, und <strong>i</strong> verweist auf die Indezes der <em>Embeddings</em>. Zum Beispiel bei einer gewählten Dimension der <em>Embedding</em>-Vektoren mit \(d=4\) gilt \(i \in I= \{0, 1, 2, 3\}\). Schließlich erlauben die beiden Sinus- und Kosinusfunktionen für gerade und ungerade Indizes unterschiedliche Werte zu ermitteln. Für alle gerade Indizes eines <em>Embeddings</em> können wir (10) und für alle ungeraden (11) verwenden. Erwähnenswert ist hier, dass die Frequenzen der Sinus- und Kosinusfunktionen der <em>PE</em> von der gewählten Dimension abhängig sind. Kleine <em>Embedding</em>-Dimensionen führen zu höheren Frequenzen (feinere Positionsauflösungen) und hohe Dimensionen zu niedrigeren Frequenzen (gröbere Positionsauflösungen). Anhand dieser Vorgaben wird schließlich für jede Eingabe eine Positionsenkodierungs-Matrix berechnet – also für jeden Token ein Positionsvektor (vgl. <strong>Fig. 6</strong>) – welche wir daraufhin auf die Token-<em>Embeddings</em> addieren können. Im Zusammenspiel mit den <em>Embeddings</em> wird dem Transformer-Modell dadurch eine kontextsensitive Repräsentation der Tokens zur weiteren Verarbeitung überreicht.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_encoding_ger-480.webp 480w,/assets/img/positional_encoding_ger-800.webp 800w,/assets/img/positional_encoding_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/positional_encoding_ger.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 6:</strong> Beispielsequenz, deren Tokens in Positionsenkodierungen mit \(d=4\) überführt werden</p> <h4 id="33-selbstaufmerksamkeitsmechanismus">3.3 Selbstaufmerksamkeitsmechanismus</h4> <p>Im Gegensatz zu dem Aufmerksamkeitsmechanismus nach Bahdanau et al. (2014) entwickeln Vaswani et al. (2017) einen Selbstaufmerksamkeitsmechanismus, den sie auch als ‘skalierte Skalarprodukt-Aufmerksamkeit’ beschreiben (Vaswani et al., 2017, S.3). Die für Transformer genutzte Selbstaufmerksamkeit kann vereinfacht zunächst mit der Operation aus (8) verglichen werden (Raschka et al., 2022). Wir können für einen kontextsensitiven Vektor \(z_{i}\) einer Eingabe an der Stelle \(i\) die Aufmerksamkeit wie folgt berechnen (Raschka et al., 2022): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}x^{j} \end{equation} wobei \(a_{ij}\) nicht mit einem Status \(h^{t}\), sondern mit den Eingaben \(x^{j}\) multipliziert wird, mit \(j\in{\{1, ..., T\}}\) einer Eingabesequenz der Länge \(T\) (vgl. die Summe über alle \(x^{j}\) in (12)). Im Unterschied zu Bahdanau et al. (2014) ist \(a\) dabei keine Normalisierung von einfachen <em>feedforward</em> Netzen \(e_{ij}\), sondern eine <em>Softmax</em>-Normalisierung über die Skalarprodukte \(\Omega\) der Eingabe \(x^{i}\) bezogen auf alle anderen Eingaben \(X=x^{1}, ..., x^{T}\) (Raschka et al., 2022): \begin{equation} a_{ij} = \frac{\exp(\omega_{ij})}{\sum_{j=1}^{T}\exp(\omega_{ij})} \end{equation} mit (Raschka et al., 2022): \begin{equation} \omega_{ij} = x^{(i)T}x^{j} \end{equation} Was wir hier gleichzeitig im Unterschied zum Aufmerksamkeitsmechanismus von Bahdanau et al. (2014) sehen, bei welchem die Aufmerksamkeit insbesondere die Ausgabe des Dekodierers (dort an Ausgabeposition <em>i</em>) mit einbezieht, ist, dass das Aufmerksamkeitsgewicht in (13) mit (14) sich auf die anderen Eingaben einer Sequenz bezieht. Eben aus diesem Grund ist es sinnvoll, von einer <em>Selbstaufmerksamkeit</em> zu sprechen.</p> <p>Zu dieser Darstellung der Aufmerksamkeit fügen Vaswani et al. (2017) eine weitere Veränderung für jede Eingabe \(x^{i}\) hinzu, und zwar wird das Gewicht \(a\) nicht mit \(x^{j}\) multipliziert, sondern mit einem Wert \(v^{j}\): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}v^{j} \end{equation} Denn Vaswani et al. (2017) überführen jedes \(x^{i}\) in ein Tripel aus (\(v^{i}\), \(k^{i}\), \(q^{i}\)) mittels den Projektionsmatrizen (\(W_{v}\), \(W_{k}\), \(W_{q}\) – die hier auch als zusätzliche lineare Schichten aufgefasst werden können). Die Idee dahinter entstammt dem <em>Information Retrieval</em>, das mit Wert-, Schlüssel-, Abfragetripeln arbeitet (wegen Values, Keys, Queries die Abkürzungen v, k, q). Die <strong>V</strong>, <strong>K</strong>, <strong>Q</strong> in <strong>Fig. 4</strong> und <strong>Fig. 8</strong> entsprechen: \(V=XW_{v}\), \(K=XW_{k}\) sowie \(Q=XW_{q}\) (vgl. <strong>Fig. 8</strong>). Die Skalarprodukte des Selbstaufmerksamkeitsmechanismus für jede Eingabe werden in Vaswani et al. (2017) auch nicht mit (14) berechnet, sondern mit den Abfrage- und Schlüsselwerten (Raschka et al., 2022): \begin{equation} \omega_{ij} = q^{(i)T}k^{j} \end{equation} Kurz: Neben der Aufmerksamkeit einer Eingabe \(x^i\) gegenüber allen anderen Eingaben innerhalb einer Sequenz \(X\) wird die Selbstaufmerksamkeit noch durch verschiedene Darstellungen aller umliegenden \(x^j \in X\) in Form der Abfrage-, Schlüssel-, Wertrepräsentationen berechnet.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projection_matrices_ger-480.webp 480w,/assets/img/projection_matrices_ger-800.webp 800w,/assets/img/projection_matrices_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/projection_matrices_ger.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Aufmerksamkeit für eine Eingabe X mit mehreren Köpfen (<em>heads</em>)</p> <p>Die Aufmerksamkeitsgewichte werden abschließend mit der Dimension der <em>Embeddings</em> noch skaliert (\(\frac{\omega_{ij}}{\sqrt{d}}\)) und können \(h\)-Mal parallel berechnet, wobei \(h\) einer gewählten Anzahl an Köpfen (auch <em>Attention Heads</em> gennant) entspricht. Vaswani et al. (2017) wählen \(h=8\) Köpfe, deren Werte konkateniert abschließend der <em>Layer</em>-Normalisierung in den Kodierern weitergereicht werden (siehe <strong>Fig. 8</strong> und <strong>Fig. 4</strong>). Die Verwendung mehrerer Köpfe wird als ‘<em>Multi-head Attention</em>’ bezeichnet. Die zusätzliche Skalierung begründen Vaswani et al. (2017, S. 4) mit der Beobachtung, dass zu große Werte der Skalarprodukte (vgl. (14)) die für die zusätzliche Normalisierung genutzte <em>Softmax</em>-Funktion in einen Bereich führen, der beim Lernen in sehr kleine Gradienten resultiert.</p> <h4 id="34-der-transformer-dekodierer">3.4 Der Transformer-Dekodierer</h4> <p>Der Dekodierer der Transformerarchitektur folgt strukturell dem Enkodierer. Er enthält jedoch noch eine zusätzliche Schicht (vgl. <strong>Fig. 7</strong>). In dieser Schicht werden die ausgegebenen Informationen des Enkodierers (z.B. der enkodierte Ursprungssatz einer Übersetzung) über die Wert- und Schlüsselwerte \(V\), \(K\) mitberücksichtigt. Die Abfragewerte \(Q\) kommen hingegen von der vorangegangenen Aufmerksamkeitsschicht des Dekodierers. Durch die Kombination der Informationen sowohl des Enkodierers als auch des Dekodierers wird diese weitere Schicht auch als ‘Cross-Aufmerksamkeitsschicht’ bezeichnet. Da dabei Enkodierer-Informationen mit einbezogen werden, lässt sich bei dem ganzen Modell (Enkodierer + Dekodierer) zudem von einem <em>bedingten</em> Sprachmodell sprechen, wie dieses zuvor in (5) vorgestellt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_decoder_ger-480.webp 480w,/assets/img/transformer_decoder_ger-800.webp 800w,/assets/img/transformer_decoder_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_decoder_ger.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 7:</strong> Dekodierer eines Transformer-Modells</p> <p>Die Selbstaufmerksamkeitsschicht des Dekodierers erlaubt es, Teile der Dekodierereingabe zu verdecken (dies wird auch als <em>Masking</em> beschrieben, geanuer gesagt <em>Causal Masking</em>). Ein Transformer-Enkodierer lässt hingegen grundsätzlich eine Betrachtung aller Eingaben gleichzeitig zu. Das Masking spielt eine wichtige Rolle bei dem Ziel des Dekodierers: z.B. die Vorhersage einer Übersetzung. Zur Prädiktion einer Übersetzungssequenz arbeitet der Dekodierer sich jeweils autoregressiv von Token zu Token vor (z.B. von links nach rechts, vgl. hierzu auch (7)). Das bedeutet, dass beim Inferenzieren jede Prädiktion nur mit Hilfe vorangegangener Tokens arbeitet, die anderen bleiben im übertragenen Sinne maskiert. Für das Training können dem Modell noch durch <em>Teacher-Forcing</em> korrekte Übersetzungen als Eingabe mit zugeführt werden, um eine Fehlerfortpflanzung zu minimieren – wie bei den zuvor beschriebenen Sequenz-zu-Sequenz-Modellen. Grundsätzlich ist das Trainingsziel die Vorhersagen des Modells anhand der Übersetzungslösungen zu optimieren. Ein Übersetzungsprozess terminiert in jedem Fall, wenn der Token <strong>&lt;eos&gt;</strong> oder die zuvor definierte maximale Sequenzlänge erreicht sind.</p> <p>Abschließend noch ein Wort zu der linearen Schicht am Endes des Dekodierers (vgl. <strong>Fig. 7</strong>). Die unmittelbare Ausgabe aus den Aufmerksamkeitsblöcken umfasst eine durch das Modell angereicherte Repräsentation der Eingabe-<em>Embeddings</em> \(h_{i}\). In diese Repräsentation sind durch die Aufmerksamkeitsmechanismen und den <em>feedforward</em>-neuronalen Netzen zusätzliche Informationen umliegender Tokens sowie des Enkodierers mit eingeflossen. Es gilt jedes \(h_{i}\) wieder in eine Darstellung des Vokabulars zu überführen. Dafür stellt die lineare Schicht eine Projektionsmatrix \(W\) bereit. Diese ähnelt einer Schicht eines neuronalen Netztes mit dem Unterschied, dass hier keine nicht-lineare Aktivierungsfunktion den Informationsfluss noch weiter verändert.</p> <p>Betrachten wir dazu ein Beispiel. Angenommen dem Modell liegt eine Vokabelgröße von \(10 000\) zugrunde und für die <em>Embeddings</em> bzw. die Status des Modells wählen wir exemplarisch eine Dimension von \(d=512\). Dann können wir mit \(W\) (10000 x 512) alle \(h_{i}\) in einen <em>Logits</em>-Vektor überführen, der der Dimension des Vokabulars entspricht, und dessen Wert gleichzeitig die Approximation des Modells dafür ist, wie wahrscheinlich welcher der Token des Vokabulars ist: \begin{equation} logits = W \cdot h_{i} + b \end{equation} wobei \(b\) als zusätzlicher Bias auf noch einen Einfluss auf die Abbildung nimmt. Auf der Grundlage dieses <em>Logits</em>-Vektor (z.B. \(logits = [3.4, -1.2, 0.5, ..., 2.7]\)) kann schlussendlich die <em>Softmax</em>-Aktivierung, mit der die Werte des Vektors in Wahrscheinlichkeiten überführt werden, den wahrscheinlichsten Token für den ausgegebenen Status \(h_{i}\) des Dekodierers vorhersagen. An dieser Stelle ließen sich aber auch andere Dekodierungsstrategien (z.B. <em>Beam-Search</em> oder <em>Greedy Decoding</em>) einsetzen.</p> <p>Insgesamt unterscheiden sich Inferenz und Training bei einem Transformer-Modell nicht von anderen neuronalen Netzen (siehe Kapitel <strong>1.2</strong>).</p> <h2 id="4-gpt-bert-und-co">4 GPT, BERT und co</h2> <p>Derweil die ursprüngliche Transformerarchitektur zur maschinellen Übersetzung entwickelt wurde, haben sich Transformer-Modelle bei anderen Aufgaben ebenfalls bewährt. Am bekanntesten sind große Sprachmodelle wie <em>Generative Pre-trained Transformer</em>-Modelle (Radford et al., 2018) von OpenAI, die eine Eingabe (Prompt) mit einem Transformer-Dekodierer sozusagen ‘weiterschreiben’, d.h. die nächsten <em>Tokens</em> der Sequenz vorhersagen. Das <em>Bidirectional Encoder Representations from Transformers</em>-Modell (kurz BERT, Devlin et al., 2019) ist wiederum ein Transformer-Enkodierer. Das heißt mit BERT können keine neuen Wörter oder Sätze in einer Zielsprache <em>autoregressiv</em> generiert werden. BERT stellt dafür Enkodierungen bereit, mit deren Hilfe sich zum Beispiel Klassifikationsaufgaben lösen lassen.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/language_model_types_ger-480.webp 480w,/assets/img/language_model_types_ger-800.webp 800w,/assets/img/language_model_types_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/language_model_types_ger.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Verschiedene Transformerarchitekturen: Enkodierer-Dekodierer zur maschinellen Übersetzung (auch <em>MTM</em> genannt), einzelne Dekodierer zur Generierung von Sequenzen (auch als <em>LM</em> bezeichnet), und einzelne Enkodierer für <em>Downstream</em>-Aufgaben (auch häufig als <em>MLM</em> bezeichnet)</p> <h4 id="41-das-bidirectional-encoder-representations-from-transformers-modell">4.1 Das <em>Bidirectional Encoder Representations from Transformers</em>-Modell</h4> <p>Für BERT trainieren Devlin et al. (2019) zunächst den Enkodierer eines Transformer-Modells vor dem Hintergrund zweier Aufgaben. Die erste Aufgabe des BERT-Trainings besteht aus einer maskierten Sprachmodellierung (<em>Masked Language Modelling</em>). Das Modell bekommt dabei Sätze gezeigt, bei denen es 15% zufällig ausgewählte Wörter, die verdeckt werden, vorhersagen muss. Die zweite Aufgabe besteht aus einer binären Klassifikation zweier Sätze, und zwar mit dem Ziel, vorherzusagen, ob diese aufeinander folgen oder nicht. Wobei das Modell 50% korrekte Satzfolgen und 50% inkorrekte Folgen gezeigt bekommt. Da das Modell ausschließlich einen Enkodierer verwendet und keine ‘rechtsbündige’ Maskierung der jeweils nächsten Tokens innerhalb einer Sequenz wie bei einem Transformer-Dekodierer vorgenommen wird, kann das Training auch als bi-direktional bezeichnet werden. Devlin et al. (2019) nennen das Training ihres Enkodierers auf den beiden Aufgaben außerdem als ‘Vortraining’.</p> <p>In einem weiteren Schritt nutzen Devlin et al. (2019) ihr vortrainiertes BERT-Modell für weitere Experimente aus der natürlichen Sprachverarbeitung. Dafür feintunen sie BERT beispielsweise, um die plausibelste Wortfolge für Sätze aus dem Datensatz <em>Situations With Adversarial Generations</em> (Zellers et al., 2018) vorherzusagen. Da BERT ursprünglich vor dem Hintergrund bestimmter Aufgaben trainiert wurde, die Gewichte des Modells jedoch auch für neue Aufgaben verwendet und angepasst werden können, kann eine solche Verwendung BERTs auch als eine Form des Transferlernens bezeichnet werden.</p> <h2 id="zusätzliche-ressourcen">Zusätzliche Ressourcen</h2> <ul> <li>Ich kann Brendan Bycrofts Visualisierung von Transformer-Modellen empfehlen: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></li> <li>Mit die besten Erklärungen und Visualisierungen zu verschiedenen Modellierungsansätzen in NLP, einschließlich der Transformer-Architektur, stellt Lena Voita bereit: <a href="https://lena-voita.github.io/nlp_course.html">https://lena-voita.github.io/nlp_course.html</a></li> <li>Weitere hilfreiche Visualisierungen zu BERT gibt Jay Alammar: <a href="https://jalammar.github.io/illustrated-bert/">https://jalammar.github.io/illustrated-bert/</a></li> </ul> <h2 id="bibliographie">Bibliographie</h2> <p>Ba, J., Kiros, J.R., &amp; Hinton, G.E. (2016). Layer Normalization. ArXiv, abs/1607.06450.</p> <p>Bahdanau, D., Cho, K., und Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. <em>CoRR</em>, abs/1409.0473.</p> <p>Cho, K., van Merri ̈enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., und Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Moschitti, A., Pang, B., und Daelemans, W., Herausgeber, <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, Seiten 1724–1734, Doha, Qatar. Association for Computational Linguistics.</p> <p>de Saussure, F. (1931). <em>Cours de Linguistique Generale</em>. Payot, Paris.</p> <p>Devlin, J., Chang, M.-W., Lee, K., und Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J., Doran, C., und Solorio, T., Herausgeber, <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Volume 1 (Long and Short Papers), Seiten 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Goodfellow, I., Bengio, Y., und Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p> <p>Goyal, A., Lamb, A. M., Zhang, Y., Zhang, S., Courville, A. C., und Bengio, Y. (2016). Professor Forcing: A New Algorithm for Training Recurrent Networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., und Garnett, R., Herausgeber, Advances in <em>Neural Information Processing Systems</em>, Band 29. Curran Associates, Inc.</p> <p>Hochreiter, S. und Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Comput.</em>, 9(8):1735–1780.</p> <p>Radford, A., Narasimhan, K., Salimans, T., und Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. Technical report, OpenAI.</p> <p>Raschka, S., Liu, Y., und Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sutskever, I., Vinyals, O., und Le, Q. V. (2014). Sequence to sequence learning with neural networks. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS’14, Seite 3104–3112, Cambridge, MA, USA. MIT Press.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., und Polosukhin, I. (2017). Attention is All you Need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., und Garnett, R., Herausgeber, <em>Advances in Neural Information Processing Systems</em>, Band 30. Curran Associates, Inc.</p> <p>Zellers, R., Bisk, Y., Schwartz, R., und Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Riloff, E., Chiang, D., Hockenmaier, J., und Tsujii, J., Herausgeber, <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, Seiten 93–104, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Zhang, B., and Sennrich, R. (2019). Root mean square layer normalization. <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc., Red Hook, NY, USA.</p> <p><strong>Version 1.0</strong></p>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><category term="neural"/><category term="nets"/><category term="feed-forward"/><category term="attention-mechanism"/><category term="transformer"/><category term="GPT"/><category term="BERT"/><category term="transfer-learning"/><category term="German"/><summary type="html"><![CDATA[Vom neuronalen Netz bis zu GPT und BERT, eine kontextualisierte Erklärung der Transformer-Architektur]]></summary></entry><entry><title type="html">Transformer models such as GPT and BERT explained (English version)</title><link href="https://omseeth.github.io/blog/2024/transformer_en/" rel="alternate" type="text/html" title="Transformer models such as GPT and BERT explained (English version)"/><published>2024-09-13T16:40:16+00:00</published><updated>2024-09-13T16:40:16+00:00</updated><id>https://omseeth.github.io/blog/2024/transformer_en</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/transformer_en/"><![CDATA[<p>To provide the theoretical basis for understanding models such as GPT and BERT, I outline some concepts of transformer architecture in this blog post. To do this, I discuss <em>feedforward</em> neural networks in <strong>1</strong>. In section <strong>2</strong>, I describe recurrent neural networks with an encoder-decoder architecture and the first attention mechanism. In <strong>3</strong>, I bring all the elements together to describe a transformer model. Finally, in <strong>4</strong>, I discuss the specifics of GPT models and BERT and introduce the concept of transfer learning.</p> <p>This article has two aims. On the one hand, transformer models are to be explained via their historical genesis, which is why I recommend reading sections <strong>1</strong> and <strong>2</strong>, whereby the focus here should be on the processing of sequences with an encoder-decoder structure. Secondly, the transformer models are based on a ‘new’ (i.e. new for 2017) self-attention mechanism, which is also worth understanding mathematically. Once again, it will hopefully not be difficult for readers to understand this once they have an intuition for previous attention mechanisms. This blog entry is intended as an aid to understanding the Transformer model. Nevertheless, I recommend that you also read the publications cited. It makes sense to read them in chronological order.</p> <h2 id="1-feedforward-neural-networks">1 <em>Feedforward</em> neural networks</h2> <h4 id="11-definition-of-a-neural-network">1.1 Definition of a neural network</h4> <p>In abstract terms, a neural network is initially an approach for approximating a function \(f(X; \theta)\) that can be used to map the input values \(X\) with the parameters \(\theta\) (Goodfellow et al. 2016). For example, as a classifier, a network with the correct parameters would predict \(f(x)=y\), where \(y\) corresponds to a class label (Goodfellow et al. 2016).</p> <p>Neural networks consist of a number of linked functions that process an input to an output using a directed circle-free graph. The respective functions can also be referred to as layers \(h_{i}\) with \(i \in N\) and \(N\) as the corresponding depth of the network. The last layer is called the output layer \(a\). Instead of considering each function of a layer as a mapping of a vector to a vector, the functions should rather be understood as compositions of units that together map vectors to a scalar (Goodfellow et al. 2016). The set of linked functions of a neural network also includes non-linear functions (activation functions). We can define the layers as follows: \begin{equation} h_{linear} := W \cdot x + b \end{equation} \begin{equation} h_{non-linear} := \sigma(W \cdot x + b) \end{equation} where \(W\) is a matrix with weights, \(x\) the input, \(b\) additional biases, and \(\sigma\) an activation function (e.g.: <em>Sigmoid</em>, <em>Tanh</em>, <em>Softmax</em>). A neural network is described as <em>feedforward</em> if no form of feedback is taken into account from the input to the output of the information flow (Goodfellow et al. 2016).</p> <h4 id="12-training-of-a-neural-network">1.2 Training of a neural network</h4> <p>The approximation of the parameters \(\theta\) (i.e. the weights and biases of the network) follows the typical machine learning scheme of three steps for each training instance.</p> <ol> <li>For \(x \in X\) the network predicts a value \(y\).</li> <li>This value is ‘evaluated’ with another function, a cost function. The costs provide information on the extent to which the prediction of the network differs from a target value (typical cost functions are e.g. <em>Mean Squared Error</em> or <em>Binary Cross Entropy</em>). By including a target value (sometimes also referred to as <em>ground truth</em>), this can also be referred to as supervised learning.</li> <li>In order to reduce future costs, an optimisation algorithm is used to adjust all parameters of the network with regard to the cost function. This algorithm attempts to minimise the cost function by approximating the global minimum of the function by adjusting the model parameters with respect to the cost for an input. The last step involves calculating the gradients of the parameters with which the model can be changed for the next learning round according to the gradient (e.g. with the <em>gradient descent</em> algorithm). In a multi-layer neural network, partial derivatives must be found for all parameters of the concatenated functions. The calculation can be carried out using the <em>Backpropagation</em> method, which recursively executes the chain rule within a calculation graph and thus finds the derivatives of all gradients.</li> </ol> <h2 id="2-recurrent-neural-networks-with-an-encoder-decoder-architecture">2 Recurrent neural networks with an encoder-decoder architecture</h2> <h4 id="21-definition-of-a-recurrent-neural-network">2.1 Definition of a recurrent neural network</h4> <p>In contrast to <em>feedforward</em> networks, recurrent neural networks exchange information within a layer with the states \(h_{i}^{t}\) (also called <em>hidden states</em> or hidden states). Each state at time \(t\) not only receives information from the input, but also from the states \(t-1\), i.e. from \(h_{i}^{t-1}\) (cf. <strong>Fig. 1</strong>). In principle, the recurrence can also be performed from the output to each intermediate layer or only for each output.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rnn-480.webp 480w,/assets/img/rnn-800.webp 800w,/assets/img/rnn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rnn.png" class="img-fluid mx-auto d-block" width="40%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> Status of a neural network without (<em>feedforward</em>) and with feedback (recurrent) and in each case an input layer \(x\), a hidden layer \(h1\) and an output \(a\)</p> <p>The advantage of recurrent neural networks such as the <em>Long Short-Term Memory</em> model (LSTM for short in Hochreiter and Schmidhuber, 1997) is that they are particularly good at modelling sequential data such as speech. As inspired by Ferdinand de Saussure: The meaning of a word is derived from the interplay of the differences of the surrounding words (de Saussure, 1931). Thus, even a neural network can derive little meaning from an isolated consideration of each word. However, if the meanings of the surrounding word inputs are included in a layer of a recurrent network, i.e. a sequence as a whole, more information is taken into account.</p> <h4 id="22-auto-regressive-language-models-lms">2.2 Auto-regressive language models (LMs)</h4> <p>With these tools, we can already develop a simple language model (LM) for predicting sequences. Suppose we want a model that generates a sequence \(w\) with \(w=(w_{1}, w_{2}, w_{3}, ..., w_{n})\), where \(n\) corresponds to the length of the words belonging to the sequence, for example a sentence. A much-used approach in natural language processing is to derive the prediction of each word from all previous words in the sequence. We can illustrate this idea with the chain rule for probabilities as follows: \begin{equation} p(w) = p(w_{1}) \cdot p(w_{2}|w_{1}) \cdot p(w_{3}|w_{1}, w_{2}) \cdot … \cdot p(w_{n}|w_{1}, …, w_{n-1}) \end{equation} At this point, it is already useful to understand that we are following an auto-regressive prediction of the sequence, where each word is treated as dependent on all previous words.</p> <p>Applied to machine learning using the data \(w\), it follows from the proposed language modelling that we want to approximate the following function: \begin{equation} p(w; \theta) \end{equation} i.e. we are looking for the best parameters \(\theta\) with language data (also called corpora) for our model, with which we can achieve a prediction for a sequence of words \(w\) that corresponds to the data used. We can realise the approximation by training both a simple <em>feedforward</em> neural network and a recurrent one. The recurrent neural network has the advantage that it better incorporates the preceding words by passing additional information through the states within each of its layers.</p> <h4 id="23-encoder-decoder-models-for-machine-translation-mt">2.3 Encoder-decoder models for machine translation (MT)</h4> <p>Using LSTM models, Sutskever et al. (2014) develop a sequence-to-sequence architecture for machine translation (MT). Their approach combines two important ideas. Firstly, a translation should be conditioned by the original language, i.e. a translated sentence \(O\) (output) depends on its original sentence \(I\) (input). Secondly, translations cannot always be carried out literally. For this reason, it makes sense for a model to consider the whole original sentence before predicting a potential translation.</p> <p>The first idea (a) leads to the <em>conditional</em> language models: \begin{equation} p(w | c; \theta) \end{equation} In these models, the prediction of the word sequence not only depends on each preceding word, but is also conditioned by the source sentence \(c\), which is so important for the translation. In principle, however, it can also be other information that would be included in the prediction.</p> <p>The second idea is implemented by Sutskever et al. (2014) by developing an architecture consisting of two parts, an encoder and a decoder (see <strong>Fig. 2</strong>) (see also Cho et al. 2014). Whereby the encoder summarizes the source sentence into a fixed representation \(c\) and then passes it to the decoder to predict the translation in the target language.</p> <p>For the encoder, Sutskever et al. (2014) use an LSTM model that is fed vector representations (also called <em>embeddings</em>) for the words of an input sequence from the original language. Embeddings are used for the simple reason that neural networks can only operate with numbers and not letters. The hidden states of these inputs are then merged by the model into a final state \(c\): \begin{equation} c = q({h^{1},…,h^{T}}) \end{equation} where \(q\) corresponds to the LSTM model and \(T\) to the length of the input sequence. The state \(c\) is passed to the decoder.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/seq2seq_en-480.webp 480w,/assets/img/seq2seq_en-800.webp 800w,/assets/img/seq2seq_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/seq2seq_en.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2:</strong> Sequence-to-sequence architecture with encoder and decoder</p> <p>The decoder also consists of an LSTM model, which predicts a translation in the target language word by word based on the input state. Each translated word and the final encoder state of the original input \(c\) are regressively fed to the decoder until the model completes the translation: \begin{equation} p(w)= \prod_{t=1}^{T}p(w_{t}|{w_{1},…,w_{t-1}},c) \end{equation} It completes the translation as soon as it predicts the token <strong>&lt;eos&gt;</strong>. We use this special token to show the model where sequences begin and end during training. In its predictions, the model will therefore also predict this token at the end of a sequence in the best case and thus terminate the inference process itself.</p> <p>Finally, one more word about the training of a sequence-to-sequence model: during training, the encoder of the model is shown sentences from the original language and its decoder is shown their translations according to a hyperparameter (e.g. with <em>Professor Forcing</em> (Goyal et al., 2016)), whereby the weights \(\theta\) of both encoders can always be learned together and matched to each other.</p> <h4 id="24-the-first-attention-mechanism">2.4 The first attention mechanism</h4> <p>To improve the quality of translations, especially for long sequences, Bahdanau et al. (2014) introduce an attention mechanism. The weakness of Sutskever et al.’s (2014) architecture is that the input to be translated is forced into a single representation \(c\), with which the decoder must find a translation. However, not all words in a sentence play an equally important role in a translation and the relationship between the words can also vary. Whether the article in ‘the annoying man’ for ‘l’homme ennuyeux’ is translated as ‘le’ or ‘l’’, for example, depends in French on whether the article is followed by a vowel, possibly with a silent ‘h’ in front of it (Bahdanau et al. 2014). Bahdanau et al. (2014) therefore develop a mechanism that does better justice to these nuances (cf. <strong>Fig. 3</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_seq_en-480.webp 480w,/assets/img/attention_seq_en-800.webp 800w,/assets/img/attention_seq_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention_seq_en.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3:</strong> Attention weights for an input with reference to the output at position <em>i=2</em></p> <p>The architecture extended by attention transmits context-dependent states \(c_{i}\) to the decoder instead of \(c\) for each input: \begin{equation} c_{i} = \sum_{t=1}^{T}a_{it}h^{t} \end{equation} The weight \(a_{it}\) for each state \(h^{t}\) (also called ‘annotation’ in (Bahdanau et al. 2014)) is determined as follows: \begin{equation} a_{it} = \frac{\exp(e_{it})}{\sum_{k=1}^{T}\exp(e_{ik})} \end{equation} where \(a_{it}\) is a normalization (<em>softmax</em> function) for the model \(e_{it}\). This model is again a <em>feedforward</em> network with a single layer that evaluates how well the input at time \(t\) matches the output at position \(i\). Thus, overall, each input \(x^{1}...x^{T}\) receives its own set of attention weights, resulting in \(c_{i}\), a context vector that helps the decoder determine the appropriate output (e.g. ‘l’homme’) for each input.</p> <h2 id="3-transformer-models-with-self-attention">3 Transformer models with self attention</h2> <h4 id="31-the-structure-of-a-transformer-model">3.1 The structure of a transformer model</h4> <p>The transformer architecture (Vaswani et al., 2017) combines some of the previously mentioned elements. The architecture gives the attention mechanism a much greater role and dispenses with recurrent structures.</p> <p>The encoder of the transformer architecture consists of stacks, each with two components, through which the incoming information is processed (see <strong>Fig. 4</strong>). Inputs are first fed in <strong>parallel</strong> to a layer with a self-attention mechanism, which is presented in Vaswani et al. (2017). After this mechanism has been applied, the information is normalized and then passed to another layer with a <em>feedforward</em> neural network. With this network, the processing of the input takes place <strong>individually</strong>. With an intermediate normalization step, mean values and standard deviations are calculated according to the principle of <em>layer normalization</em> (Ba et al. 2016; there is also <em>Root Square Layer Normalization</em> by Zhang &amp; Sennrich (2019), which was used in Llama 2, for example). In addition, the normalized output is added to the output of the previous layer. This is also referred to as ‘<em>Residual Connection</em>’ and is a method to counteract the problem of disappearing gradients during <em>Backpropagation</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_encoder_en-480.webp 480w,/assets/img/transformer_encoder_en-800.webp 800w,/assets/img/transformer_encoder_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_encoder_en.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4:</strong> Encoder of a transformer model</p> <h4 id="32-embeddings-with-positional-encoding">3.2 Embeddings with positional encoding</h4> <p>As with the previous sequence-to-sequence architectures, the input is first converted into embeddings. However, the embeddings are additionally provided with positional encoding, which is realized via a frequency representation (sine and cosine functions). The reason for this is as follows. In contrast to the recurrent approaches, the attention layer of a transformer encoder processes an input sequence all at once and not word by word in the case of a translation, for example. Without additional information on the position of each input within a sequence, the coders would lack the important information on how the individual words follow each other.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embeddings_en-480.webp 480w,/assets/img/embeddings_en-800.webp 800w,/assets/img/embeddings_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embeddings_en.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 5:</strong> Example sequence whose tokens are converted into embeddings with \(d=4\)</p> <p>For processing the input, the transformer architecture provides an embedding matrix for all vocabulary of the data that is used in the training of a transformer model. The size of the matrix corresponds to the number of vocabulary words (otherwise referred to as tokens, which also include punctuation marks, for example) with a selected dimension (i.e. n x d), with which each input token can be assigned exactly one row within the matrix (cf. <strong>Fig. 5</strong>). The number of columns corresponds to the selected dimension. The matrix values for the token types are randomly selected during the initialization of a transformer model. It is the rows of this matrix that are also referred to as embeddings. It can also be said that each token type has a vector representation. In a further step, these vectors are added to the positional encoding to give the input a unique representation. It is crucial that the embeddings of the transformer architecture also change in the course of training, i.e. they can be adapted according to the data, i.e. ‘learned’. (Note: the first step is to convert the input tokens into a <em>one-hot</em> encoding, with which each token can be assigned a row in the embedding matrix).</p> <p>To illustrate positional encoding, it is helpful to first consider a very simplified example. Assume that each embedding is simply assigned the positions (<em>pos</em>) of the respective token in the form of integers with \(n \in N\), where \(N\) corresponds to the length of the input tokens including <strong>&lt;eos&gt;</strong> tokens. If we select the token “heiße” and its embedding [0.11, 0.45, 0.23, 0.77] (imaginary values), then a positional encoding of [2, 2, 2, 2] could be determined for the token within the sequence “&lt;eos&gt; ich heiße max &lt;eos&gt;” as follows. The vector of the token would have these values because we chose the second position of the sequence (sequence starts at 0) and an embedding dimension of \(d=4\). According to the transformer architecture, we could then add this vector to the embedding of the token [2.11, 2.45, 2.23, 2.77] and thus add additional information to it. However, this approach would lead to several problems. For example, large values for long sequences whose positional encoding would strongly influence the values of the embeddings and a relative reference of recurring positional patterns would be missing.</p> <p>Vaswani et al. (2017, p. 6) therefore present positional encoding that provides each token embedding with additional information about the position of the token within a sequence via the radians of the trigonometric functions. The advantages of this approach include the fact that the positional encoding values can be limited to an interval of \([-1,1]\), and the periodicity of the trigonometric functions also allows recurring patterns to be mapped. This is because certain distances between positions will produce similar values. This makes it easier for the model to learn that certain patterns or distances between tokens are repeated in different areas of a sequence, regardless of the exact position in the sequence.</p> <p>According to Vaswani et al. (2017), the positions of the tokens are calculated as follows: \begin{equation} PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} \begin{equation} PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} <em>pos</em> corresponds to the absolute position within an input sequence of length <strong>N</strong>, the value \(10000\) is a selected constant, and <strong>i</strong> refers to the indices of the embeddings. For example, for a selected dimension of the embedding vectors with \(d=4\), \(i \in I= \{0, 1, 2, 3\}\) applies. Finally, the two sine and cosine functions allow different values to be determined for even and odd indices. We can use (10) for all even indices of an embedding and (11) for all odd indices. It is worth mentioning here that the frequencies of the sine and cosine functions of the <em>PE</em> depend on the selected dimension. Small ebedding dimensions lead to higher frequencies (finer position resolutions) and high dimensions to lower frequencies (coarser position resolutions). Based on these specifications, a positional encoding matrix is finally calculated for each input – i.e. a position vector for each token (see <strong>Fig. 6</strong>). In combination with the embeddings, the transformer model is given a context-sensitive representation of the tokens for further processing in this fashion.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_encoding_en-480.webp 480w,/assets/img/positional_encoding_en-800.webp 800w,/assets/img/positional_encoding_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/positional_encoding_en.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 6:</strong> Beispielsequenz, deren Tokens in Positionsenkodierungen mit \(d=4\) überführt werden</p> <h4 id="33-self-attention">3.3 Self-attention</h4> <p>In contrast to the attention mechanism according to Bahdanau et al. (2014), Vaswani et al. (2017) develop a self-attention mechanism, which they also describe as ‘scaled scalar product attention’ (Vaswani et al., 2017, p.3). In simplified terms, the self-attention used for transformers can first be compared with the operation from (8) (Raschka et al., 2022). We can calculate the attention for a context-sensitive vector \(z_{i}\) of an input at position \(i\) as follows (Raschka et al., 2022): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}x^{j} \end{equation} where \(a_{ij}\) is not multiplied by a state \(h^{t}\), but by the inputs \(x^{j}\), with \(j\in{\{1, ..., T\}}\) an input sequence of length \(T\) (cf. the sum over all \(x^{j}\) in (12)). In contrast to Bahdanau et al. (2014), \(a\) is not a normalization of simple feedforward networks \(e_{ij}\), but a softmax normalization over the scalar products \(\Omega\) of the input \(x^{i}\) related to all other inputs \(X=x^{1}, ..., x^{T}\) (Raschka et al., 2022): \begin{equation} a_{ij} = \frac{\exp(\omega_{ij})}{\sum_{j=1}^{T}\exp(\omega_{ij})} \end{equation} with (Raschka et al., 2022): \begin{equation} \omega_{ij} = x^{(i)T}x^{j} \end{equation} What we also see here, in contrast to the attention mechanism of Bahdanau et al. (2014), in which attention includes in particular the output of the decoder (there at output position <em>i</em>), is that the attention weight in (13) with (14) refers to the other inputs of a sequence. For this very reason, it makes sense to speak of self-attention.</p> <p>To this representation of attention, Vaswani et al. (2017) add a further change for each input \(x^{i}\), namely the weight \(a\) is not multiplied by \(x^{j}\), but by a value \(v^{j}\): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}v^{j} \end{equation} Vaswani et al. (2017) transform each \(x^{i}\) into a triple of (\(v^{i}\), \(k^{i}\), \(q^{i}\)) using the projection matrices (\(W_{v}\), \(W_{k}\), \(W_{q}\) – which can also be understood here as additional linear layers). The idea behind this comes from <em>information retrieval</em>, which works with value, key and query triples (hence the abbreviations v, k, q). The <strong>V</strong>, <strong>K</strong>, <strong>Q</strong> in <strong>Fig. 4</strong> and <strong>Fig. 8</strong> correspond to: \(V=XW_{v}\), \(K=XW_{k}\) and \(Q=XW_{q}\) (cf. <strong>Fig. 8</strong>). The scalar products of the self-attention mechanism for each input are also not calculated with (14) in Vaswani et al. (2017), but with the query and key values (Raschka et al., 2022): \begin{equation} \omega_{ij} = q^{(i)T}k^{j} \end{equation} In short: In addition to the attention of an input \(x^i\) to all other inputs within a sequence \(X\), the self-attention is calculated by different representations of all surrounding \(x^j \in X\) in the form of query, key and value representations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projection_matrices_en-480.webp 480w,/assets/img/projection_matrices_en-800.webp 800w,/assets/img/projection_matrices_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/projection_matrices_en.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Attention for input \(X\) with multiple heads</p> <p>Finally, the attention weights are scaled with the dimension of the embeddings (\(\frac{\omega_{ij}}{\sqrt{d}}\)) and can be calculated \(h\) times in parallel, where \(h\) corresponds to a selected number of heads (also called <em>attention heads</em>). Vaswani et al. (2017) choose \(h=8\) heads, whose values are concatenated and finally passed on to the layer normalization in the encoders (see <strong>Fig. 8</strong> and <strong>Fig. 4</strong>). The use of multiple heads is referred to as ‘multi-head attention’. Vaswani et al. (2017, p. 4) justify the additional scaling with the observation that too large values of the scalar products (see (14)) lead the softmax function used for additional normalization into a range that results in very small gradients during learning.</p> <h4 id="34-the-transformer-decoder">3.4 The Transformer decoder</h4> <p>The decoder of the transformer architecture follows the structure of the encoder. However, it contains an additional layer (see <strong>Fig. 7</strong>). In this layer, the information output by the encoder (e.g. the encoded original sentence of a translation) is also taken into account via the value and key values \(V\), \(K\). The query values \(Q\), on the other hand, come from the previous attention layer of the decoder. Due to the combination of information from both the encoder and the decoder, this additional layer is also referred to as the ‘cross-attention layer’. Since encoder information is included, the whole model (encoder + decoder) can also be referred to as a <em>conditional</em> language model, as previously presented in (5).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_decoder_en-480.webp 480w,/assets/img/transformer_decoder_en-800.webp 800w,/assets/img/transformer_decoder_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_decoder_en.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 7:</strong> Decoder of a Transformer model</p> <p>The self-attention layer of the decoder allows parts of the decoder input to be masked (this is also described as <em>masking</em>, or more precisely <em>causal masking</em>). A transformer encoder, on the other hand, allows all inputs to be viewed simultaneously. Masking plays an important role in the goal of the decoder: e.g. the prediction of a translation. To predict a translation sequence, the decoder works autoregressively from token to token (e.g. from left to right, see also (7)). This means that during inferencing, each prediction only works with the help of previous tokens, the others remain masked in a figurative sense. For training, correct translations can be added to the model as input by <em>teacher forcing</em> in order to minimize error propagation – as with the sequence-to-sequence models described above. Basically, the training goal is to optimize the predictions of the model based on the translation solutions. A translation process terminates in any case when the token <strong>&lt;eos&gt;</strong> or the previously defined maximum sequence length is reached.</p> <p>Finally, a word about the linear layer at the end of the decoder (see <strong>Fig. 7</strong>). The immediate output from the attention blocks comprises a representation of the input embeddings \(h_{i}\) enriched by the model. Additional information from surrounding tokens and the encoder has been incorporated into this representation through the attention mechanisms and the feedforward neural networks. Now each \(h_{i}\) must be converted back into a representation of the vocabulary. For this purpose, the linear layer provides a projection matrix \(W\). This is similar to a layer of a neural network with the difference that no non-linear activation function further alters the information flow.</p> <p>Let’s look at an example. Suppose the model is based on a vocabulary size of \(10 000\) and we choose a dimension of \(d=512\) as an example for the embeddings or the status of the model. We can then use \(W\) (10000 x 512) to convert all \(h_{i}\) into a logits vector that corresponds to the dimension of the vocabulary and whose value is also the model’s approximation of how likely each of the vocabulary’s tokens is: \begin{equation} logits = W \cdot h_{i} + b \end{equation} where \(b\) as an additional bias has an influence on the mapping. Based on this logits vector (e.g. \(logits = [3.4, -1.2, 0.5, ..., 2.7]\)), the softmax activation, with which the values of the vector are converted into probabilities, can finally predict the most probable token for the output status \(h_{i}\) of the decoder. However, other decoding strategies (e.g. <em>beam search</em> or <em>greedy decoding</em>) could also be used at this point.</p> <p>Overall, inference and training in a transformer model do not differ from other neural networks (see chapter <strong>1.2</strong>).</p> <h2 id="4-gpt-bert-and-co">4 GPT, BERT and co</h2> <p>While the original Transformer architecture was developed for machine translation, Transformer models have also proven themselves in other tasks. The best known are large language models such as <em>Generative Pre-trained Transformer</em> models (Radford et al., 2018) from OpenAI, which continue an input (prompt) with a transformer decoder, i.e. predict the next tokens of the sequence. The <em>Bidirectional Encoder Representations from Transformers</em> model (abbreviated as BERT, Devlin et al., 2019) is in turn a transformer encoder. This means that BERT cannot be used to generate new words or sentences in a target language autoregressively. Instead, BERT provides encodings that can be used to solve classification tasks, for example.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/language_model_types_en-480.webp 480w,/assets/img/language_model_types_en-800.webp 800w,/assets/img/language_model_types_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/language_model_types_en.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Different transformer architectures: encoder-decoders for machine translation (also called MTM), single decoders for sequence generation (also called LM), and single encoders for downstream tasks (also often called MLM)</p> <h4 id="41-the-bidirectional-encoder-representations-from-transformers-model">4.1 The <em>Bidirectional Encoder Representations from Transformers</em> model</h4> <p>For BERT, Devlin et al. (2019) first train the encoder of a transformer model against the background of two tasks. The first task of training consists of masked language modeling (MLM). The model is shown sentences in which it has to predict 15% randomly selected words that are masked. The second task consists of a binary classification of two sentences with the aim of predicting whether or not they follow each other. The model is shown 50% correct sentence sequences and 50% incorrect sequences. Since the model only uses an encoder, and no ‘right-aligned’ masking of the next tokens within a sequence is performed as with a transformer decoder, the training can also be described as bi-directional. Devlin et al. (2019) also refer to the training of their encoder on the two tasks as ‘pre-training’.</p> <p>In a further step, Devlin et al. (2019) use their pre-trained BERT model for further experiments in natural language processing. For example, they fine-tune BERT to predict the most plausible word order for sentences from the data set Situations With Adversarial Generations (Zellers et al., 2018). Since BERT was originally trained against the background of specific tasks, but the weights of the model can also be used and adapted for new tasks, such use of BERT can also be described as a form of transfer learning.</p> <h2 id="additional-resources">Additional resources</h2> <ul> <li>I can recommend Brendan Bycroft’s visualizations of Transformer models: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></li> <li>Lena Voita provides excellent explanations and visualizations of different modeling approaches in NLP, including the Transformer architecture: <a href="https://lena-voita.github.io/nlp_course.html">https://lena-voita.github.io/nlp_course.html</a></li> <li>Jay Alammar provides further helpful visualizations of BERT: <a href="https://jalammar.github.io/illustrated-bert/">https://jalammar.github.io/illustrated-bert/</a></li> </ul> <h2 id="bibliography">Bibliography</h2> <p>Ba, J., Kiros, J.R., &amp; Hinton, G.E. (2016). Layer Normalization. ArXiv, abs/1607.06450.</p> <p>Bahdanau, D., Cho, K., und Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. <em>CoRR</em>, abs/1409.0473.</p> <p>Cho, K., van Merri ̈enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Moschitti, A., Pang, B., and Daelemans, W., editors, <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, Seiten 1724–1734, Doha, Qatar. Association for Computational Linguistics.</p> <p>de Saussure, F. (1931). <em>Cours de Linguistique Generale</em>. Payot, Paris.</p> <p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J., Doran, C., und Solorio, T., editors, <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p> <p>Goyal, A., Lamb, A. M., Zhang, Y., Zhang, S., Courville, A. C., and Bengio, Y. (2016). Professor Forcing: A New Algorithm for Training Recurrent Networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., und Garnett, R., Herausgeber, Advances in <em>Neural Information Processing Systems</em>, Band 29. Curran Associates, Inc.</p> <p>Hochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Comput.</em>, 9(8):1735–1780.</p> <p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. Technical report, OpenAI.</p> <p>Raschka, S., Liu, Y., and Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS’14, pages 3104–3112, Cambridge, MA, USA. MIT Press.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is All you Need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., und Garnett, R., editors, <em>Advances in Neural Information Processing Systems</em>, Band 30. Curran Associates, Inc.</p> <p>Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Riloff, E., Chiang, D., Hockenmaier, J., und Tsujii, J., editors, <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 93–104, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Zhang, B., and Sennrich, R. (2019). Root mean square layer normalization. <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc., Red Hook, NY, USA.</p> <p><strong>Version 1.0</strong></p>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><category term="neural"/><category term="nets"/><category term="feed-forward"/><category term="attention-mechanism"/><category term="transformer"/><category term="GPT"/><category term="BERT"/><category term="transfer-learning"/><category term="English"/><summary type="html"><![CDATA[From neural networks to GPT and BERT, a contextualized explanation of the Transformer architecture]]></summary></entry></feed>