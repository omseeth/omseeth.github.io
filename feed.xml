<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://omseeth.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://omseeth.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-22T10:43:36+00:00</updated><id>https://omseeth.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Explaining BERT-based hate speech detection with LIME and Saliency using Captum</title><link href="https://omseeth.github.io/blog/2025/Explaining_BERT/" rel="alternate" type="text/html" title="Explaining BERT-based hate speech detection with LIME and Saliency using Captum"/><published>2025-03-19T10:00:00+00:00</published><updated>2025-03-19T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2025/Explaining_BERT</id><content type="html" xml:base="https://omseeth.github.io/blog/2025/Explaining_BERT/"><![CDATA[<p>It has been shown that BERT’s domain-specific fine-tuning approaches for hate speech detection are still competitive for hate speech classification (Roy et al., 2023). Although LLMs can also be used for classification tasks, they are trained on a wider domain of tasks and more data. Their predictive power is also tied to the right model instructions and decoding strategies. Roy et al. (2023) confirm that LLMs are sensitive to input variations and struggle in particular with implicit cases of hate speech. This aspect makes it also more difficult to probe LLMs with explainability methods. In contrast, BERT models, as classifiers, are easier to explain.</p> <p>This notebook’s idea is to investigate how well BERT-based hate speech detection is aligned with human judgments of German hate speech. To arrive at plausible explanations, we also need to determine which method, that is, a model agnostic simplification such as LIME or a gradient-based method such as Saliency, works best for our purpose.</p> <p>As we can see in the following example, the explainability methods will tell us which tokens influenced BERT’s prediction.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/visualization_example_1-480.webp 480w,/assets/img/Explaining_BERT/visualization_example_1-800.webp 800w,/assets/img/Explaining_BERT/visualization_example_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/visualization_example_1.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="overview">Overview</h3> <ul> <li>Loading and preprocessing Gaze4Hate dataset</li> <li>Loading and initializing the BERT model for hate speech detection</li> <li>Deriving explanations through attributions from BERT with LIME</li> <li>Deriving explanations through attributions from BERT with Saliency</li> <li>Evaluating the results (human vs. BERT rationales)</li> <li>Visualizing the results (alignment)</li> </ul> <p>Before we start, let us begin with installing and loading libraries, which we will use later in the project.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">pandas</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">numpy</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">seaborn</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">scipy</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="n">torchvision</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">openpyxl</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">captum</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">sentencepiece</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">ipywidgets</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">show</span> <span class="n">transformers</span>
</code></pre></div></div> <p>We’ll need the following packages</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Basic libraries
</span><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># Colab
# from google.colab import drive
</span>
<span class="c1"># Data libraries
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sn</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Deeplearning libraries
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">captum.attr</span> <span class="kn">import</span> <span class="n">LimeBase</span><span class="p">,</span> <span class="n">Saliency</span>
<span class="kn">from</span> <span class="n">captum._utils.models.linear_model</span> <span class="kn">import</span> <span class="n">SkLearnLasso</span>

<span class="c1"># Evaluation libraries
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">pearsonr</span>
</code></pre></div></div> <h2 id="loading-and-preprocessing-gaze4hate-dataset">Loading and preprocessing Gaze4Hate dataset</h2> <p>We use the GAZE4HATE dataset from Alacam et al. (2024), which consists of “hatefulness ratings of text w.r.t. gender, eye movements during plain readings of the statements” (Alacam et al., 2024, p.189) from 43 participants (32 female, 10 male, 1 non-binary, Mean age = 23.5, SD = 5.3 as reported in (Alacam et al., 2024)). The participants’ ratings were gathered on a Likert scale from 1 to 7, where 1 corresponded to ‘very positive’ and 7 to ‘extremely hateful’. 4 and 5 were considered as ‘neutral’ and ‘mean’. The data comes with rationales for the participants’ judgments. These rationales were collected through clicks on words that the participants found relevant. Conversely, if a word is not clicked, it is not considered a determining factor. Finally, the participants’ response were assigned to two hate annotations: the first measured the answers along the binary levels of ‘hate’ and ‘no-hate’ and the second along three levels, namely ‘hate’, ‘neutral’, and ‘positive’. All participants saw all sentences in the data set.</p> <p>In this project, we are interested in the explicit rationales, which we will later compare with BERT’s attributions with respect to the words that influenced its predictions.</p> <p>We can find and download the dataset under a CC-BY-NC 4.0 license from: <a href="https://osf.io/fgdjw">https://osf.io/fgdjw</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># URL of the CSV file
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://osf.io/download/fgdjw/</span><span class="sh">"</span>

<span class="c1"># Downloading the file
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="nb">ConnectionError</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No internet connection</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Saving it as a local CSV file
</span><span class="n">filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Gaze4Hate.csv</span><span class="sh">"</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">File downloaded and saved as </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">File</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">saved</span> <span class="k">as</span> <span class="n">Gaze4Hate</span><span class="p">.</span><span class="n">csv</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">os</span><span class="p">.</span><span class="nf">chdir</span><span class="p">(</span><span class="sh">"</span><span class="s">/path</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">Gaze4Hate.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
# Mount Google Drive
drive.mount(</span><span class="sh">'</span><span class="s">/content/gdrive</span><span class="sh">'</span><span class="s">)

PATH = r</span><span class="sh">"</span><span class="s">/content/gdrive/My Drive/Colab Notebooks/EXPL_LIME</span><span class="sh">"</span><span class="s">
os.chdir(PATH)

directory = os.getcwd()
df = pd.read_csv(</span><span class="sh">"</span><span class="s">Gaze4Hate_data.csv</span><span class="sh">"</span><span class="s">, delimiter=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="s">)</span><span class="sh">"</span><span class="s">
</span><span class="sh">"""</span>
</code></pre></div></div> <p>Let’s inspect the raw dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">p1</span><span class="sh">"</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">19</span><span class="p">)]</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/Gaze4Hate_data-480.webp 480w,/assets/img/Explaining_BERT/Gaze4Hate_data-800.webp 800w,/assets/img/Explaining_BERT/Gaze4Hate_data-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/Gaze4Hate_data.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We have many columns with different data, including the sentences, the participants’ hate ratings in terms of gaze data, and click data. Let’s look an example:</p> <p><code class="language-plaintext highlighter-rouge">Frauen gehören in die Küche.</code></p> <p>Which translates to:</p> <p><code class="language-plaintext highlighter-rouge">Women belong in the kitchen.</code></p> <p>The experiment’s first subject clicked on the tokens: “Frauen” and “Küche” (see column <code class="language-plaintext highlighter-rouge">Clicked</code>) to justify her judgement of the sentence, which was ‘hate’. Since the dataset has 35 columns they might not all be displayed in your IDE and you won’t see all columns right away.</p> <p>Let’s select the columns and rows from the dataset that we need for our project. We also want to create a list of the experiment’s unique sentences that we can use to make predictions with our models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rationale_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">assertion</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">IA_LABEL</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Clicked</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]]</span>

<span class="n">experiment_sents</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">sno</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">rationale_df</span><span class="p">[</span><span class="sh">"</span><span class="s">assertion</span><span class="sh">"</span><span class="p">],</span><span class="n">rationale_df</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]):</span>
    <span class="n">experiment_sents</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="n">sno</span><span class="p">:</span> <span class="n">sent</span><span class="p">})</span>

<span class="n">experiment_sents</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">))</span>  <span class="c1"># The dataset contains 90 sentences
</span><span class="nf">print</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
<span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">Frauen haben einen signifikant geringeren Durchsetzungswillen.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">Frauen sind psychisch nicht für komplexe Arbeit geschaffen.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="p">...}</span>
</code></pre></div></div> <p>From the human annotation, we want to have the average rationales for the sentences that were judged as hateful or non-hateful.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">participants_n</span> <span class="o">=</span> <span class="mi">43</span>
<span class="n">human_hate_rationale</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="n">human_no_hate_rationale</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rationale_df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">sentence_num</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">IA_LABEL</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">clicked</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Clicked</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">category</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]</span>

    <span class="c1"># Selecting the rationales from those judgments which labeled the given sentence as hate
</span>    <span class="k">if</span> <span class="n">category</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">clicked</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">human_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">human_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">clicked</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">human_no_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">human_no_hate_rationale</span><span class="p">[</span><span class="n">sentence_num</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">0</span>

<span class="c1"># We average the responses by the number of participants
</span><span class="n">average_human_hate_rationales</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">sentence_num</span><span class="p">:</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">43</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">words</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">for</span> <span class="n">sentence_num</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">human_hate_rationale</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">average_human_hate_rationales</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>    
<span class="nf">print</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">)</span>

<span class="n">average_human_no_hate_rationales</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">sentence_num</span><span class="p">:</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">43</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">words</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">for</span> <span class="n">sentence_num</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">human_no_hate_rationale</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">average_human_no_hate_rationales</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>      
<span class="nf">print</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">Word 1</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.37209302325581395</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 2</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.11627906976744186</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 3</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.023255813953488372</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 4</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.5116279069767442</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 5</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.7674418604651163</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 6</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.6511627906976745</span><span class="p">},</span> <span class="p">...}</span>
<span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">Word 1</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.046511627906976744</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 2</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 3</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 4</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 5</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">,</span> <span class="sh">'</span><span class="s">Word 6</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.09302325581395349</span><span class="p">},</span> <span class="p">...}</span>
</code></pre></div></div> <p>We can see that not all sentences have received rationales, some were univocally annotated as hateful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">82</span>
</code></pre></div></div> <p>Futhermore, we want to derive labels for the sentences from the human judgments on average. We select a subframge with only the participants (see column <code class="language-plaintext highlighter-rouge">RECORDING_SESSION_LABEL</code>) and judgments (see column <code class="language-plaintext highlighter-rouge">Intensity_Category_Binary</code>) for each respective sentence (see column <code class="language-plaintext highlighter-rouge">sno</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">judgment_df</span> <span class="o">=</span> <span class="n">rationale_df</span><span class="p">[[</span><span class="sh">"</span><span class="s">pno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]]</span>

<span class="c1"># We can drop all duplicates
</span><span class="n">judgment_df</span> <span class="o">=</span> <span class="n">judgment_df</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>

<span class="c1"># 90 (sentences) * 43 (participants) = 3870 (judgments); as of 19.03.2025 we report 3616 judgments from the dataset which indicates that some are missing
</span><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">judgment_df</span><span class="p">))</span>

<span class="n">human_labels</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">avg_human_labels</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">judgment_df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">sno</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">sno</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">Intensity_Category_Binary</span><span class="sh">"</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">human_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">human_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">human_labels</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">human_labels</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span> 

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">judgments</span> <span class="ow">in</span> <span class="n">human_labels</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">judgments</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">count</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">avg_human_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">count</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">avg_human_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">3616</span>
<span class="mi">90</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="loading-and-initializing-the-bert-model">Loading and initializing the BERT model</h2> <p>Having prepared our data to make comparisons between the annotators’ rationales and the model’s attributions, we can now intialize our BERT model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_built</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">mps</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># For mac use
</span><span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BERT_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">deepset/bert-base-german-cased-hatespeech-GermEval18Coarse</span><span class="sh">"</span><span class="p">)</span>

<span class="n">BERT</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">deepset/bert-base-german-cased-hatespeech-GermEval18Coarse</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">BERT</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> 

<span class="c1"># We can derive the classes used in the model as follows
</span><span class="n">classes</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">BERT</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">id2label</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>  <span class="c1"># 1 for "hate" and 0 for "no-hate"  
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <p>For our model’s predictions, we need to define a few BERT specificities, such as the encodings that the model will use for its predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encodings</span> <span class="o">=</span> <span class="p">[</span><span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">experiment_sents</span><span class="p">.</span><span class="nf">values</span><span class="p">()]</span>
</code></pre></div></div> <p>We can define a function to make predictions from a given BERT model. We should assure that this function only takes a single argument for the encodings and handles the masking necessary for BERT internally because our Captum LIME class that we will use later passes a single input to its perturbation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">encoding</span><span class="p">):</span>
    <span class="c1"># Converting masks and encodings to integers is necessary when using LIME
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nc">BERT</span><span class="p">(</span><span class="n">encoding</span><span class="p">.</span><span class="nf">long</span><span class="p">(),</span> <span class="n">mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="n">logits</span>
</code></pre></div></div> <p>The predictions from the encodings can be realized through a loop. We’ll have to transform each encoding into a PyTorch tensor. We also need to add another dimension here (see <code class="language-plaintext highlighter-rouge">.unsqueeze(0)</code>).</p> <p>Usually, it is best practice to let the model make several rounds of predictions and average over the results. For simplicity, we will just do a single round.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="nf">predict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sent</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">)]</span>
    <span class="c1"># Selecting the label with highest probability
</span>
<span class="n">BERT_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">logit</span> <span class="ow">in</span> <span class="n">logits</span><span class="p">:</span>
    <span class="n">pred_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">BERT_predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_id</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">BERT_predictions</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">BERT_predictions</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>We can now make a comparison of the model’s predictions with the human judgments.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">precision</span>    <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>   <span class="n">support</span>

           <span class="mi">0</span>       <span class="mf">0.66</span>      <span class="mf">0.84</span>      <span class="mf">0.74</span>        <span class="mi">50</span>
           <span class="mi">1</span>       <span class="mf">0.69</span>      <span class="mf">0.45</span>      <span class="mf">0.55</span>        <span class="mi">40</span>

    <span class="n">accuracy</span>                           <span class="mf">0.67</span>        <span class="mi">90</span>
   <span class="n">macro</span> <span class="n">avg</span>       <span class="mf">0.67</span>      <span class="mf">0.65</span>      <span class="mf">0.64</span>        <span class="mi">90</span>
<span class="n">weighted</span> <span class="n">avg</span>       <span class="mf">0.67</span>      <span class="mf">0.67</span>      <span class="mf">0.65</span>        <span class="mi">90</span>
</code></pre></div></div> <p>From here on we would like to understand on what basis, that is, due to which tokens BERT made its predictions. Before we move on, let us have a quick look at how the BERT model would embed a given example sentence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0094</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0187</span><span class="p">,</span>  <span class="mf">0.0148</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0247</span><span class="p">,</span>  <span class="mf">0.0179</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0125</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0353</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0362</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0270</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0264</span><span class="p">,</span>  <span class="mf">0.0183</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0400</span><span class="p">,</span>  <span class="mf">0.0027</span><span class="p">,</span>  <span class="mf">0.0385</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0340</span><span class="p">,</span>  <span class="mf">0.0006</span><span class="p">,</span>  <span class="mf">0.0448</span><span class="p">],</span>
        <span class="p">...,</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0541</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0053</span><span class="p">,</span>  <span class="mf">0.0424</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mf">0.0058</span><span class="p">,</span>  <span class="mf">0.0142</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0847</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0209</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0086</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0202</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0561</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0209</span><span class="p">,</span>  <span class="mf">0.0158</span><span class="p">]],</span>
       <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">mps:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">13</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>
</code></pre></div></div> <h2 id="deriving-explanations-through-attributions-with-lime">Deriving explanations through attributions with LIME</h2> <p>LIME analyzes how changes to the input affect the model’s predictions. It creates a local linear approximation for each instance by making small modifications to the input and observing the resulting changes in output. The weights from this linear model indicate the importance of each token, serving as saliency scores. The original paper (Ribeiro et al. 2016) introducing LIME can be found here: <a href="https://dl.acm.org/doi/10.1145/2939672.2939778">https://dl.acm.org/doi/10.1145/2939672.2939778</a>.</p> <p>In this part of the project, we will make use of the <a href="https://captum.ai">Captum</a> library to investigate how the BERT model made its predictions. Captum is built on PyTorch and an excellent library for many explainability methods in machine learning. As we want to implement LIME for text, the following approach is built upon the Captum <a href="https://captum.ai/tutorials/Image_and_Text_Classification_LIME">tutorial</a> for LIME. For analyzing texts, we need to use the LimeBase version. The documentation can be found here: <a href="https://captum.ai/api/lime.html">https://captum.ai/api/lime.html</a>.</p> <p>Captum’s LimeBase requires to define several parts of the LIME approach manually. We shall start with this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encode text indices into latent representations &amp; calculate cosine similarity
</span><span class="k">def</span> <span class="nf">exp_embedding_cosine_distance</span><span class="p">(</span><span class="n">original_inp</span><span class="p">,</span> <span class="n">perturbed_inp</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    
    <span class="n">original_inp</span> <span class="o">=</span> <span class="n">original_inp</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>
    <span class="n">perturbed_inp</span> <span class="o">=</span> <span class="n">perturbed_inp</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>

    <span class="c1"># We want to compare the BERT embeddings from the final hidden states and use their mean
</span>    <span class="c1"># LIME will ablate some of the input tokens and the left over is perturbed_inp; we embed this with our BERT model and make a comparison through cosine similarity with the original embedded input
</span>
    <span class="c1"># The model internally adds position embeddings based on the input length and assumes a default sequence starting from position 0 
</span>    <span class="n">original_emb</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">original_inp</span><span class="p">,</span> <span class="bp">None</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">perturbed_emb</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">perturbed_inp</span><span class="p">,</span> <span class="bp">None</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">F</span><span class="p">.</span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">original_emb</span><span class="p">,</span> <span class="n">perturbed_emb</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">distance</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># One hot encoding for masking tokens randomly for ablation
</span><span class="k">def</span> <span class="nf">bernoulli_perturb</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>  <span class="c1"># Amount of masked tokens
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

    <span class="c1"># Ensuring at least one token remains (by forcing the first token to stay) because the BERT model expects an input of at least one token
</span>    <span class="k">if</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))]</span> <span class="o">=</span> <span class="mi">1</span>  

    <span class="k">return</span> <span class="n">mask</span>

<span class="c1"># Removing absent token based on the intepretable representation sample
</span><span class="k">def</span> <span class="nf">interp_to_input</span><span class="p">(</span><span class="n">interp_sample</span><span class="p">,</span> <span class="n">original_input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">original_input</span><span class="p">[</span><span class="n">interp_sample</span><span class="p">.</span><span class="nf">bool</span><span class="p">()].</span><span class="nf">view</span><span class="p">(</span><span class="n">original_input</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Captum's customizable Lime class
</span><span class="n">LIME</span> <span class="o">=</span> <span class="nc">LimeBase</span><span class="p">(</span>
    <span class="n">predict</span><span class="p">,</span>
    <span class="n">interpretable_model</span><span class="o">=</span><span class="nc">SkLearnLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.08</span><span class="p">),</span>
    <span class="n">similarity_func</span><span class="o">=</span><span class="n">exp_embedding_cosine_distance</span><span class="p">,</span>
    <span class="n">perturb_func</span><span class="o">=</span><span class="n">bernoulli_perturb</span><span class="p">,</span>
    <span class="n">perturb_interpretable_space</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">from_interp_rep_transform</span><span class="o">=</span><span class="n">interp_to_input</span><span class="p">,</span>
    <span class="n">to_interp_rep_transform</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</code></pre></div></div> <p>Everything is in place to loop through the data’s hateful and non-hateful sentences, deriving attributions with our custom LimeBase class.</p> <p>The parameter <code class="language-plaintext highlighter-rouge">show_progress</code> is a widget that might not work in all IDEs right away; I had to adjust my vscode settings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Saving all attributions to this list for later comparisons
</span><span class="n">LIME_attributions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="n">LIME</span><span class="p">.</span><span class="nf">attribute</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">encoding</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="c1"># Adding batch dimension for Captum
</span>        <span class="n">target</span><span class="o">=</span><span class="n">BERT_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="c1"># This is a hyperparameter that turned out to work well when set to 200 for our task
</span>        <span class="n">show_progress</span><span class="o">=</span><span class="bp">False</span>  <span class="c1"># vscode is not properly showing progress, must edit vscode's settings.json for widgets (add: "jupyter.widgetScriptSources": ["jsdelivr.com", "unpkg.com"]); if problems persist, set to False
</span>    <span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">LIME_attributions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attrs</span><span class="p">.</span><span class="nf">tolist</span><span class="p">())</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>We can have a look at the attributions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">LIME_attributions</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2673813998699188</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16184958815574646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0753924623131752</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2013273388147354</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.452242910861969</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0483330637216568</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11272305995225906</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.14384473860263824</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">...]</span>
</code></pre></div></div> <p>The original data collection from Alacam et al. (2024) contained instances that were later dropped. This led to unique sentence numbers from 1 to 93. To avoid mismatches with the human rationales for given sentences, we should create a list of the actual sentence numbers from the experiment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_sentence_num</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">experiment_sents</span><span class="p">.</span><span class="nf">keys</span><span class="p">()))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">unique_sentence_num</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">90</span>
</code></pre></div></div> <p>Let’s save the attributions together with their unique ids and sentences so we can later load them for our evaluation with respect to the human rationales.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">encoding</span><span class="p">)</span> <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">encodings</span><span class="p">]</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">LIME_attributions</span><span class="p">):</span>
    <span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">attr</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_LIME_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">BERT_LIME_attributions</span> <span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
</code></pre></div></div> <h2 id="deriving-explanations-through-attributions-with-saliency">Deriving explanations through attributions with Saliency</h2> <p>It is reasonable to use another explainability method because these methods can differ with respect to models and datasets (consider Atanasova et al. (2020)). In what follows, we will implement Saliency for our BERT model and data. The method was introduced in this paper (Simonyan et al. 2013): <a href="https://arxiv.org/abs/1312.6034">https://arxiv.org/abs/1312.6034</a>.</p> <p>Saliency is a gradient based explainability approach. Gradient-based methods generate saliency maps by calculating the gradient of the output with respect to the input. Unfortunately, it requires a little different set-up for our BERT model. We need to change our predict function for BERT; instead of letting it make predictions from the encodings, it needs to base its predictions on the embeddings from the final hidden states of the BERT model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_new</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">BERT</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>As with the Lime class we assign the forward function <code class="language-plaintext highlighter-rouge">predict_new(embeddings, mask)</code> to our Saliency object.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SALIENCY</span> <span class="o">=</span> <span class="nc">Saliency</span><span class="p">(</span><span class="n">predict_new</span><span class="p">)</span>
</code></pre></div></div> <p>If we use the embeddings for the model’s predictions, we’ll also need to handover masks as they are usually calculated based on the encodings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_SALIENCY</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">cls</span><span class="p">):</span>
    
    <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="nb">input</span> <span class="o">=</span> <span class="n">BERT</span><span class="p">.</span><span class="n">bert</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">attrs</span> <span class="o">=</span> <span class="n">SALIENCY</span><span class="p">.</span><span class="nf">attribute</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>  <span class="c1"># Feeding the embeddings to the predict_new() function
</span>        <span class="n">target</span><span class="o">=</span><span class="n">cls</span><span class="p">,</span> 
        <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">masks</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Passing masks to predict_new()
</span>    
    <span class="c1"># L2 normalization for the attributions of Saliency
</span>    <span class="n">attributions</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attributions</span>
</code></pre></div></div> <p>We can now run through the data as with LIME and derive attributions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">cls</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">encodings</span><span class="p">),</span><span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Processing</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">attr</span> <span class="o">=</span> <span class="nf">calculate_SALIENCY</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">cls</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">BERT_tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attr</span><span class="p">.</span><span class="nf">tolist</span><span class="p">())</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Let’s save the attributions to another file.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_SALIENCY_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">BERT_SALIENCY_attributions</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
</code></pre></div></div> <h2 id="evaluating-the-results-human-vs-bert-rationales">Evaluating the results (human vs. BERT rationales)</h2> <p>Finally, we want to make comparisons between the tokens that were relevant for BERT’s hate detection and those that were considered as relevant by the annotators. We are interested in how well the model’s attributions align with the human rationales. We can use both explainability methods to check for the alignment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_LIME_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">BERT_SALIENCY_attributions.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
    <span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>

<span class="c1"># Converting string keys to integers
</span><span class="n">BERT_LIME_attributions</span> <span class="o">=</span> <span class="p">{</span><span class="nf">int</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">BERT_LIME_attributions</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
<span class="n">BERT_SALIENCY_attributions</span> <span class="o">=</span> <span class="p">{</span><span class="nf">int</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">BERT_SALIENCY_attributions</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
</code></pre></div></div> <p>We need to convert the subword tokens into full words. This requires a customized function for the dataset. It’s not perfect, but shall suffice for most instances.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">subwords</span><span class="p">,</span> <span class="n">subword_rationales</span><span class="p">):</span>

    <span class="n">human_level_rationales</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="n">word_rationale</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">subword</span><span class="p">,</span> <span class="n">rat</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">subwords</span><span class="p">,</span> <span class="n">subword_rationales</span><span class="p">)):</span>

        <span class="k">if</span> <span class="n">subword</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">[CLS]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">[SEP]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">]:</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word_rationale</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">rat</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nf">any</span><span class="p">([</span><span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">##</span><span class="sh">'</span><span class="p">),</span>
                        <span class="n">subword</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">#</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">],</span>
                        <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> 
                        <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">#</span><span class="sh">"</span><span class="p">]):</span>
                <span class="c1"># Mean from subwords will be added to human level rationales
</span>                <span class="n">human_level_rationales</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">word_rationale</span><span class="p">))</span>
                <span class="n">word_rationale</span> <span class="o">=</span> <span class="p">[]</span>
            
    <span class="k">return</span> <span class="n">human_level_rationales</span>
</code></pre></div></div> <p>We can evaluate the alignment in terms of the Pearson correlation coefficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
       
    <span class="n">corrs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
        <span class="c1"># Zero variance check because some attributions might be only zeros
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">corrs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Default to 0 correlation
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">corrs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">pearsonr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">mean_correlation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">corrs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mean_correlation</span>  
</code></pre></div></div> <p>To calculate the correlation, we focus on instances where the model’s prediction matches the majority human annotation. Specifically, we analyze local explanations only for sentences where the model made a correct prediction, allowing us to assess how closely its rationales correspond to the average human rationales.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">human_rationales</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">correct_prediction_labels</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">BERT_rationales_LIME</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">BERT_rationales_SALIENCY</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">gold</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">avg_human_labels</span><span class="p">,</span> <span class="n">BERT_predictions</span><span class="p">):</span>

    <span class="n">sent_id</span> <span class="o">=</span> <span class="n">unique_sentence_num</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gold</span> <span class="o">==</span> <span class="n">pred</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gold</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">average_human_hate_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">].</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">average_human_no_hate_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">].</span><span class="nf">values</span><span class="p">())</span>

        <span class="n">LIME_attrs_word_level</span> <span class="o">=</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">BERT_LIME_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># LIME comes with positive and negative attributions; we're only interested in the positive ones because these are correlated with the model's targets
</span>        <span class="n">p_LIME</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">LIME_attrs_word_level</span><span class="p">]</span>

        <span class="n">p_SALIENCY</span> <span class="o">=</span> <span class="nf">subword_to_human_level</span><span class="p">(</span><span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">BERT_SALIENCY_attributions</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">human_rationales</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="n">correct_prediction_labels</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">gold</span>
        <span class="n">BERT_rationales_LIME</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_LIME</span>
        <span class="n">BERT_rationales_SALIENCY</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_SALIENCY</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Finally, we can compute the Pearson correlation coefficient for LIME attributions…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> <span class="n">BERT_rationales_LIME</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">float64</span><span class="p">(</span><span class="mf">0.14380772549112125</span><span class="p">)</span>
</code></pre></div></div> <p>… and for Saliency attributions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">compute_mean_correlation</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">float64</span><span class="p">(</span><span class="mf">0.6632999675122192</span><span class="p">)</span>
</code></pre></div></div> <p>LIME’s sampling includes an element of randomness; therefore, it would be reasonable to repeat the LIME attribution several times to arrive at average results.</p> <p>With five repititions, I get a positive correlation of 0.17 using LIME, which suggests a weak correlation between the BERT and human rationales. For Saliency, I have a much stronger colleration of 0.66. Now we can see how much influence also the chosen explainability method has. For LIME and Saliency, the BERT model has been the same. However, the BERT model appears much more aligned when seen from the perspective of the Saliency attributions.</p> <h2 id="visualizing-the-results-alignment">Visualizing the results (alignment)</h2> <p>Finally, we can visualize our results to see how the explainability methods make attributions with respect to the BERT model and its predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_triple_heatmaps</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">,</span> <span class="n">data3</span><span class="p">,</span> <span class="n">xticklabels</span><span class="p">,</span> <span class="n">title1</span><span class="p">,</span> <span class="n">title2</span><span class="p">,</span> <span class="n">title3</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Creating three vertical subplots
</span>    
    <span class="c1"># Plotting the first heatmap (top) - Human rationales
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data1</span><span class="p">]),</span> 
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span>
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Human rationales</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
    
    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title1</span><span class="p">)</span>
    
    <span class="c1"># Plotting the second heatmap (middle) - Saliency attributions
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data2</span><span class="p">]),</span>
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span> 
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span> 
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Saliency</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
    
    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title2</span><span class="p">)</span>

    <span class="c1"># Plotting the third heatmap (bottom) - LIME attributions
</span>    <span class="n">sn</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data3</span><span class="p">]),</span> 
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
               <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span> 
               <span class="n">xticklabels</span><span class="o">=</span><span class="n">xticklabels</span><span class="p">,</span> 
               <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">LIME</span><span class="sh">"</span><span class="p">],</span> 
               <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">)</span>
    
    <span class="n">ax3</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title3</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>  <span class="c1"># Adjusting layout to prevent overlap
</span>    
    <span class="c1"># Saving the figure
</span>    <span class="k">if</span> <span class="n">save_path</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>We can define a directory path here where we will save our visualizations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dir_path_visual</span> <span class="o">=</span> <span class="sh">"</span><span class="s">visualizations</span><span class="sh">"</span>
</code></pre></div></div> <p>We loop through the rationales and attributions to create heatmaps for the words from the experiment’s sentences.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_and_plot_heatmaps</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">,</span> <span class="n">correct_prediction_labels</span><span class="p">,</span><span class="n">BERT_rationales_SALIENCY</span><span class="p">,</span> <span class="n">BERT_rationales_LIME</span><span class="p">,</span> <span class="n">experiment_sents</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>  <span class="c1"># Creating output directory if it doesn't exist
</span>    
    <span class="k">for</span> <span class="n">sno</span> <span class="ow">in</span> <span class="n">human_rationales</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>

        <span class="n">hate_data</span> <span class="o">=</span> <span class="n">human_rationales</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>
        <span class="n">saliency_data</span> <span class="o">=</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>
        <span class="n">lime_data</span> <span class="o">=</span> <span class="n">BERT_rationales_LIME</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Plotting heatmap for rationales, saliency, and LIME for given sno: </span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                        
        <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">experiment_sents</span><span class="p">[</span><span class="n">sno</span><span class="p">].</span><span class="nf">strip</span><span class="p">(</span><span class="sh">'</span><span class="s">.,!?</span><span class="sh">'</span><span class="p">).</span><span class="nf">split</span><span class="p">()</span>
                            
        <span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">hate_heatmap_key_</span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Hate</span><span class="sh">"</span> <span class="k">if</span> <span class="n">correct_prediction_labels</span><span class="p">[</span><span class="n">sno</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="sh">"</span><span class="s">No Hate</span><span class="sh">"</span>
                            
        <span class="nf">plot_triple_heatmaps</span><span class="p">(</span><span class="n">hate_data</span><span class="p">,</span> 
                            <span class="n">saliency_data</span><span class="p">,</span> 
                            <span class="n">lime_data</span><span class="p">,</span> 
                            <span class="n">xticklabels</span><span class="p">,</span> 
                            <span class="n">title1</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentence No. </span><span class="si">{</span><span class="n">sno</span><span class="si">}</span><span class="s"> Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> 
                            <span class="n">title2</span><span class="o">=</span><span class="sa">f</span><span class="sh">""</span><span class="p">,</span>
                            <span class="n">title3</span><span class="o">=</span><span class="sa">f</span><span class="sh">""</span><span class="p">,</span>
                            <span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">process_and_plot_heatmaps</span><span class="p">(</span><span class="n">human_rationales</span><span class="p">,</span> <span class="n">correct_prediction_labels</span><span class="p">,</span> <span class="n">BERT_rationales_SALIENCY</span><span class="p">,</span> <span class="n">BERT_rationales_LIME</span><span class="p">,</span> <span class="n">experiment_sents</span><span class="p">,</span> <span class="n">dir_path_visual</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Explaining_BERT/visualization_example_2-480.webp 480w,/assets/img/Explaining_BERT/visualization_example_2-800.webp 800w,/assets/img/Explaining_BERT/visualization_example_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Explaining_BERT/visualization_example_2.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="references">References</h2> <p>Özge Alacam, Sanne Hoeken, and Sina Zarrieß. 2024. Eyes don‘t lie: Subjective hate annotation and detection with gaze. In <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, pages 187–205, Miami, Florida, USA. Association for Computational Linguistics.</p> <p>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classification. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 3256–3274, Online. Association for Computational Linguistics.</p> <p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “why should i trust you?”: Explain- ing the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD ’16, page 1135–1144, New York, NY, USA. Association for Computing Machinery.</p> <p>Sarthak Roy, Ashish Harshvardhan, Animesh Mukherjee, and Punyajoy Saha. 2023. Probing LLMs for hate speech detection: strengths and vulnerabilities. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 6116–6128, Singapore. Association for Computational Linguistics.</p> <p>Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. <em>CoRR</em>, abs/1312.6034.</p> <p>The <strong>Jupyter notebook</strong> for this project can be found here: <a href="https://github.com/omseeth/explaining_BERT_with_LIME_Saliency">https://github.com/omseeth/explaining_BERT_with_LIME_Saliency</a></p>]]></content><author><name></name></author><category term="BERT"/><category term="LIME"/><category term="Saliency"/><category term="captum"/><category term="PyTorch"/><category term="explainable"/><category term="AI"/><summary type="html"><![CDATA[Tutorial with full code]]></summary></entry><entry><title type="html">How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch</title><link href="https://omseeth.github.io/blog/2024/CNN_sentiment_analysis/" rel="alternate" type="text/html" title="How to use a Convolutional Neural Network (CNN) for text classification (sentiment analysis) with PyTorch"/><published>2024-12-27T10:00:00+00:00</published><updated>2024-12-27T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/CNN_sentiment_analysis</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/CNN_sentiment_analysis/"><![CDATA[<p>This tutorial is an introduction to <strong>Convolutional Neural Networks</strong> (CNNs) for <strong>sentiment analysis</strong> with PyTorch. There are already a few tutorials and solutions for this task by <a href="https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7">Gal Hever</a>, <a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/">Jason Brownlee</a>, or <a href="https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/3%20-%20Convolutional%20Neural%20Networks.ipynb">Ben Trevett</a>. However, they are either written in Keras or lack some explanations that I find essential for understanding the underlying mechanics of CNNs as well as their PyTorch specific implementations. I hope this tutorial will therefore help the interested reader to learn more about CNNs and how to implement them for NLP tasks, such as sentiment analysis.</p> <p>What follows is partially the result of a lecture given by <a href="https://bplank.github.io/">Barbara Plank</a> at LMU in 2024, whom I’d like to thank along with her lab <a href="https://mainlp.github.io/">MaiNLP</a> for letting me use some of their code. The theoretical part discussed in this tutorial is also highly indebted to <a href="https://github.com/rasbt/machine-learning-book">Raschka et al. (2022)</a>, whose book on machine learning is for me one of the best resources.</p> <p>This tutorial is divided into the following chapters:</p> <ul> <li><strong>Preliminaries</strong> imports</li> <li><strong>Section 1)</strong> Preprocessing the dataset</li> <li><strong>Section 2)</strong> Theoretical foundations of CNNs for classification</li> <li><strong>Section 3)</strong> Implementing a CNN with PyTorch</li> <li><strong>Section 4)</strong> Evaluation of results</li> </ul> <h2 id="preliminaries-imports">Preliminaries: imports</h2> <p>In this tutorial, we’ll be using several different libraries. The following imports shall become clear as we develop our project. I’m using Python <strong>3.12.0</strong>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">In case the packages are not installed on your machine, let</span><span class="sh">'</span><span class="s">s begin with 
installing the ones that we</span><span class="sh">'</span><span class="s">ll need for our task. We can run commands with an 
exclamation mark in Jupyter Notebooks.</span><span class="sh">"""</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span> <span class="n">transformers</span> <span class="n">tokenizers</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span> <span class="n">torch</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">trainers</span>
<span class="kn">from</span> <span class="n">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</code></pre></div></div> <h2 id="section-1-preprocessing-the-dataset">Section 1) Preprocessing the dataset</h2> <h4 id="loading-the-data">Loading the data</h4> <p>Our goal is to use data to train a model that can identify the sentiment of a given text instance. In other words, we’ll implement a classifier using supervised learning. The backbone of our sentiment classifier will be a CNN. The data we’re using is taken from <a href="https://huggingface.co/datasets/dair-ai/emotion">Saravia et al. (2018)</a>. It consists of English Twitter messages labeled with six “emotions”: <em>anger</em>, <em>fear</em>, <em>joy</em>, <em>love</em>, <em>sadness</em>, and <em>surprise</em>. The dataset is available on HuggingFace and we can load it with the <a href="https://pypi.org/project/datasets/">datasets package</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">dair-ai/emotion</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can have a quick look at how the dataset is splitted and structured.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emotions</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">16000</span>
    <span class="p">})</span>
    <span class="n">validation</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="nc">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
<span class="p">})</span>
</code></pre></div></div> <p>The labels are already numericalized and the text is lowercased, as we can see when inspecting a single instance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The label IDs correspond to the following label order:
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">sadness</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">joy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">anger</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fear</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">surprise</span><span class="sh">"</span><span class="p">]</span>

<span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">i didnt feel humiliated</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">im grabbing a minute to post i feel greedy wrong</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am ever feeling nostalgic about the fireplace i will know that it is still on the property</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">i am feeling grouchy</span><span class="sh">'</span><span class="p">],</span>
 <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
</code></pre></div></div> <p>To allow easier processing, it helps to bind the respective splits to different variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">emotions</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <h4 id="tokenization">Tokenization</h4> <p>Before we can use the data to train our model, we need to split up the sentences into tokens. We also want to batch our data so that the model can be trained with more instances at the same time. In this tutorial, we’ll do the tokenization and batching with custom functions.</p> <p>For our tokenization, we can use a subword tokenization algorithm that is called <a href="https://huggingface.co/learn/nlp-course/en/chapter6/5">Byte Pair Encoding (BPE)</a>, which was introduced in Sennrich et al. (2016) and is based on an algorithm introduced by Gage (1994). The motivation of this tokenization technique is to let the data decide what subword tokens we’ll have. Another advantage of BPE is that it allows us to deal with unknown words that won’t be part of the training vocabulary but might appear in test data. With BPE, unknown words can be decomposed into their respective subword parts and in this fashion still be processed.</p> <p>Instead of implementing the BPE tokenizer completely from scratch, we can use the <a href="https://pypi.org/project/tokenizers/">tokenizers package</a>. We also want to pad and truncate each text with a fixed sequence length. If text instances are short, we pad them with 0s; if they are long, we truncate them to the maximum length. This will make the training and inference processes easier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We want to begin with only 5000 words in our vocabulary and a fixed sequence 
# length of 64
</span><span class="n">vocab_n</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Initialize a tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="nc">BPE</span><span class="p">())</span>

<span class="c1"># We can have a preliminary splitting of the text on spaces and punctuations 
# with Whitespace()
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="nc">Whitespace</span><span class="p">()</span>

<span class="c1"># Padding instances to the right with 0s
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_padding</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># Truncating long sequences
</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">enable_truncation</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

<span class="c1"># We limit our vocabulary and train the tokenizer
</span><span class="n">tokenizer_trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="p">.</span><span class="nc">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_n</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">train_from_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">trainer</span><span class="o">=</span><span class="n">tokenizer_trainer</span><span class="p">)</span>
</code></pre></div></div> <p>After training our custom BPE tokenizer, we aim to transform the raw training data into vector representations, where the input is tokenized and converted into corresponding token IDs. Fortunately, the labels are already numerical. That said, we still need to convert these numerical representations into PyTorch tensors to enable the PyTorch model to process the training instances. To accomplish this, we can write a few custom functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to tokenize text and return corresponding token IDs as tensors.

    Args:
        text, str: Text instance from training data.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        Tensor: One-dimensional PyTorch tensor with token IDs.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">).</span><span class="n">ids</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Helper function to return label as tensor.

    Args:
        label, int: Label from instance.
    Returns:
        Tensor: One-dimensional PyTorch tensor containing the label index.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Transforms input dataset to tokenized vector representations.

    Args:
        data, dict: Dictionary with text instances and labels.
        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.
    Returns:
        list: List with tensors for the input texts and labels.
    </span><span class="sh">"""</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="nf">preprocess_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        
        <span class="n">instances</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">instances</span>
</code></pre></div></div> <p>Let’s tokenize, pad, and truncate our datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">val_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">validation_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_instances</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>We shall inspect the second training instance, which was:</p> <p><code class="language-plaintext highlighter-rouge">i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake.</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_instances</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="p">(</span><span class="nf">tensor</span><span class="p">([</span>   <span class="mi">8</span><span class="p">,</span>  <span class="mi">161</span><span class="p">,</span>  <span class="mi">103</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>   <span class="mi">55</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span> <span class="mi">1173</span><span class="p">,</span>   <span class="mi">36</span><span class="p">,</span>   <span class="mi">58</span><span class="p">,</span>  <span class="mi">807</span><span class="p">,</span>  <span class="mi">587</span><span class="p">,</span> <span class="mi">1129</span><span class="p">,</span>
            <span class="mi">130</span><span class="p">,</span>  <span class="mi">215</span><span class="p">,</span>  <span class="mi">219</span><span class="p">,</span>  <span class="mi">382</span><span class="p">,</span>  <span class="mi">444</span><span class="p">,</span>  <span class="mi">197</span><span class="p">,</span> <span class="mi">2819</span><span class="p">,</span>   <span class="mi">42</span><span class="p">,</span>   <span class="mi">47</span><span class="p">,</span> <span class="mi">2670</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">]),</span>
    <span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <p>We can observe that “i” has token index 8. Also notice how “so” appears in the vector representation of the text with index 58 twice. The corresponding label for the sequence is 0 and would be “sadness”.</p> <h4 id="batching">Batching</h4> <p>Batching the instances will enable the model to process multiple instances simultaneously. To achieve this, let’s write a custom batching function. There are different ways to perform batching, which involves concatenating instances and dividing them into manageable groups. One convenient method is to use the <a href="https://pytorch.org/docs/main/generated/torch.stack.html">torch.stack()</a> function, which concatenates a sequence of tensors. However, all tensors must be of the same length. Since we already padded and truncated our text instances, this requirement is satisfied.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    Batches input instances along the given size and returns list of batches.

    Args:
        instances, list: List of instances, containing a tuple of two tensors 
            for each text as well as corresponding label.
        batch_size, int: Size for batches.
        shuffle, bool: If true, the instances will be shuffled before batching.
    Returns:
        list: List containing tuples that correspond to single batches.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>

    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># We iterate through the instances with batch_size steps
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">instances</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>

        <span class="c1"># Stacking the instances with dim=0 (default value)
</span>        <span class="n">batch_texts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">instance</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">batch_texts</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">batches</span>
</code></pre></div></div> <h2 id="section-2-theoretical-foundations-of-cnns-for-classification">Section 2) Theoretical foundations of CNNs for classification</h2> <p>After having prepared our data, we can start with implementing our model. In this tutorial, we’ll be using a Convolutional Neural Network (CNN), which was introduced in LeCun et al. (1989). The CNN bears resemblance to the Neocognitron (Fukushima 1980) as well as Time-Delay Neural Networks (Waibel et al. 1989). This section is dedicated to the theoretical background of CNNs. In <strong>Section 3</strong>, we’ll implement the model with PyTorch.</p> <h4 id="the-architecture-of-a-cnn">The architecture of a CNN</h4> <p>A CNN is a fully connected network that operates similar to Feedforward Neural Networks (FNNs) in a feedforward fashion. The input is processed along layers and forwarded to the final layer, which can be used for predictions. The idea of a CNN is to consecutively extract salient features, such as shapes in terms of their respective pixels, building an abstract feature hierarchy for any given input. In this fashion, the model will be aligned through its training to reoccurring patterns, which it can “recognize.” It’s no surprise that CNNs were invented for handwritten digit recognition in images.</p> <p>The basic architecture of a CNN is as follows: For any given input, the CNN uses <strong>convolutions</strong>, that is, a <strong>filter</strong> (also called <strong>kernel</strong>) technique to extract local features from the input (e.g., pixels). After this filter has <strong>shifted</strong> over the whole input, the extractions are combined together into <strong>feature maps</strong> and then transformed with <strong>pooling</strong>. Pooling helps with singling out dominant features and allows to reduce the dimensions of the feature maps. This process can be repeated multiple times where additional filters can shift again over the pooled output from previous convolutions. Finally, for the last part of the network a FNN is used to produce logits for the desired task.</p> <p>One CNN layer consists of one convolutional and one pooling layer.</p> <h4 id="filtering">Filtering</h4> <p>Filters shift over the input. If the input is one-dimensional, that is, a vector, then the filter is itself a one-dimensional, usually smaller vector. If the input is two-dimensional, a matrix, then the filter needs to be a matrix, too.</p> <p>For a one-dimensional input of length \(n=8\), such as \([2, 1, 0, 3, 6, 7, 9, 1]\), we can define a filter of size \(m=3\), such as \([1, -1, 0]\). As the shifting starts, we’ll multiply the first three entries of the input vector with the filter:\([2, 1, 0] * [1, -1, 0]^T = 1\) In this fashion, we obtain the first entry for our feature map: \(y[0]=1\). Next, we shift the filter one step forward (with <strong>stride</strong>=1) and repeat the multiplication, until the filter reaches the end of the input, bounded by its last index (see <strong>Fig. 1</strong>). We can formalize this operation between the filter vector <em>f</em> and the input vector <em>e</em> as follows:</p> \[e*f\rightarrow y[i] = \sum_{k=1}^m{e[i+k-1]\cdot f[k]}\] <p>In fact, the filter size is a hyperparameter that can be set to any positive integer smaller than the input length. In those scenarios where the filter is very large and exceeds the input, we can pad the input with additional elements so that the filter operation would be possible. There a several padding strategies to avoid that elements from the middle of the input will be covered more frequently by the filter than those from the edges.</p> <p>In our implementation (<strong>Section 3</strong>), we’ll be using the <em>same</em> padding strategy, which adds zeros to the input on all sides so that the output of the filter operation can have the same size as the input. Therefore, the amount of padding will depend on the size of the filter(/kernel). Consider Raschka et al. (2022, p. 457) for further details.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_1d-480.webp 480w,/assets/img/CNN_sentiment/convolution_1d-800.webp 800w,/assets/img/CNN_sentiment/convolution_1d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/convolution_1d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1</strong>: One-dimensional convolution</p> <p>The convolution described previously for one-dimensional inputs works in exactly the same way for two-dimensional inputs (see <strong>Fig. 2</strong>). If \(E_{n_1\times n_2}\) is the input matrix and \(F_{m_1\times m_2}\) the filter where \(m_1\leq n_1\) and \(m_2\leq n_2\), for each stride we can compute our feature map as follows:</p> \[E*F\rightarrow y[i][j] = \sum_{k_1=1}^{m_1}\sum_{k_2=1}^{m_2}{e[i+k_{1}-1][j+k_{2}-1]\cdot f[k_1][k_2]}\] <p>with \(y[i][j]\) corresponding to the respective feature representation in the map, which is also a matrix.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/convolution_2d-480.webp 480w,/assets/img/CNN_sentiment/convolution_2d-800.webp 800w,/assets/img/CNN_sentiment/convolution_2d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/convolution_2d.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2</strong>: Two-dimensional convolution</p> <h4 id="subsampling-layers-pooling">Subsampling layers: pooling</h4> <p>The idea of pooling layers is to reduce the size of the feature map and abstract from the features, either by picking out one local maximum feature value or averaging over a group of feature values. The former strategy is called max pooling, the latter is called average pooling. We’ll focus on max pooling (see <strong>Fig. 3</strong>).</p> <p>A pooling layer can be defined as \(P_{n_1 \times n_2}\), where the subscript indicates the area size over which the max operation is performed, while shifting across the entire map.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/max_pooling-480.webp 480w,/assets/img/CNN_sentiment/max_pooling-800.webp 800w,/assets/img/CNN_sentiment/max_pooling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/max_pooling.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3</strong>: Max pooling</p> <h4 id="adding-activation-functions">Adding activation functions</h4> <p>A FNN usually consists of a layer that is defined as \(z=Wx + b\) where \(W\) are the weights and \(b\) is an extra bias for input \(x\). Such a layer is often wrapped by a non-linear activation function \(\sigma\), as in \(A=\sigma (z)\). We adapt the convolutional layer so that it resembles the previous FNN construction. Let \(c = E*F + b\) be our layer with an additional bias term \(b\). As with the FFN we can wrap an activation function \(\sigma\) around \(c\). In many CNN implementations, the ReLu activation function is used for this purpose.</p> <p>LeCun et al. (1989) published their paper titled ‘Handwritten Digit Recognition with a Back-Propagation Network.’ Let us now examine which parts of a CNN are trainable using back-propagation. It’s the convolutional layers (filters) with their weights \(F\) and biases \(b\) (if included) that can be updated through back-propagation, guided by a loss function and gradient optimization. Recall that most CNNs also include a final fully connected feedforward neural network (FNN) layer for producing the output, and this layer is trainable as well. However, the pooling layer is not involved in the training process, as it does not contain learnable parameters.</p> <h4 id="multiple-channels">Multiple channels</h4> <p>The input to a CNN can be greater than one. If we’re dealing with images that are encoded in terms of RGB colors, we can split up the input into three two-dimensional matrices corresponding to red, green, and blue color information and feed them to the CNN. Each input matrix would be called one <strong>channel</strong>. For this matter, in most CNN implementations the convolutional layers expect an input with <strong>3 dimensions</strong> where \(E_{n_1\times n_2 \times c_{in}}\) would be the input of dimensions \(n_1\times n_2\) times \(c_{in}\), such as \(c_{in} = 3\) for three different color matrices.</p> <p>The further processing of the input allows a lot of variability: Each \(c_{in}\) input will have its own filter. The filters can be also stored as three-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}}\). Usually, the filtered results from each respective input will be element-wise added to create the output feature map. But sometimes it also helps to have multiple feature maps as outputs to capture different aspects from the input. In this case, the filters can be changed to four-dimensional tensors: \(F_{m_1\times m_2 \times c_{in}\times c_{out}}\) where \(c_{out}\) determines the numbers of feature maps that we want (see <strong>Fig. 4</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CNN_sentiment/four_dim_filter-480.webp 480w,/assets/img/CNN_sentiment/four_dim_filter-800.webp 800w,/assets/img/CNN_sentiment/four_dim_filter-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/CNN_sentiment/four_dim_filter.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4</strong>: Multiple filters (kernels) for three input channels, producing four feature maps</p> <p>Note, if we have for example four feature maps, we’ll also need four pooling layers. As the size of convolutional layers changes we’ll also have more parameters to train. This being said, remember that a FNN processing an input image would require weights of same size for its first layer. In contrast, the parameters of CNNs are bounded by the filter size. LeCun et al. (1989, p. 399) describe this aspect as “weight sharing” and note as one distinctive feature of CNNs that they significantly reduce the amount of parameters to be trained.</p> <h2 id="section-3-implementing-a-cnn-with-pytorch">Section 3) Implementing a CNN with PyTorch</h2> <p>Let’s begin with defining our classifier model. It’s reasonable to use embeddings for our input tokens. For that matter, our model should start with an embedding layer. The embeddings will be handed over to our CNN, which will consist of two convolutional layers with two different filters(/kernels). Finally, the input abstracted throughout the network needs to be passed to a final FFN layer for our sentiment prediction.</p> <p>To build an intuition for the whole process, let’s think again of the input. The input is a sequence (a tweet) from our data. This sequence will be translated into its correspoding token representation based on our custom trained BPE encoder. In other words, any incoming sequence will be a tensor of different indices. Next, the indices are handed over to our embedding layer, which serves as a look-up table for all tokens in the vocabulary and assigns the embedding representation to the input indices. This layer is a simple linear layer that is going to be trained along with the whole network later on. The initial values of the embeddings are random numbers.</p> <p>Up to this point, we can think of two strategies to run a first convolution over the input. If the input is 12 tokens long and each token has an embedding vector of size 300, the input to a potential convolutional layer would be \(12 \times 300\). The first strategy could be to apply a two-dimensional filter striding over this input matrix. The second strategy would be to use a one-dimensional filter that is going over one embedding dimension per time for the whole sequence, that is, over \(12 \times 1\) but with 300 channels for all embedding dimensions. If we want the output to be of the same length as the input, that is, 12, we need to pad the input to adjust for the filter length.</p> <p>In our implementation, we shall try the second strategy. Remember that the input channels will all be added together so before having our 300 embedding dimensions shrunk to one, we shall define an appropriate output channel size for the convolution. In other words, we’ll use as many as 100 filters for example to create an output of 100 dimensions for each respective input token.</p> <p>After having defined our model, we need to implement the training loop with a fitting method. To understand our training process we also need an evaluation mechanism. Finally, we need another method for making predictions from the trained classifier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN_Classifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> 
    CNN for sentiment classification with 6 classes, consisting of an embedding 
    layer, two convolutional layers with different filter sizes, different 
    pooling sizes, as well as one linear output layer.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># We can implement embeddings as a simple lookup-table for given word 
</span>        <span class="c1"># indices
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_vocab_size</span><span class="p">(),</span> <span class="mi">300</span><span class="p">)</span>

        <span class="c1"># One-dimensional convolution-layer with 300 input channels, and 100  
</span>        <span class="c1"># output channels as well as kernel size of 3; note that the
</span>        <span class="c1"># one-dimensional convolutional layer has 3 dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with with a one-dimensional sliding window of length 3, 
</span>        <span class="c1"># reducing in this fashion the sequence length 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># The input will be the reduced number of maximum picks from the
</span>        <span class="c1"># previous operation; the dimension of those picks is the same as the
</span>        <span class="c1"># output channel size from self.conv_1. We apply a different filter of 
</span>        <span class="c1"># size 5.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Pooling with window size of 5
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># Final fully connected linear layer from the 50 output channels to the
</span>        <span class="c1"># 6 sentiment categories 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Defining the forward pass of an input batch x.

        Args:
            x, tensor: The input is a batch of tweets from the data.
        Returns:
            y, float: The output are the logits from the final layer.
        </span><span class="sh">"""</span>
        <span class="c1"># x will correspond here to a batch; therefore, the input dimensions of 
</span>        <span class="c1"># the embedding will be by PyTorch convention as follows:
</span>        <span class="c1"># [batch_size, seq_len, emb_dim]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Unfortunately the embedding tensor does not correspond to the shape 
</span>        <span class="c1"># that is needed for nn.Conv1d(); for this reason, we must switch its 
</span>        <span class="c1"># order to [batch_size, emb_dim, seq_len] for PyTorch
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We can wrap the ReLu activation function around our convolution layer
</span>        <span class="c1"># The output tensor will have the following shape: 
</span>        <span class="c1"># [batch_size, 100, seq_len]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Applying max pooling of size 3 means that the output length of the 
</span>        <span class="c1"># sequence is shrunk to seq_len//3
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Output of the following layer: [batch_size, 50, seq_len//3]
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Shrinking the sequence length by 5
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>
        <span class="c1"># At this point we have a tensor with 3 dimensions; however, the final layer 
</span>        <span class="c1"># requires an input of size [batch_size x 50]. To get this value we can 
</span>        <span class="c1"># aggregate the values and continue only with their mean
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># In this fasion, the linear layer can be used to make predictions
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        Gradient based fitting method with Adam optimization and automatic 
        evaluation (F1 score) for each epoch.

        Args:
            train_instances, list: List of instance tuples.
            val_instances, list: List of instance tuples.
            epochs, int: Number of training epochs.
            batch_size, int: Number of batch size.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">train_batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span>
                <span class="n">train_instances</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_batches</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            
            <span class="n">train_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">val_f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">val_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> train F1 score: </span><span class="si">{</span><span class="n">train_f1</span><span class="si">}</span><span class="s">, validation F1 score: </span><span class="si">{</span><span class="n">val_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make inferences from the model.

        Args:
            input, tensor: Single instance.
        Returns:
            int: Integer for most probable class.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> 
        To make evaluations against the gold standard (true labels) from the 
        data.

        Args:
            instances, list: List of instance tuples.
            batch_size, int: Batch size.
        Returns:
            float: Macro F1 score for given instances.
        </span><span class="sh">"""</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="nf">batching</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">true</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
            <span class="n">true</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">pred</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

        <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="sh">"</span><span class="s">macro</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can now train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="nc">CNN_Classifier</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_instances</span><span class="p">,</span> <span class="n">val_instances</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">26</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">37.29</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.6349625340555586</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.5885997748733948</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">27</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">36.16</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">2</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9181883927660527</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8391332797173209</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">31</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">31.86</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">3</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9548322090243025</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8665508014136801</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">30</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">32.63</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">4</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9715161826479329</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.851259672136001</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">██████████</span><span class="o">|</span> <span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">33</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">30.13</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="n">Epoch</span> <span class="mi">5</span> <span class="n">train</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.9830196392925995</span><span class="p">,</span> <span class="n">validation</span> <span class="n">F1</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.8649424900902641</span>
</code></pre></div></div> <h2 id="section-4-evaluation-of-results">Section 4) Evaluation of results</h2> <p>After we have trained our model, we can use our test set to see how well it predicts sentiments for unseen examples.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f1_test</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">test_instances</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">F1 score for test set: </span><span class="si">{</span><span class="n">f1_test</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">F1 score for test set: 0.8403978699590415</code></p> <h4 id="hyperparameters">Hyperparameters</h4> <p>There are many hyperparameters in the entire modeling process that we can adjust to achieve even better results. These include different layer sizes, filters, poolings, and more efficient optimization techniques. Typically, dropout is applied during the training of such models to prevent overfitting. It is also worthwhile to explore the possibility of using pre-trained embedding representations.</p> <p>The <strong>Jupyter notebook</strong> for this project can be found here: <a href="https://github.com/omseeth/cnn_sentiment_analysis">https://github.com/omseeth/cnn_sentiment_analysis</a></p> <h2 id="references">References</h2> <p>Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biol. Cybernetics 36</em>, pages 193–202.</p> <p>Gage, P. (1994). A new algorithm for data compression. <em>C Users J.</em>, 12(2):23–38.</p> <p>LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. (1989). Handwritten digit recognition with a back-propagation network. In Touretzky, D., editor, <em>Advances in Neural Information Processing Systems</em>, volume 2. Morgan-Kaufmann</p> <p>Saravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y.-S. (2018). CARER: Contextualized affect representations for emotion recognition. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 3687–3697, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Sebastian, R., Yuxi, L., and Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Erk, K. and Smith, N. A., editors, <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics</p> <p>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. and Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. In <em>IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37</em>, no. 3, pages 328-339.</p>]]></content><author><name></name></author><category term="NLP"/><category term="CNN"/><category term="sentiment"/><category term="analysis"/><category term="text"/><category term="classification"/><category term="PyTorch"/><category term="Deep"/><category term="Learning"/><category term="tutorial"/><summary type="html"><![CDATA[Tutorial with full code]]></summary></entry><entry><title type="html">AI ethics, a primer</title><link href="https://omseeth.github.io/blog/2024/AI_ethics_primer/" rel="alternate" type="text/html" title="AI ethics, a primer"/><published>2024-11-23T10:00:00+00:00</published><updated>2024-11-23T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/AI_ethics_primer</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/AI_ethics_primer/"><![CDATA[<p>Many of us have heard of ethics in AI before. But what does ethics actually mean? And what could we learn from the different ethical traditions, some of which that date more than two thousand years back, when dealing with AI? We will see that it is worthy to learn from the classics of philosophy to reflect upon AI.</p> <p>In this post, we will touch on three aspects:</p> <ul> <li>Deep dive: traditions in ethics</li> <li>Inspiration: philosophy of technology</li> <li>Application: challenges when using AI ethically</li> </ul> <h2 id="deep-dive-traditions-in-ethics">Deep dive: traditions in ethics</h2> <p><strong>Ethics and morality:</strong> Ethics is a sub-discipline of philosophy, theology, psychology, sociology, education, political science, or anthropology. It is often helpful to divide ethics into three areas: 1) <strong>descriptive ethics</strong> dealing with the empirical question which moral rules and norms exist in each community, 2) <strong>normative ethics</strong> which reflects on reasons for moral systems, and 3) <strong>metaethics</strong> questioning the language we use in moral discourse. For example, from a descriptive point of view, we could investigate what psychological factors affect our moral decision making. The predominant ethical traditions, however, adhere to a normative approach.</p> <p><strong>The three traditions of classical ethics:</strong> Throughout the history of ethics, three traditions have influenced the field. Today’s ethicist can often still be traced to one of these traditions. The first tradition which many have heard before is often referred to as <strong>utilitarianism</strong>. A utilitarian would try to maximize the utility for the greatest number of people. It is sometimes also described as the greatest happiness principle. The second tradition in ethics is closely tied to the German philosopher, Immanuel Kant, who introduced the <strong>categorial imperative</strong> as man’s and woman’s best guide for moral behavior. The imperative says: “Act only according to that maxim whereby you can at the same time will that it should become a universal law.” (Kant 1993, p. 30) In other words, Kant provides us with a tool with which we can check whether any action is moral. The third tradition is the oldest. It is linked to the Ancient Greek philosopher Aristotle. In contrast to the other two traditions, Aristotle’s ethics consists in multiple virtues (e.g. prudence and temperance) that he and many of his followers believe to be leading to <strong>the good life</strong>.</p> <p><strong>Three different perspectives:</strong> The three dominant traditions in ethics can also be divided by what aspect they focus on. For example, utilitarianism considers only the results of actions. What counts is the aftermath! A Kantian ethicist, in contrast, sets her eyes on each particular action. She wonders whether it is ethically licensed by the categorical imperative. A virtue ethicist is rather interested in what virtues lead us to a particular action.</p> <h2 id="inspiration-philosophy-of-technology">Inspiration: philosophy of technology</h2> <p><strong>I, technology, and the world:</strong> The philosopher Don Ihde (1990) introduces a simple, but powerful, <strong>abstraction</strong> that can help us reflect on technologies. Consider yourself looking through a microscope, seeing the wonders of the world from a different angle. As you observe bacteria on a little petri dish, you forget everything around yourself. In fact, you have morphed with the microscope. It has become part of your vision and even of yourself. Ihde (1990) describes this relation with the technology with this short formula:</p> <p><code class="language-plaintext highlighter-rouge">(I – Technology) – World</code></p> <p>Imagine you are in a warm room and the rain is pouring down on your window. As you are looking at the thermostat on your wall, you are seeing that it is almost freezing outside. When you become aware of the temperature, you are not feeling the cold. It is transmitted to you by the thermostat. Ihde (1990) describes this constellation as follows:</p> <p><code class="language-plaintext highlighter-rouge">I – (Technology – World)</code></p> <p>Differently put, the temperature of the outside world in such situations is only indirectly accessible to you through the measuring device on your wall.</p> <h2 id="application-challenges-when-using-ai-ethically">Application: challenges when using AI ethically</h2> <p><strong>Technological relations and moral decisions:</strong> When making decisions involving AI, it is helpful to understand how the technology is being used. For instance, are we merging with a driving assistant that is powered with AI? Or do we confront reality, as it is mediated to us by AI powered algorithms; for instance, when we use social media platforms that present a particular representation of what a pleasant vacation ought to entail? Realizing our <strong>position with which we relate to AI</strong> can then help us with understanding how AI would mediate and even change our actions. For instance, AI could help us with achieving the best outcome for the most people by ameliorating our capacity to react quickly in critical situations. But it could also deceive us when we would ask a chatbot whether something would be acceptable as a universal moral law. For an ethically desirable way of developing and using AI, we need to be aware of how it affects <strong>our actions</strong>.</p> <p><strong>The Other:</strong> Finally, Ihde also considers situations where we find ourselves confronted with technology that we perceive as another being. He describes this sort of relation as follows (Ihde 1990):</p> <p><code class="language-plaintext highlighter-rouge">I – Technology – World</code></p> <p>And as AI advances, we might be more inclined to also consider AI as such a being that should deserve more attention. But whether we will ever see this one day is yet to be seen!</p> <p>In this post, you have learned a basic understanding of ethics. You saw an abstraction from the philosophy of technology to help position yourself when using AI. With this knowledge, I hope you feel motivated to use AI ethically.</p> <h2 id="bibliography">Bibliography</h2> <p>Ihde, D. (1990). Technology and the Lifeworld – From Garden to Earth. Indiana University Press.</p> <p>Kant, I. (1993). Groundwork of the Metaphysics of Morals. Translated by Ellington, James W. (3rd ed.). Hackett.</p>]]></content><author><name></name></author><category term="AI"/><category term="ethics"/><category term="utilitarianism"/><category term="Kant"/><category term="virtue"/><category term="ethics"/><category term="Don"/><category term="Ihde"/><category term="philosophy"/><category term="of"/><category term="technology"/><summary type="html"><![CDATA[Introduction to AI ethics that’s less than 1000 words long]]></summary></entry><entry><title type="html">Responsibility Protraction with Remotely Controlled Combat Vehicles</title><link href="https://omseeth.github.io/blog/2024/responspibility_protraction/" rel="alternate" type="text/html" title="Responsibility Protraction with Remotely Controlled Combat Vehicles"/><published>2024-10-19T10:00:00+00:00</published><updated>2024-10-19T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/responspibility_protraction</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/responspibility_protraction/"><![CDATA[<p><strong>Abstract</strong> This paper deals with the phenomenon that there has been little to no prosecution of misconduct with remotely controlled combat vehicles (RCCVs), such as armed drones, although it is likely that RCCVs have been deployed unjustly. While responsibility gaps are considered as one reason for this apparent lack of prosecution, it is argued that there are no such gaps because no realistic cases of unintended, uncontrolled, physical outcomes, unbeknownst to their operators, are conceivable. Instead, lack of prosecution is explained with responsibility protractions: The involvement of many stakeholders makes it difficult to pin down one responsible individual, errors can slip in unbeknownst to all parties involved, and “surgical attacks” with drones may have been considered to be less significant as bombing by aircraft and therefore largely neglected. To address possible misconduct with RCCVs, this paper finally considers the following measures: (a) that RCCVs should be deployed as little as possible, (b) more effort must be spent on ascribing responsibilities, (c) conscientious engagement should be promoted among RCCV operators, and (d) a novel military convention ought to mandate that pilots be stationed in countries where RCCVs are being deployed, whenever there is no official declaration of war against these respective countries.\(^1\)</p> <h2 id="introduction">Introduction</h2> <p>One recurring theme in robotics and AI is responsibility gaps (Matthias 2004; Coeckelbergh 2020). It follows from the thought that we can build machines for whose action neither the machine nor its developer(s) can be fully held responsible. Consider a machine that has been trained from vast amounts of data and been let to run on its own, causing consequences that could not be predicted. Since the machine’s developer(s) would not be the physical source of its actions and could not fully predict its performance, they would be only responsible for the machine in a reduced sense. But the machine could neither be held responsible when we agree that a machine lacks the necessary capacities to regulate and adapt its output, based on a moral understanding of the situation and a propensity to foresee any consequences (Köhler 2020, 3127). Furthermore, if we consider responsibility to be a form of accountability, a machine would also be incapable of answering or demonstrating accountability for its output, as would a human being, because a machine is not an appropriate recipient for praise or punishment (Sparrow 2007, 71; Danaher 2016). These points, when considered together, indicate that autonomous machine actions cause a gap in the assignment of responsibility.</p> <p>In military ethics, responsibility gaps have been discussed by considering autonomous military robots, also known as lethal autonomous weapons systems (LAWS) (Sparrow 2007; Lokhorst and van den Hoven 2012; Leveringhaus 2016; Müller 2016; Oimann 2023). In these discussions, it is assumed that their operators are “on” or “out” of the loop. If they are on the loop, it means that the systems run autonomously but the operators can intervene, if necessary. If the operators are out of the loop, the deployed systems are fully autonomous and follow their own programs and data. In both respects, the human-robot constellations that were discussed in this fashion have anticipated scenarios that are still rare because no fully automated military robots are being deployed as of today. The Kargu-2 drone by the Turkish company STM appears to be a new exception.</p> <p>In contrast, responsibility gaps with existing remotely controlled combat vehicles (RCCVs), such as the MQ-9 Reaper drone, have not been considered to a similar degree.\(^2\) For it is assumed that their pilots are “in” the loop, that is, that they control the system and everything it produces. Hence, most people think that the operators who deploy and steer RCCVs, such as aerial drones, can be held responsible for the consequences.</p> <p>Since drones from the MQ series are still being used by various countries (e.g., by the U.S., the U.K., or the Netherlands) and there is little literature on responsibility gaps with RCCVs, another consideration of responsibility seems justified. But even more interesting is the fact that, to my knowledge, no one has been held liable for drone strikes that were done without just cause while ground troops are periodically sentenced for war crimes. In the United States, for example, court cases have resulted in no convictions. What is also striking is that all of them were brought to court by civil parties only. Some cases on excessive force with RCCVs were eventually dismissed by referring to the “political question doctrine”, assuming that drone strikes must first be politically addressed (Kaufman and Diakun 2017, 120). A path to the merits has also been barred by state-secrets privileges.\(^3\) Conversely, a notable war crime that ended in prison time were the “thrill killings” by U.S. infantry soldier Pvt 1st Class Andrew Holmes in Kandahar, Afghanistan, in 2011 (Fallon 2011). The mention of these examples, and legal liability more generally, may help us in what follows to better understand the apparent problem with RCCVs, as liability is also rooted in the attribution of responsibility.</p> <p>Another reason to discuss responsibility allocation with RCCVs is to gain a better understanding of who would be the source for possible misconduct with RCCVs. In military ethics, it is generally accepted that no excessive force should be used to achieve military goals (Lazar 2016). However, one would need to be able to call out those who have created more harm and pain than can be justified, that is, those who are responsible, if this standard were to have any force. Therefore, considering responsibility regarding RCCVs would also be important if we are to maintain military standards.</p> <p>Having now introduced some of the thoughts that lie behind this paper, let me briefly sum up what follows: I discuss responsibility gaps with Alex Leveringhaus’s Ethics and Autonomous Weapons (2016). Leveringhaus provides us with useful criteria that lead me to the conclusion that there are no responsibility gaps with RCCVs; for no realistic cases of unintended, uncontrolled, physical outcomes occur unbeknownst to their operators with contemporary RCCVs. However, I argue in the next section that the legal disinterest regarding misconduct with RCCVs could still be explained by considering responsibility. For the complexity of RCCV deployment leads to responsibility protraction. Responsibility is protracted when too many stakeholders are involved so that it is very difficult to pin down one responsible individual. Furthermore, accumulative actions might also create undesirable outcomes by allowing errors to slip in unbeknownst to all parties involved. Ultimately, “surgical attacks” may appear less significant in comparison to heavy bombing by aircraft and have been therefore mostly ignored.</p> <p>In the last section, I discuss several measures that could help with reducing possible misconduct: (a) RCCVs should be deployed as little as possible, (b) more effort must be spent on ascribing responsibilities, (c) conscientious engagement should be promoted among RCCV operators, with the aim to incorporate their views into decision making processes, and (d) the introduction of a new military convention should require operators to be stationed in those countries where RCCVs are being deployed, provided that their states have not officially declared war against the respective country where they carry out their missions.</p> <h2 id="responsibility-gaps-with-remotely-controlled-combat-vehicles">Responsibility Gaps with Remotely Controlled Combat Vehicles</h2> <p>There are many theories of responsibility, but there is no uniquely accepted definition of what it means to be responsible. For example, David Miller (2001) maintains that there are roughly four types of responsibility. According to Miller, we can be causally (1) and morally (2) responsible, as well as responsible due to certain capacities that we might have (3), or because of our affiliation with a community (4). Briefly summarized: 1) If I damage your bike, by kicking it, I am causally responsible for the damage. 2) If your bike gets stolen because I have forgotten to lock it after having it borrowed, I would be morally responsible for your loss: I am not the actual source of the larceny, so I am not causally responsible, but I have neglected my duty to secure your possession while it had been under my custody. Moral responsibility also implies for Miller the possibility of appraisal or blame. Causal responsibility, in turn, might not incur any blame or legal action because things can be done without intention. For example, I could damage someone’s property while having a seizure. 3) Moreover, Miller argues that people can be responsible because of their capacities. Take into consideration doctors who may be responsible for helping the sick due to their knowledge and training. 4) Finally, our affiliation with a group might imply certain responsibilities that are owed to fellow members, such as an obligation to pay taxes because one shares a political community with them.</p> <p>However, Miller’s variations on responsibility are not sufficient for addressing responsibility gaps in human-machine interaction. His types focus mainly on concepts that we can apply to human action, but which are ignorant of whether humans act with or without machines, or within or without complex machine systems. Therefore, it makes sense to enrich the concept of responsibility with another account that is better attuned to the human-machine-action that we find in RCCV’s deployments.</p> <p>In this regard, it is worthy to consider Alex Leveringhaus’s <em>Ethics and Autonomous Weapons</em> (2016). Leveringhaus suggests three points that must be fulfilled to allocate responsibility when machines are being used. Machines’ operators are responsible for any physical output: (1) if it is the result of the operator’s intent, (2) the machine action happens to her knowledge, and (3) when it is under her control (Leveringhaus 2016, 78-79). For these points to hold, it is generally assumed that the machines are tools, extending or enhancing their operators’ capacities. Where (1)-(3) do not apply, and we observe physical output, it is reasonable to speak of a situation where neither the operator behind the machine is responsible nor the machine itself for it is but a tool. In other words, in such a situation we would be confronted with a responsibility gap.</p> <p>At the outset, it is important to note that accidents might not be responsibility gaps in the sense I am trying to discuss here.\(^4\) If an operator loses control of her machine because of unforeseeable weather conditions, and if her machine crashes into a densely populated area, she might not be responsible for this outcome, or only in a reduced manner, even though some action had occurred with devastating results. This is so because it was arguably not in her power to alter the weather, nor the crash of her machine. We would excuse her, if she had spent due care on preparing the deployment of the machine, by assuring its functionality, by looking at weather reports, by following conventional security measures etc. And, in doing so, we would reach the conclusion that no one is ascribed responsibility, at least in the sense of culpability.</p> <p>There are further scenarios where machines could produce outcome without violating Leveringhaus’s principles. For example, a machine could evade its operators control and run against her intention when it is hacked or even repurposed by a third party. (Leveringhaus 2016, 78) In such a scenario, we would not speak of any responsibility gap because the machine would follow someone else’s intention and control. For example, the Australian company DroneShield Limited develops “Counter-Unmanned Aircraft Systems”, such as its DroneGun MK4, with the aim to disrupt signals to and from third party vehicles, in the hopes that the disrupted vehicle either lands on the spot or returns to its operator by default. In short, we may have situations where machines run against their operator’s intentions and go out of control, but we can still find a way to avoid talking about responsibility gaps in these situations.</p> <p>To press the issue, we need to consider whether RCCVs systematically produce unintended, uncontrolled, physical outputs, unbeknownst and unforeseeable to their operators. In this respect, Leveringhaus is not sure whether all three conditions need to be fulfilled to speak of responsibility gaps, or whether two of them might already be sufficient. However, he thinks that the first criterion, intent, is necessary: Only if RCCVs, such as armed drones, systematically run against their operators’ intention, we could think of gaps when trying to ascribe responsibility. For example, if we have a machine that nudges its operators to always shoot at innocent people against her will.</p> <p>Are there any situations where responsibility gaps realistically occur with armed drones? Lack of intent (1) seems unlikely. If drone missiles kept hitting the wrong targets, this might be the result of a bug or malicious programming. They would be reprogrammed to fully serve the commands of their operators.\(^5\) Likewise, military drones would not simply fly in the opposite direction as commanded by the operator. If that happens, it could be a sign of interference from a third party. In any case, drones would ideally be stopped before more unintended action would follow.</p> <p>Let us examine the knowledge-condition (2). We might want to ask: Do contemporary RCCVs lead to indiscriminate killings unbeknownst to their operators? This seems to be unlikely as well. It has not been the case that drones created a surge of massacres without the knowledge of those who have steered them, as they have already been used for more than a decade. As a result, we can conclude that drones, as far as we know, do not involve any unforeseeable physical actions.</p> <p>That said, two arguments that are discussed regarding unmanned warfare seem to be pressing this issue. The first is the so-called Threshold Argument. According to this argument, military technologies, such as armed drones, might lower the threshold for states to enter war because their deployment is less costly in terms of soldiers’ lives and money (Sparrow 2012; Galliott 2015). While I cannot go into a more detailed discussion of this argument, I would like to emphasize that it is possible to say that the deployment of armed drones has consequences that are not directly anticipated by their operators. In a similar fashion, it has been argued that asymmetric warfare can create unethical responses, such as guerrilla tactics or terror, because the technologically inferior side might have no other means to retaliate (Killmister 2008). Again, one can argue that RCCV deployment produces these outcomes in the long run, which would not be easily predicted.</p> <p>One might thus conclude that RCCV deployment violates Leveringhaus’s third criterion, that is, RCCVs could systematically produce long-term effects unbeknownst to their operators. However, it is far more difficult to prove for each RCCV deployment respectively that these vehicles are the primary causal source of such effects. If a drone is flown to a terrorist outpost with the goal to eliminate it, the possible outcomes that arise seem to be calculable. Additionally, pilots could introduce sufficient risk analyses, evaluating chances of failure. They could also include rather unlikely events to estimate whether their drone deployment is justified. Eventually, the operator would know whether the attack succeeds in damaging the terrorist’s infrastructure or even in eliminating the terrorist if the drone’s telecommunication works properly. If not, she could always adapt its action. It appears that such a drone mission would not lead to any outcomes that could not be anticipated after careful consideration.</p> <p>When considering Leveringhaus’s last criterion, we observe that contemporary military drones are not sophisticated enough to undermine their operators’ control. If connection is lost and they run out of energy, they would land on the spot or crash. Some models are also equipped with the capacity to automatically return to the point from where they were started whenever telecommunication with the operator is interrupted. As with many other machines, they require the operator to be always in the loop. Hence, everything contemporary RCCVs produce is controlled by their operators, unless auto-pilots and other automated control mechanism are being used extensively. But in this paper, I want to focus on those RCCVs that are not automated yet. A discussion of automated military robots would entail many more issues concerning control, intention, and anticipation on the operator’s side.</p> <p>For the reasons given, it is difficult to claim that contemporary RCCVs systematically lead to responsibility gaps. If unintended, uncontrolled action occurs with RCCVs, unbeknownst to their operators, it would be likely due to accidents or due to insufficient foresight or control from their operators or from those who deploy them, thus making the operators or deployers responsible. Putting such exceptions aside, action by RCCVs can indeed be linked to the decisions, control, and knowledge of their operators. If that is the case, namely, if there is no responsibility gap with contemporary RCCVs, one is further justified to conclude that their operators would be responsible for whatever consequences the RCCVs might cause.</p> <h2 id="responsibility-protractions-with-remotely-controlled-combat-vehicles">Responsibility Protractions with Remotely Controlled Combat Vehicles</h2> <p>Given the number of missions and lethal strikes that have taken place with RCCVs, predominantly with armed drones, chances are high that abuses have occurred. Why has there been no conviction? One explanation could be that the military suppresses any misapplication of drones. There might have been abuses and unjust drone strikes, but the misconduct has not been addressed within the military apparatus. One recent investigation by the New York Times, How the U.S. Hid an Airstrike That Killed Dozens of Civilians in Syria, points into this direction (Philipps and Schmitt 2021). While the article focuses on aircraft, it would make sense that similar attempts of hiding and neglecting misconduct in the military has happened with armed drones. But why, on the other hand, are infantry soldiers, such as Andrew Holmes, prosecuted and convicted for war crimes? Another explanation would be that states may knowingly and willfully engage in unjust and abusive conduct, overriding international law, to exercise their power without regard for legal consequences. The justifications for dismissing litigations involving armed drones in the U.S. would suggest such a position. Yet there are states that could be said to subscribe to this stance, and which neither ratify the Rome Statute of the International Criminal Court, like the United States of America, but which still hold some of their soldiers responsible for excessive and unlawful force. Given these circumstances, I think it is possible to provide a third explanation for the lack of prosecution and conviction for unjust and abusive drone strikes.</p> <p>Another possible explanation could be that armed drones result in a phenomenon I would like to refer to as responsibility protraction, which can muddy the waters of our ethical assessments and comprehensive legal proceedings. The idea of protraction is not novel. For example, it is extremely difficult to find the right kind of addressees in large corporations when the business has been involved in criminal action (Nucci and de Sio 2016). The complex way of how responsibilities are shared within large institutions and how the minute acts of each employee that add up to what the institution eventually “does” make it difficult to pin down one person who is responsible for the outcome. This difficulty has also been called the Problem of Many Hands (Thompson 1980; van de Poel et al. 2012). Although one would assume that politicians or managers are the kind of persons in charge, examples of corporate scandals have shown that they can successfully exempt themselves from taking responsibility, by claiming that things were done unbeknownst to them. On a similar note, we can assume that the deployment of armed drones happens within a complex institution, involving many people: ground personnel fueling and repairing drones, pilots steering and targeting, officers and intelligence service agents discussing who will be killed next. If Jo Becker and Scott Shane (2012) are right, there are up to 100 individuals involved in such debates, including generals, a minister of defense, or even the president, who might sign the final verdict to kill suspects with drones. In fact, there might be so many people involved that it becomes almost impossible to evenly distribute responsibilities among all stakeholders. Arguably, this is already the case with every military mission and even more so, as more sophisticated technologies are involved. But the crucial point is to be aware that RCCVs, such as armed drones, would add a further layer to this situation.</p> <p>Due to this complexity, we may not blame a drone pilot for unjust strikes in the same sense as a combat soldier. While one can assume that eventually the pilot would be causally responsible for the strikes, which he sets off by clicking or pressing a button, as he is the last in the chain of command and hence physically involved in the action of the machine, we can say with Miller (2001) that he is not fully morally responsible for the killing to a similar degree as a soldier would be by shooting her rifle. For the drone pilot has had less leeway in applying his own interpretations to the combat. We have seen that the decision who is going to be killed entails in contemporary drone warfare the involvement of many agents, whereby the final verdict to attack a target is put out of the hands of the pilots. To put it more bluntly, pilots might be just the final cog within the military complex, executing orders without much chance for considering situations based on their own application of military standards.</p> <p>Infantry soldiers, in contrast, seem to have more leeway as to what they will do and whom they will kill, if they deem it justified. Consider Sam Mendes’s film 1917. Two British soldiers, Lance Corporal William “Will” Schofield (George MacKay) and Lance Corporal Thomas “Tom” Blake (Dean-Charles Chapman), are sent to the front to warn their troops of a German ambush. After having passed the first trench, Will and Tom reach a small, abandoned farm behind the lines. While they search for German outposts, they observe an airplane battle that ends with a German plane crashing into the barn. Tom decides to help the severely wounded pilot, holds him in his hands, and feeds him water from his own bottle. While Will continues his search, the next scene reveals how the German stabs Tom. The film presents us with how differently soldiers can treat the enemy. While Tom shows mercy, the pilot decides to take another life with his own. Finally, after Will discovers what happened, he shoots the German.</p> <p>What is also worthy keeping in mind is that Tom and Will could not only spare the German’s life, but they would also have the chance to take him prisoner, which was likely Tom’s intention when he decided to help him. Drone operators, on the other hand, do not have as many options as infantry soldiers have. All that they could do is to fully disobey their orders, by not flying and shooting with their drone. It is likely these individual circumstances that make it easier to identify infantry soldiers as the fully responsible source of outcomes, and why they are therefore more often put on trial for excessive force than RCCV pilots.</p> <p>A further explanation for the muddying of responsibility allocation is that the complexity of RCCV deployment also increases the possibilities for errors. There might be situations where errors slip into the process of deciding which suspect should be killed with RCCVs, as well as into the execution of this decision, unbeknownst to all parties involved. Whether we should understand these cases as accidents for which no one could be taken neither responsible, as with unforeseeable weather, or whether such cases hint at a systematic responsibility gap in RCCVs is open to debate. On the one hand, if such a situation were to happen, responsibility might be distributed in terms of moral responsibility because it could be argued that the drone’s operators failed to detect the error and therefore neglected their duty to oversee and care for the RCCV’s deployment. On the other hand, I am inclined to say, if RCCVs repeatedly imply errors, which remain unknown, Leveringhaus second criterion for responsibility gaps is fulfilled here. That is, RCCVs could potentially do something because of undetected errors, which were unforeseeable to their operators.</p> <p>Finally, the lack of prosecution of wrong drone strikes could also be explained by looking at the bigger picture of wars. Armed drones allow “surgical” and small-scale strikes which do not appear as grave and obvious, as, for example, erroneous and unjust bombings by aircraft. RCCVs can fly at lower altitudes than aircraft and, thus, much closer to their targets. This provides their operators with the opportunity to attack with precision and less collateral damage. In contrast, consider when the German Colonel Georg Klein mistakenly ordered the bombing of two fuel trucks in Afghanistan in 2009, and the incident led to approximately 100 deaths of innocent civilians. This resulted in an investigation by public prosecution, although charges were eventually dropped. Compared with such damage, it is possible that drone strikes have not been adequately addressed. If considered firmly, however, (legal) responsibility for wrong drone operations could and, of course, should be allocated.</p> <p>To sum up, in this section I have argued that the deployment of RCCVs is prone to responsibility protractions. This is so because they are highly sophisticated technologies whose deployment requires many hands. For this reason, killings perpetrated with RCCVs appear to be qualitatively different from those which are perpetrated with conventional rifles by infantry soldiers. The complexity of remotely controlled machines seems to deprive military personnel from an individual application of their own ethical standards. Additionally, errors can happen without individual responsibility because they could slip in undetected. While surgical effectiveness of RCCVs is arguably an advantage, this aspect might be another reason as to why unjust drone strikes have rarely been prosecuted and not let to any conviction.</p> <h2 id="conclusion-and-measures-to-address-possible-rccv-misconduct">Conclusion and Measures to Address Possible RCCV Misconduct</h2> <p>The previous analysis agrees with the established view that armed drones do not create responsibility gaps\(^6\); for there seem to be no realistic cases of unintended, uncontrolled, physical outcomes, unbeknownst to their operators, with contemporary RCCVs. However, there is evidence suggesting that RCCVs create other problems with responsibility, by protracting its allocation. If that is the case, RCCV deployments pose a problem for our ethical standards.</p> <p>Just War principles and established military conventions, such as The Convention on Certain Conventional Weapons, are meant to prevent excessive force. But if unjust harm is being done with armed drones, and no one seems to be fully responsible for it, it would be difficult to hold someone accountable. Who should it be? The pilot? The officer or the intelligence service agent who discuss suspects? The general? The head of state? This is an ethically undesirable situation because the responsible use of machines is not only achieved through good will on the operators’ side, but also by sanctions for misconduct that can only be applied if it is clear who has been responsible for the machine’s output. If, however, no one can be held responsible for RCCVs, then there is no way for sanctions and further legal action; and the deployment of RCCVs eventually invites excessive and unjust use.</p> <p>How can this situation be ameliorated? While it seems that one should as much as possible avoid technologies, such as RCCVs, that diffuse responsibility allocations, this answer will only partly suffice because RCCVs are already being deployed by many states. The chances that this will change are slim. Therefore, one amelioration would be to spell out at least as clearly as possible in what way and to what extent agents are responsible for RCCV strikes. This paper has already started with pointing out the complexity of responsibility that arises with RCCVs, but further research, especially empirical research, is needed. We need to discuss in what way military institutions prosecute misconduct, how RCCV sorties are being internally monitored, whether one pilot suffices for steering RCCVs, and whether pilots or superiors duly take responsibility for their deeds. It would require the military of many states to step up their internal monitoring.</p> <p>At the same time, I believe that considering RCCVs on their own will reach its limits. If RCCVs are seen as being part of a complex institution, a view that mainly focuses on individual responsibility is too narrow. In this regard, Luciano Floridi (2016) suggests one way of reversing our classical understanding of responsibility that deserves further attention, but upon which I can only concentrate briefly. Considering actions, we should, says Floridi, focus less on agents and their intentions and more on outcomes. The underlying idea is that if a network of human, artificial, and hybrid agents produces effects that are morally undesirable, then one way of addressing the issue could be by introducing systematic changes, especially when it is unclear who or what has led to the outcome of the network. For Floridi, this is best achieved through “soft legislation, rules and codes of conducts, nudging, incentives and disincentives” (Floridi 2016, 7). In other words, Floridi’s approach is trying to circumvent the scavenger hunt for individual responsibility by varying different systematic stimuli until the morally desired outcome for a network of agents is achieved.</p> <p>Considering RCCVs, it would be hence reasonable to complement our strategy of assigning responsibility with additional strategies to disincentivize misconduct. We therefore need to ask ourselves: What systematic measures could be useful to reduce possible unjust RCCV deployment?</p> <p>One opportunity to reduce unjust RCCV strikes is to strengthen soldiers’ and pilots’ possibilities for “conscientious refusal to fight”, as Jeff McMahan suggests (McMahan 2009, 97). This could be achieved by giving pilots a greater leeway as to how they also apply their own moral standards, for example by being involved in the process of deciding which target is being hit. Because RCCV missions can often be prepared, questioning operators whether killing such prospective targets is also consistent with their moral views does not seem to conflict with other practical requirements that must be met for a successful mission. In this way, operators, especially the pilots, would also have more grounds to object if they thought the killing of people was morally wrong. Ultimately, we would want to strive for a situation where the operators are as morally responsible for their action with RCCVs as infantry fighters are in combats, for whom it appears easier for us to assign responsibility. If this is achieved, RCCV operators could take a greater share of responsibility, as when they are only cogs in the military machinery.</p> <p>Another problem with contemporary RCCV missions is that RCCVs are often deployed within the context of terrorist hunts that are not in any way part of a justified war. This then points to an issue with RCCVs which we need to address, namely, the fact that they invite military action outside of our classical understanding of just and unjust wars, for example, by breaching sovereignty of states where terrorists hide. Therefore, we may try to directly re-enforce the principle of state sovereignty that has been weakened with RCCV deployments. We should require clear agreements between the deploying countries and those countries where RCCV missions take place.</p> <p>To undermine quick and dirty missions, we could further demand that RCCV pilots must be stationed in the country where they deploy their systems, provided that there is no official declaration of war against the country where they carry out their missions. For it is one thing, to quickly fly through a foreign and often militarily inferior country, but another to station active military personnel abroad. Physically locating pilots in a foreign country where they undertake military strikes would also necessitate that such a country allows for their presence, helping to reinforce state sovereignty. Additionally, this requirement would mean that disagreeing countries could more easily prevent deployments by, for example, expelling RCCV pilots from their territory. In the traditional scenario of defending against a belligerent state, it appears neither feasible nor ethically imperative to deploy RCCV personnel abroad, as missions are likely to be accompanied by additional military action against the adversary and can be justified to effectively halt their attacks. For instance, a nation defending itself may employ drone strikes to impede the adversary’s military infrastructure.</p> <p>Such an approach to promoting a specific outcome may prove effective if it were to become a mandatory requirement that is accepted by all nations in the spirit of war conventions. If the international community agrees that RCCVs lead to ethically undesirable situations, they might be willing to cede certain RCCV missions in the hopes of ameliorating the overall legal and moral standards with which they can be deployed. At the same time, it would allow RCCVs being deployed for defensive purposes. Having states agree on the use of certain weapons in this sense does not seem to be impossible if we think of The Convention on Certain Conventional Weapons that prohibits or restricts the use of certain weapons, such as booby traps or incendiary weapons. Even in the absence of a formal agreement, states may wish to station their pilots in the areas where they deploy RCCVs to ensure a more responsible approach to these systems.</p> <p>In sum, I am suggesting several measures to address possible RCCV misconduct: (a) the military should deploy RCCVs as little as possible to avoid responsibility protractions, (b) if RCCVs are being deployed, the military must assure that for each RCCV mission responsibility can be allocated. To heighten the standards with which RCCVs are being deployed, (c) conscientious engagement among RCCV operators should be promoted, allowing them a greater leeway to apply their own moral interpretations. Finally (d), states should station pilots in countries where their RCCV mission takes place whenever there is no official declaration of war against these respective countries.</p> <h2 id="footnotes">Footnotes</h2> <p>\(^1\) The paper is an ameliorated and extended version of Chapter 3.3. from my master’s thesis Drone Ethics: Duties and Responsibilities for Unmanned Aerial Combat Vehicles from 2020, which is accessible at the library of the University of Vienna, Austria.</p> <p>\(^2\) One notable exception is Jail Galliott’s nineth chapter on responsibility gaps in his Military Robots. Mapping the Moral Landscape (2015).</p> <p>\(^3\) Al-Aulaqi v. Obama, in which these privileges played a role, was ultimately dismissed on different grounds (U.S. Congressional Research Service 2022, 27)</p> <p>\(^4\) Thomas Simpsons and Vincent Müller (2016) as well as Sebastian Köhler (2020, 3137) also defend this view.</p> <p>\(^5\) The rise of amateur drone deployments (as was the case in Ukraine against the Russian military in 2022) increases the chance of bugs and flawed designs. But do amateur models lead to systematic responsibility gaps? To address this question, further information concerning actual deployments is required that is not yet available as of 2023. This paper focuses on RCCVs that have been produced by licensed companies, undergone professional testing, and which are maintained by professional mechanics.</p> <p>\(^6\) That is, there are no responsibility gaps when we analyze armed drones based on Alex Leveringhaus’s criteria (2016). Ibo van de Poel et al. (2012) speak of the Problem with Many Hands also in terms of responsibility “gaps”.</p> <h2 id="bibliography">Bibliography</h2> <p>Becker, Joe and Scott Shane. 2012. Secret ‘Kill List’ Proves a Test of Obama’s Principles and Will. The New York Times. https://www.nytimes.com/2012/05/29/world/ obamas-leadership-in-war-on-al-qaeda.html (accessed May 6, 2023).</p> <p>Coeckelbergh, Mark. 2020. AI Ethics. Cambridge, MA: MIT Press.</p> <p>Danaher, John. 2016. Robots, law and the retribution gap. Ethics and Information Technology, 18(4): 299–309.</p> <p>Fallon, Amy. 2011. US soldier jailed for seven years over murders of Afghan civilians. The Guardian. https://www.theguardian.com/world/2011/sep/24/us-soldier-jailed-afghan-civilians (accessed June 17, 2023)</p> <p>Floridi, Luciano. 2016. Faultless responsibility: on the nature and allocation of moral responsibility for distributed moral actions. Philosophical Transactions of the Royal Society, vol. 374 (A): 1-13.</p> <p>Galliott, Jai. 2015. Military Robots. Mapping the Moral Landscape. London: Routledge.</p> <p>Kaufman, Brett Max, and Anna Diakun. 2017. United States Targeted Killing Litigation Report. In Litigating Drone Strikes: Challenging the Global Network of Remote Killing, eds. Andreas Schüller and Wolfgang Kaleck. Berlin: European Center for Constitutional and Human Rights (ECCHR): 118-133.</p> <p>Killmister, Suzy. 2008. Remote Weaponry: The Ethical Implications. Journal of Applied Philosophy, vol. 25, no. 2: 121-133.</p> <p>Köhler, Sebastian. 2020. Instrumental Robots. Science and Engineering Ethics, (2020) 26: 3121–3141.</p> <p>Lazar, Seth. 2016. War. The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta. https://plato.stanford.edu/entries/war/ (accessed May 6, 2023).</p> <p>Leveringhaus, Alex. 2016. Ethics and Autonomous Weapons. London: Palgrave Macmillan.</p> <p>Lokhorst, Get-Jan and Jeroen van den Hoven. 2012. Responsibility for Military Robots. In Robot Ethics. The Ethical and Social Implications of Robotics, eds. Patrick Lin, Keith Abney,and George Bekey. Cambridge, MA: MIT Press: 145-156.</p> <p>Matthias, Andreas. 2004. The responsibility gap: Ascribing responsibility for the actions of learning automata. Ethics and Information Technologies, vol. 6: 175-183.</p> <p>McMahan, Jeff. 2009. Killing in War. Oxford: Oxford University Press.</p> <p>Miller, David. 2001. Distributing Responsibilities. The Journal of Political Philosophy, vol. 9, no. 4: 453-471.</p> <p>Müller, Vincent C. 2016. Autonomous Killer Robots are Probably Good News. In Drones and Responsibility. Legal, Philosophical and Socio-Technical Perspectives on Remotely Controlled Weapons, eds. Ezio Di Nucci and Filippo Santoni de Sio. London: Routledge: 67-81.</p> <p>Nucci, Ezio Di and Filippo Santoni de Sio. 2016. Drones and Responsibility. Mapping the Field. In Drones and Responsibility. Legal, Philosophical and Socio-Technical Perspectives on Remotely Controlled Weapons, eds. Ezio Di Nucci and Filippo Santoni de Sio. London: Routledge: 1-14.</p> <p>Oimann, Ann-Katrien. 2023. The Responsibility Gap and LAWS: a Critical Mapping of the Debate. Philosophy &amp; Technology (2023): 36:3.</p> <p>The Organization for Security and Co-operation in Europe. 1994. Code of Conduct on Politico-Military Aspects of Security. https://www.osce.org/files/f/documents/5/7/ 41355.pdf (accessed May 6, 2023).</p> <p>Phillips, Dave, and Eric Schmitt. 2021. How the U.S. Hid an Airstrike That Killed Dozens of Civilians in Syria. The New York Times. https://www.nytimes.com/2021/11/13/us/us airstrikes-civilian-deaths.html (accessed May 6, 2023).</p> <p>Simpson, Thomas W., and Vincent C. Müller. 2016. Just War and Robot’s Killings. The Philosophical Quarterly, vol. 66, No. 263.</p> <p>Sparrow, Robert. 2007. Killer Robots. Journal of Applied Philosophy, vol. 24, no. 1: 62-77.</p> <p>Sparrow, Robert. 2012. ”Just Say No” to Drones. IEEE Technology and Society Magazine, Spring: 56-63.</p> <p>Thompson, Dennis F. 1980. Moral Responsibility of Public Officials: The Problem of Many Hands. The American Political Science Review, vol. 74, no. 4 (Dec.): 905-916.</p> <p>U.S. Congressional Research Service. 2022. The State Secrets Privilege: National Security Information in Civil Litigation (R47081, April 28, 2022) by Jennifer K. Elsea and Edward C. Liu. ProQuest® Congressional Research Digital Collection (accessed June 17, 2023).</p> <p>van de Poel, Ibo, Jessica Nihlén Fahlquist, Neelke Doorn, Sjoerd Zwart, and Lambèr Royakkers. 2012. The Problem of Many Hands: Climate Change as an Example. Science and Engineering Ethics, vol. 18: 49-67.</p>]]></content><author><name></name></author><category term="drone"/><category term="autonomous"/><category term="combat"/><category term="vehicles"/><category term="ethics"/><category term="responsibility"/><category term="protraction"/><category term="gap"/><category term="just"/><category term="war"/><category term="theory"/><summary type="html"><![CDATA[This paper examines drone warfare from an ethical perspective]]></summary></entry><entry><title type="html">Using LLMs in Syntactic Research</title><link href="https://omseeth.github.io/blog/2024/llms_syntactic_research/" rel="alternate" type="text/html" title="Using LLMs in Syntactic Research"/><published>2024-10-17T10:05:00+00:00</published><updated>2024-10-17T10:05:00+00:00</updated><id>https://omseeth.github.io/blog/2024/llms_syntactic_research</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/llms_syntactic_research/"><![CDATA[<p><strong>Abstract</strong> Large language models (LLMs) have revolutionized natural language processing. This study investigates their potential for replicating and contributing to syntactic research. I leverage GPT-3.5 Turbo to reproduce Experiment 2 from Salzmann et al. (2022), which explored Principle C (coreference restrictions) in German. The LLM successfully replicated human behavior in confirming Principle C’s influence. However, unlike human participants, GPT-3.5 Turbo did not exhibit sensitivity to syntactic movement, but to argument and adjunct positions, suggesting a potential difference in underlying processing mechanisms. These findings highlight the potential of LLMs for probing syntactic principles as well as linguistic phenomena more generally but also raise questions about their ability to mirror human language processing. I discuss the methodological implications of using LLMs for linguistic inquiry and the potential for uncovering insights into their inner workings.</p> <h2 id="1-introduction">1 Introduction</h2> <p>The advent of large language models (LLMs) through the Transformer architecture (Vaswani et al., 2017) has led to models with unprecedented capabilities. This contribution attempts to harness these capabilities to reproduce an empirical experiment from Salzmann et al. (2022) with one Transformer based model, GPT 3.5 Turbo developed by OpenAI.</p> <p>The background to the experiment is an investigation of Principle C (also called Condition C). According to syntactic theory, Principle C governs the possibilities of coreference between pronouns and R-expressions (i.e., proper names and definite determiner phrases). However, how the principle works remains a matter of debate (Adger et al., 2017; Bruening and Khalaf, 2018; Salzmann et al., 2022; Stockwell et al., 2021). For that matter, Salzmann et al. (2022) developed several experiments to explore the principle in German contexts. This paper focuses on their second experiment, Experiment 2, where the authors attempted with a 2 x 2 x 2 experimental design to investigate whether participants reconstructed the syntactic hierarchy of given sample sentences when asked if different interpretations were possible (Salzmann et al., 2022, 601).</p> <p>To reproduce Experiment 2, GPT 3.5 Turbo was queried with the exact same items that were used in the original work by Salzmann et al. (2022). I employed diverse prompt types, including zero-shot and one-shot, and varied sizes of iterations. In order to match the identical quantity of data points collected from 32 participants in Salzmann et al. (2022), I finally chose the outcomes of a configuration of four iterations with a zero-shot prompt to query GPT 3.5 Turbo for fitting a generalized linear mixed model. With this model, I tested the same three hypotheses from Experiment 2 in Salzmann et al. (2022).</p> <p>I report the following results: In line with human participants, the LLM qualified sentences with referring expressions in such a fashion that the influence of Principle C for respective cases with R-expressions and pronouns is attested. In contrast to the human proportion of answers, GPT 3.5 Turbo’s output did not indicate a significant influence of movement when interpreting the test item sentences. If movement is understood as a variable for reconstruction, it appears that the LLM does not reconstruct the pre-moved positions of the phrases. Finally, a significant influence of position of R-expressions in either arguments or adjuncts can be shown with GPT 3.5 Turbo. Overall, GPT 3.5 Turbo’s judgments of sentences differ from those of humans in more ambiguous cases. However, whether movement and reconstruction play a role in human interpretations is neither fully clear from Experiment 2, as Salzmann et al. (2022) conclude in their discussion of the original experiment.</p> <p>The novel possibility to reproduce an empirical experiment in linguistics with LLMs also raises many methodological questions. Are LLMs like GPT 3.5 Turbo computational accounts of how humans process language (Blank, 2023)? Can they be deployed to test hypotheses in syntax as well as other linguistic fields? Even if these questions remain open to debate, querying LLMs with test items from linguistics would also provide us with valuable insight into the ”metacognition” of these models (Begus et al., 2023). Knowing how the models respond can help engineers with improving their performance.</p> <p>In <strong>Section 2</strong>, I will introduce Principle C and the syntactic considerations that led to Experiment 2 in Salzmann et al. (2022). The reports of the original experiment will be reported in <strong>Section 3</strong>. I will discuss in <strong>Section 4</strong> the prompts and details used to query GPT 3.5 Turbo with the items from Salzmann et al. (2022). I will summarize the LLM’s results in <strong>Section 4.2</strong>. Finally, I will provide a short discussion of possible implications of using LLMs in linguistic research in <strong>Section 5</strong>.</p> <h2 id="2-syntactic-principle-for-referring-expressions">2 Syntactic principle for referring expressions</h2> <p>Several foundational assumptions hold in generative syntax. Generally, its goal is to analyze language by assuming that rules generate well-formed patterns. Some of these rules, also called principles, are considered to be universal, others only apply to specific languages. It is further assumed that each language is structured hierarchically where the constituents of a syntactically formed phrase can be moved within its hierarchy to accommodate the large variety of linguistic possibilities. When a constituent is being moved, analysis states that it is generally assumed that it leaves a trace behind. However, many movements are also restricted, for example by the depth of embedding of each constituent within the hierarchy.</p> <p>Among the syntactic principles, generative syntax has identified three types that restrict in particular co-reference of noun phrases (R-expressions, pronouns, anaphors). These types are called Principle A, B, and C. Principle C will be of interest to us in this contribution.</p> <p>Principle C states that a pronoun cannot refer to an R-expression that it c-commands. R-expressions are proper names or definite determiner phrases, such as ”Mary” or ”the dog”. C-command is a syntactic constraint where a constituent within a phrase’s hierarchy would occupy a structurally higher position than another one from which syntactic restrictions as in Principle C follow. For example:</p> <blockquote> <p>(1) \(^*\)He\(_{i}\) thinks that <strong>Peter</strong>\(_{i}\) is the happiest.</p> </blockquote> <p>is ungrammatical according to Principle C when it is assumed that the pronoun should refer to the noun (see indices). It occupies a structurally higher position than the R-expression Peter. For the same reasons, a sentence like</p> <blockquote> <p>(2) \(^*\)[Which of <strong>Lara</strong>\(_{i}\)’s sweaters]\(_{1}\) do you think she\(_{i}\) likes __\(_{1}\)?</p> </blockquote> <p>is considered ungrammatical, even if the R-expression Lara appears on the surface before the pronoun. This can be explained with reconstructing to the trace of the movement of the wh-phrase (consider index 1 and square brackets) which occupies a lower position than the pronoun before its movement. This is also called an A’-movement since the movement does not affect the type of phrase.</p> <p>In their paper, ”Condition C in German A’-movement: Tackling challenges in experimental research on reconstruction”, Salzmann et al. (2022) analyze Principle C (also called Condition C) within particular contexts. Their research follows a shift in syntax where principles that were suggested by syntactitians based on introspective judgements (sometimes described as armchair syntax) are checked through experimental setups. If principles can be confirmed in a variety of linguistic examples by a sufficient size of native speakers, the empirical evidence provides an additional justification for them. If a pattern is equivocally observed, it might not apply in a principled sense.</p> <p>The aim of Salzmann et al. (2022) is to examine the workings of Principle C within three contexts. I will explain them briefly. <em>Context 1)</em> When the R-expression is part of an adjunct, as opposed to when it is part of an argument, the established view states that there is no Principle C reconstruction (Lebeaux, 1991; Salzmann et al., 2022). <em>Context 2)</em> Principle C is always violated whenever the R-expression is contained within a predicate. The containment of an R-expression within an argument does not always lead to a violation, contrary to the assumption in Context 1 (Huang, 1993; Salzmann et al., 2022). <em>Context 3)</em> Condition C is less strict with relative clauses than with wh-movement (Citko, 1993; Salzmann et al., 2022).</p> <p>Salzmann et al. (2022) conducted three experiments in German to examine the previously mentioned syntactic phenomena. This paper focuses on their second experiment (Salzmann et al., 2022, pp. 601-609, the experiment was also registered at <a href="https://osf.io/mjgpz]">https://osf.io/mjgpz</a>).</p> <p>In Experiment 2, Salzmann et al. (2022) investigate coreference between pronoun and R-expression in embedded clauses. The clauses have either moved or in situ wh-phrases, serving either as objects or as subjects. Additionally, it is investigated whether the position of an R-expression as part of an argument or and adjunct makes a difference. The following two examples from Salzmann et al. (2022, 603) illustrate the conditions in Experiment 2:</p> <blockquote> <p>(3)… [welche Geschichte im Buch über <strong>Hanna</strong>] <em>sie</em> __ ärgerlich fand.</p> <blockquote> <p>’which story in the book about Hanna she found upsetting.’</p> </blockquote> </blockquote> <blockquote> <p>(4)… [welche Geschichte über <strong>Hanna</strong>] __ <em>sie</em> verärgert hat.</p> <blockquote> <p>’which story about Hanna upset her.’</p> </blockquote> </blockquote> <p>Note in (3) that the object is moved and the R-expression Hanna serves as an adjunct to ”im Buch”. If Principle C were to have its assumed force in all instances, one would assume for (3) that the pronoun cannot refer to the R-expression. (Though the established view has noticed that the adjunct position indicates a less strict application of Principle C.) In (4), the wh-phrase serves as the subject which is moved and the R-expression is part of an argument. One would assume in contrast to the previous example that the pronoun can refer to the R-Expression. Also consider the for the depth of embedding and hence the structural hierarchy in both examples for the assumptions.</p> <p>The aim of Experiment 2 in (Salzmann et al., 2022) is to create a statistical baseline for syntactically well-formed clauses (no violation of Principle C) through positive responses as well as a percentage of responses for those examples where Principle C would be violated if movement is assumed. If no violation in terms of negative responses is reported for ungrammatical examples, Salzmann et al. (2022) consider surface factors as another influence for the acceptability of coreference between R-expression and pronoun in such cases.</p> <p>Salzmann et al. (2022, 603-604) test three hypotheses in their experiment to consider the effect of Principle C (also called Condition C) in the previously mentioned variety of contexts:</p> <ul> <li><strong>H1</strong> Condition C hypothesis: R-expressions cannot be coreferential with a c-commanding expression.</li> <li><strong>H2(a)</strong> Reconstruction hypothesis: the base position of moved phrases matters for Condition C.</li> <li><strong>H2(b)</strong> Surface hypothesis: the surface position of moved phrases matters for Condition C.</li> <li><strong>H3</strong> Argument/adjunct asymmetry hypothesis: in contrast to argument po- sitions, there is no reconstruction if the R-expression is part of an adjunct.</li> </ul> <p>H1 assumes that phrases with an in situ subject receive more positive responses for the subject than for the object, as coreference with the object would clearly violate Principle C. Consider the following example:</p> <blockquote> <p>(5) \(^*\)Jim erklärt, dass <em>er</em> die neue Geschichte über <strong>Mert</strong> ärgerlich fand.</p> <blockquote> <p>’Jim explains that he found the new story about Mert upsetting.’</p> </blockquote> </blockquote> <p>That the pronoun ”er” refers to <strong>Mert</strong> contradicts our syntactic intuition.</p> <p>If H1 is confirmed, the authors investigate with H2 if more positive responses for a moved subject can be observed than with a moved object, that is, whether Principle C also holds with movement (H2(a)). If there is a significant difference between H2(a) and H1, then it is assumed that H2(b) would be confirmed, that is, surface positions influence the force of Principle C in case that the surface effects differ for objects and subjects. Finally, H3 investigates the difference of responses with instances where the R-expression is positioned as an adjunct compared to those within argument structures.</p> <h2 id="3-experimental-results-from-32-participants">3 Experimental results from 32 participants</h2> <p>In this section, I will briefly describe the experimental setting of Experiment 2 from (Salzmann et al., 2022) and report their results.</p> <h3 id="31-experimental-setting">3.1 Experimental setting</h3> <p>To test their hypotheses, Salzmann et al. (2022) developed a 2 x 2 x 2 design where the first factor was Phrase (containment of R-expression either within the subject or the object), the second Position (R-expression either as argument or adjunct) and the third Movement (in situ or moved). 32 participants were recruited via www.prolific.co. They were given 78 stimuli which comprised 32 critical items, 44 fillers, and 2 additional items for exploration.</p> <p>The general set-up of each experimental round consisted in a shown example sentence with two R-expressions and a pronoun. The sentence was followed by two binary questions. Before the start of the experiment, the participants received an instruction with an example that deliberately allowed two interpretations for resolving the references of the example sentence. The participants were told to answer ”Yes” and ”Yes” in such cases. Generally, a sentence such as (5) was accompanied with two questions of the following type: Q1) ”Can this sentence be interpreted that Jim found a story upsetting?” (asking about the subject from the matrix clause ’Jim explains that…’) and Q2) ”Can this sentence be interpreted that Mert found a story upsetting?” (asking about the subject and/or object, depending on the respective condition, from the embedded clause). To avoid the order of Q1 and Q2 as a confound, the questions were randomly switched. Salzmann et al. (2022) collected the respective ”Yes” and ”No” answers for all stimuli. On average, the authors report that the experiments took 24 minutes until completion.</p> <h3 id="32-results-from-32-participants">3.2 Results from 32 participants</h3> <p>For their 32 participants, Salzmann et al. (2022) report the results for all conditions as shown in Table 1.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_1-480.webp 480w,/assets/img/using_llms/table_1-800.webp 800w,/assets/img/using_llms/table_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_1.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 1:</strong> Results of Experiment 2 with proportion of positive answers</p> <p>Noticeably, the interpretations of questions of type Q1 with respect to the refer- ring R-expression were accepted in nearly all stimuli. This comes as no surprise as its intention was to establish a baseline for syntactically well-formed patterns where the subject from the matrix clause occupies the structurally highest position. Given that some noise is considerably part of every empirical evaluation, 100% positive answers for the well-formed examples would be more concerning. A greater difference is observable for those readings that were elicited with Q2. Q2 received a proportion of more than 50% positive answers where the R-expression was part of a subject phrase. To a lesser degree, Q2 was accepted when it was contained within an object phrase, almost never when the R-expression stayed in situ (consider example (5)).</p> <p>With their findings, the authors fit two generalized linear mixed models, following the recommendations of Bates et al. (2015), where the items are considered as random effects. For both models the baseline of <em>Phrase</em> was ’object’ and for <em>Position</em> ’argument’. To test all hypotheses two contrast encodings were necessary so that <em>Movement</em> received ’in situ’ as its baseline in <strong>Model 1</strong> and ’moved’ in <strong>Model 2</strong>. I report their results in Table 2 and 3 where <em>Position</em> was originally called ”arg/adj”.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_2-480.webp 480w,/assets/img/using_llms/table_2-800.webp 800w,/assets/img/using_llms/table_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_2.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 2:</strong> Generalized linear mixed model results from Experiment 2 (<strong>Model 1</strong>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_3-480.webp 480w,/assets/img/using_llms/table_3-800.webp 800w,/assets/img/using_llms/table_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_3.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 3:</strong> Generalized linear mixed model results from Experiment 2 (<strong>Model 2</strong>)</p> <p>Salzmann et al. (2022, 606) use <strong>Model 1</strong> and <strong>2</strong> to validate or reject their hypotheses. They report an effect of Phrase for the levels ’in situ’ and ’argument’ with respect to the other factors: z = 8.226, p &lt; 0.001 in <strong>Model 1</strong>, so H1 is being confirmed. The reasoning behind this is that for sentences with subject, in situ, argument, such as in <strong>Condition e</strong>:</p> <blockquote> <p>(6) Jim erklärt, dass die neue Geschichte über <strong>Mert</strong> <em>ihn</em> verärgert.</p> <blockquote> <p>’Jim explains that the new story about Mert upsets him.’</p> </blockquote> </blockquote> <p>there is no c-command for the pronoun over the R-expression in the embedded clause, compared with sentences with object, in situ, argument <strong>Condition a</strong>, as in:</p> <blockquote> <p>(7) Jim erklärt, dass <em>e</em>r* die neue Geschichte über <strong>Mert</strong> ärgerlich fand.</p> <blockquote> <p>’Jim explains that he found the new story about Mert upsetting.’</p> </blockquote> </blockquote> <p>An effect of Phrase was also noticed within the levels of ’moved’ and ’argument’ with respect to the other factors: z = 2.391, p = 0.017 in <strong>Model 2</strong>. This confirms H2(a) of their hypotheses: that the base position of moved phrases matters for Principle C. Compare <strong>Condition c</strong> (not the principle):</p> <blockquote> <p>(8) Lisa erzählt, [welche Geschichte über <strong>Hanna</strong>] <em>sie</em> __ ärgerlich fand.</p> <blockquote> <p>’Lisa tells which story about Hanna she found upsetting.’</p> </blockquote> </blockquote> <p>with <strong>Condition g</strong>:</p> <blockquote> <p>(9) Lisa erzählt, [welche Geschichte über <strong>Hanna</strong>] __ <em>sie</em> verärgert hat.</p> <blockquote> <p>’Lisa tells which story about Hanna upset her.’</p> </blockquote> </blockquote> <p>Salzmann et al. (2022) also explain in their study pre-registration: ”If there is reconstruction for Principle C, we should see the same effect in the conditions with wh-movement as in the conditions without wh-movement, irrespective of the differences in surface structure.”</p> <p>The authors further confirm hypothesis H2(b) (the relevance of surface positions) because an interaction between <em>Movement</em> and <em>Phrase</em> is found in the level ’argument’ of the other factor: |z| = 5.596, p &lt; 0.001 in both models. The reasoning is that Principle C would be reduced in contexts with movement compared to those without because on the surface the moved phrase would not look like a violation of the principle.</p> <p>Finally, hypothesis H3 (argument/adjunct asymmetry) is not confirmed as there is overall no effect of <em>Position</em> and neither an interaction between <em>Phrase</em> and <em>Position</em> (arg/adj) within the level of ’moved’ (z = 0.087,p = 0.931 in <strong>Model 2</strong>) nor between <em>Movement</em> and <em>Position</em> (arg/adj) within ’object’ (|z| = 0.209, p = 0.834 in <strong>Model 1</strong> and <strong>2</strong>). Salzmann et al. (2022) argue that H3 can also only hold if H2(a) is borne out.</p> <h2 id="4-experimental-results-from-llm">4 Experimental results from LLM</h2> <p>In section 4, I will discuss the model and prompts used in the experiment and report its results.</p> <h3 id="41-experimental-setting">4.1 Experimental setting</h3> <p>The experiment was executed with the updated GPT 3.5 Turbo (gpt-3.5-turbo- 0125) large language model from <a href="www.openai.com">www.openai.com</a>. The model’s training data dates up to September 2021. For the experiment, GPT 3.5 Turbo is queried twice with an adjusted prompt for each test sentence with Q1 in the first round and Q2 in the second. This process was repeated for two and four iterations, leading to an overall data with 512 and 1024 data points for each type of questions (Q1 or Q2). Four iterations led to the same amount of data points as in Experiment 2 from Salzmann et al. (2022). The temperature of the model, which renders its answers more deterministic, was set to 0. The role – an option by OpenAI to further customize the prompts – used for the queries was the default ”user”.</p> <p>Prompt P1 was chosen for the task after some manual experimentation. Since Experiment 2 from (Salzmann et al., 2022) was conducted in German, German was also used for the model instructions in the prompt (Consider the Appendix for the original prompts):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P1-480.webp 480w,/assets/img/using_llms/P1-800.webp 800w,/assets/img/using_llms/P1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P1.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The variables ’content’ and ’question’ in the prompt referred to the different conditioned sentences that belonged to the experimental items from Salzmann et al. (2022).</p> <p>Since the participants in Experiment 2 received instructions, a second prompt P2 with a similar instruction for the large language model was used.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P2-480.webp 480w,/assets/img/using_llms/P2-800.webp 800w,/assets/img/using_llms/P2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P2.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>P2 was an instance of what is commonly known as few shot learning because GPT 3.5 Turbo received one example with an ideal solution.</p> <h3 id="42-results-from-gpt-35-turbo">4.2 Results from GPT 3.5 Turbo</h3> <p>The closest overall results compared to the baseline of human answers can be derived from a zero shot configuration of GPT 3.5 Turbo (Table 4). In the configuration with four iterations and a zero shot prompt, Q1 received a great share of positive answers in all conditions. Q2 was almost never positively answered with phrases that contained the R-expression as an object (<strong>Condition a-d</strong>). The adjunct position led to some greater variability though (21.1% and 35.9% positive answers). For Q2, GPT 3.5 Turbo provided mostly more positive answers than humans when queried with subject phrases, with one exception (<strong>Condition e</strong>). For subjects, GPT 3.5 Turbo’s proportions of answers differ to those from humans up to 9,4%. The four times iterated one shot configuration led to a lot of positive answers for both Q1 and Q2 except for those cases where the R-expression was contained in an argument within an object phrase (<strong>Condition a</strong> and <strong>b</strong>). Again, <strong>Condition e</strong> was slightly less positively answered than the other subject conditions. The configurations with only two iterations followed the trend of both previously mentioned configurations. The configuration with a zero shot prompt and two iterations led to results which are even closer to the human baseline than the same prompt four times iterated. The greatest difference to the human baseline for the zero shot configurations can be noticed in <strong>Condition b</strong> with a difference of 35.1% and 34.3%.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_4-480.webp 480w,/assets/img/using_llms/table_4-800.webp 800w,/assets/img/using_llms/table_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_4.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 4:</strong> Proportion of GPT 3.5 Turbo’s positive answers, using zero and one shot prompts with 4 and 2 iterations of all items from Experiment 2 (the conditions are the same as those in Table 1)</p> <p>Following Salzmann et al. (2022), I fit a generalized linear mixed model with the data from the zero shot configuration after four iterations. I chose this configuration because its amount of data points resembles the amount from Experiment 2 in Salzmann et al. (2022). The items were again considered as random effects. As with the models from Salzmann et al. (2022), the baseline of <em>Phrase</em> was ’object’ and for <em>Position</em> ’argument’. Similarly, two baselines for <em>Movement</em> were used to account for the differences between ’in situ’ (<strong>Model 3</strong>) and ’moved’ (<strong>Model 4</strong>). I report the results in Table 5 and 6.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_5-480.webp 480w,/assets/img/using_llms/table_5-800.webp 800w,/assets/img/using_llms/table_5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_5.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 5:</strong> Generalized linear mixed model results from the zero shot configuration with four iterations (<strong>Model 3</strong>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/table_6-480.webp 480w,/assets/img/using_llms/table_6-800.webp 800w,/assets/img/using_llms/table_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/table_6.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table 6:</strong> Generalized linear mixed model results from the zero shot configuration with four iterations (<strong>Model 4</strong>)</p> <p>With <strong>Model 3</strong>, an effect of <em>Phrase</em> can be observed for the levels ’in situ’ and ’argument’ from <em>Movement</em> and <em>Position</em> with z = 5.844, p &lt; 0.001 confirming H1 from Experiment 2. The same effect is observed with <strong>Model 4</strong> where <em>Phrase</em> leads to z = 6.075, p &lt; 0.001. If the interpretation of the LLM results were to follow Salzmann et al. (2022), the effect of <em>Phrase</em> in either generalized linear mixed model with ’in situ’ as well as ’moved’ as baseline for Movement would imply the acceptance of H2(a). However, no effect of <em>Movement</em> nor an interaction of <em>Movement</em> in the other levels is observed in <strong>Model 3</strong> and <strong>4</strong>, rejecting H2(b). As H2(a) appears to be dependent on <em>Movement</em>, it is questionable whether H2(a) can be accepted when no noticeable effect of Movement is given. Salzmann et al. (2022) predicted an effect of Position if H2(a) is rejected. Precisely, this effect is observed in <strong>Model 3/4</strong> with z = 3.705,p = 0.001 and z = 4.697,p &lt; 0.001, that is, where the appearance of the R-expression in either an argument or an adjunct position makes a difference. Position also interacts with Phrase in both models.</p> <h2 id="5-discussion-of-llms-in-linguistic-research">5 Discussion of LLMs in linguistic research</h2> <p>The implications of the results as well as the methodical approach presented in this contribution appear to be so extensive that I will only provide a few thoughts in this section.</p> <h3 id="51-interpretation-of-results">5.1 Interpretation of results</h3> <p>To begin with, the results from the experiment with GPT 3.5 Turbo as well as with human participants clearly indicate the presence of Principle C (hypothesis H1). Generally, questions of type Q2 were answered in such a fashion that the pronoun in the given example was only referring to the corresponding R-expression from the main clause whenever the second R-expression was part of an embedded object. An increase of positive proportions for Q2 is noticed when the second R-expression is part of an embedded subject phrase.</p> <p>The disagreement between the LLM and human responses with respect to hypothesis H2(a&amp;b) aggravates the question of whether reconstruction is part of processing sentences with Principle C. While the results from 32 participants indicate an effect of movement which Salzmann et al. (2022) interpret as possible evidence for reconstruction, that is, the application of Principle C in Syntax under the assumption that the pre-moved position is the key to its force, the large language model responses do not indicate such an effect. This raises multiple questions: Does a language model operate differently when applying Principle C than human beings? Is movement the right variable for investigating reconstruction? Do human beings really reconstruct? Salzmann et al. (2022, 607) also question their results from Experiment 2: ”How can the finding that the base position plays a role (pointing toward reconstruction) be reconciled with the finding that the surface position also matters (speaking against reconstruction)?” In other words, an effect of movement is attested, but it is difficult to reconcile with the fact that the surface position of the moved phrase also influences the participants’ interpretations. Given that the proportions of answers from their experiment does not lead to clear numbers, and that the authors attempt to avoid a logic of arbitrary thresholds (e.g., share of proportions below 20% should be considered as noise), they reject strong reconstruction with Principle C as the governing factor of coreference in German A ́-movements. The LLM’s results appear to confirm this conclusion because the LLM’s responses were not considerably influenced by movement.</p> <p>The effect of position (hypothesis H3) is confirmed with the LLM. The results from 32 participants indicated that the adjunct and argument position were not treated differently and for that matter H3 was rejected in Salzmann et al. (2022). However, the LLM corroborates the established view (Lebeaux, 1991) that there is an asymmetry between the argument and adjunct position as <em>Position</em> led to a significant effect in the generalized linear mixed model that was fit with the LLM’s responses.</p> <h3 id="52-the-use-of-llms-in-linguistic-research">5.2 The use of LLMs in linguistic research</h3> <p>The idea of using models to simulate responses in linguistic experiments has been previously explored with smaller language models based on recurrent neural networks (RNNs), as seen in Linzen et al. (2016) and Futrell et al. (2019). This work culminated in the development of <a href="https://syntaxgym.org/">SyntaxGym</a>, a platform led by Jennifer Hu, Jon Gauthier, Ethan Wilcox, Peng Qian, and Roger Levy at the MIT Computational Psycholinguistics Laboratory. SyntaxGym’s objective is to evaluate how well deep neural network models align with human intuitions about grammatical phenomena. Transformer-based models were later integrated into the platform. Additionally, research comparing neural language models with human responses in psycholinguistic studies is presented in Wilcox et al. (2021), while Li et al. (2024) examine the alignment between humans and large language models in processing garden-path sentences. Haider (2023) offers a qualitative perspective on potential applications of large language models in linguistic research. Nevertheless, the scientific validity of using language models in this context remains a topic of debate.</p> <p>At the outset, we might wonder: ”Do large language models (LLMs) constitute a computational account of how humans process language?” (Blank, 2023, 987) For Blank, this question is inherently linked to how the mind is understood: Does it work on symbolic or distributed structures? I cannot discuss this question in more detail here. But I would like to suggest a pragmatic affirmation of the initial question: Let us assume that current LLMs can serve as an account of how humans process language. As it appears to me, we would then find ourselves in a situation that might be abstracted as follows:</p> <blockquote> <p>Syntax Theory ← Human Language → Large Language Model</p> </blockquote> <p>where both, syntax theories and LLMs, approximate the workings and patterns of human language. The former does so based on symbolic rules, whereas the latter is running on ”distributed patterns of activation across hidden units within LLMs (i.e., the ‘neurons’ of the LLM)” (Blank, 2023, 987). If we do research in linguistics, for example syntax, by using LLMs in experiments, I believe that we would look at the following situation:</p> <blockquote> <p>Syntax Theory ← Large Language Model ← Human Language</p> </blockquote> <p>This line of research seems acceptable, if we agree that the LLM is good enough in approximating human language, although it would certainly have substantial differences compared with human participants (e.g., the LLM’s linguistic capacity is not grounded in a ”human-like trajectory of language acquisition” (Blank, 2023, 988)).</p> <p>The application of large language models (LLMs) in empirical syntax, as proposed in Chapter 4, serves as further evidence that they can enhance theory testing. In general, this research approach aims not only to simulate human language utterances using LLMs for specific research purposes but also to generate linguistic judgments (e.g., by prompting ”Yes” or ”No” responses from an LLM). The data produced by LLMs could then be used to support linguistic theories, such as providing evidence for the existence of Principle C in German.</p> <p>If this approach proves successful and scientifically valid, using LLM simulations could significantly streamline linguistic research. Researchers would no longer need to recruit participants, create filler items for distraction, or rely on laboratory settings for experiments. Instead, a syntactician could simply return to her armchair, testing theories with a computer and proper access to an LLM.</p> <p>However, a significant dependence on the underlying LLMs would also ensue. If the LLM does not represent human language appropriately, its errors would be propagated and syntactic theories might be accepted or rejected based on the LLM’s incomplete representation of human language. The decision to split up the questions Q1 and Q2 for separate queries to reduce the complexity of the task (also consider the one-shot prompt that rather ”confused” the LLM) indicates that GPT 3.5 Turbo is still processing language differently than human beings. This suggests, if an LLM is used in syntactic research, we must consider its limits and capacities thoroughly before we deploy it.</p> <h3 id="53-the-implication-of-linguistic-experiments-for-llms">5.3 The implication of linguistic experiments for LLMs</h3> <p>Deploying LLMs in linguistic research would also help with understanding and possibly improving LLMs. Begus et al. (2023) argue that ”testing complex metalinguistic abilities of large language models is now a possible line of inquiry” and ”that the results of such tests can provide valuable insights into LLMs’ general metacognitive abilities.” (Begus et al., 2023) For example, Begus et al. (2023) created several tests to specifically check how GPT 4 provides formal and theoretical answers to syntactic, semantic, and phonological questions. In similar vain, Wilson et al. (2023) used linguistically inspired tests to investigate how LLMs process argument structures. In particular, they investigated if LLMs can generalize from argument patterns that are recognizable in different contexts. They report that RoBERTa, BERT, and DistilBERT fail some of their tests because they observed that these models are prone to errors ”based on surface level properties like the relative linear order of corresponding elements.” (Wilson et al., 2023, 1391) (This could be a reason why GPT 3.5 Turbo did not show signs of reconstruction. ) Therefore, such experiments would not only help us with understanding the LLMs’ limits, but could also be used for improving future models.</p> <p>The deployment of GPT 3.5 Turbo for pronoun resolution in contexts that are covered by Principle C has shown that few shot learning does not improve the model’s answers. The ambiguous example given in the instruction rather nudged the LLM into accepting more syntactically incorrect interpretations than without. This suggests that few-shot learning may not always be helpful for LLMs if ambiguous phrases are at stake. Furthermore, GPT 3.5 Turbo was generally sensitive to the prompt. An even shorter wording for the task led in manual experimentation to diminished results. This finding corroborates the recent discussion of LLM’s fickleness by Fourrier et al. (2024).</p> <h2 id="project-scripts">Project scripts</h2> <p>The scripts used for the experiments can be found here: <a href="https://github.com/omseeth/using_llms_for_syntactic_research">https://github.com/omseeth/using_llms_for_syntactic_research</a></p> <h2 id="appendix">Appendix</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/using_llms/P_ger-480.webp 480w,/assets/img/using_llms/P_ger-800.webp 800w,/assets/img/using_llms/P_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/using_llms/P_ger.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I would like to thank Martin Salzmann for providing me with the materials and experimental data from Salzmann et al. (2022).</p> <h2 id="bibliography">Bibliography</h2> <p>Adger, D., Drummond, A., Hall, D., and van urk, C. (2017). Is there condition c reconstruction? In Lamont, A. and Tetzloff, K., editors, Nels 47: Proceedings of the forty-seventh annual meeting of the North East Linguistic Society, volume 1, pages 21–30. GLSA.</p> <p>Bates, D. M., Kliegl, R., Vasishth, S., and Baayen, H. (2015). Parsimonious mixed models. arXiv: Methodology.</p> <p>Begus, G., Dabkowski, M., and Rhodes, R. (2023). Large linguistic models: An- alyzing theoretical linguistic abilities of llms. arXiv: Computation and Language.</p> <p>Blank, I. A. (2023). What are large language models supposed to model? Trends in Cognitive Sciences, 27(11):987–989.</p> <p>Bruening, B. and Khalaf, E. A. (2018). No argument–adjunct asymmetry in reconstruction for binding condition c. Journal of Linguistics, 55:247–276. Citko, B. (1993). Deletion Under Identity in Relative Clauses. Proceedings of the North Eastern Linguistic Society (NELS), 31:131–145.</p> <p>Fourrier, C., Louf, R., and Kurt, W. (2024). Improving prompt consistency with structured generations. https://huggingface.co/blog/evaluation-structured-outputs.</p> <p>Futrell, R., Wilcox, E., Morita, T., Qian, P., Ballesteros, M., and Levy, R. (2019). Neural language models as psycholinguistic subjects: Representations of syntactic state. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 32–42, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Haider, H. (2023). Is chat-gpt a grammatically competent informant? ling-buzz/007285.</p> <p>Huang, J. (1993). Reconstruction and the Structure of VP: Some Theoretical Consequences. Linguistic Inquiry, 24(1):103–138.</p> <p>Lebeaux, D. (1991). Relative Clauses, Licensing, and the Nature of the Derivation. Perspectives on Phrase Structure: Heads and Licensing, 25:209–239.</p> <p>Li, A., Feng, X., Narang, S., Peng, A., Cai, T., Shah, R. S., and Varma, S. (2024). Incremental comprehension of garden-path sentences by large language models: Semantic interpretation, syntactic re-analysis, and attention.</p> <p>Linzen, T., Dupoux, E., and Goldberg, Y. (2016). Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535.</p> <p>Salzmann, M., Wierzba, M., and Georgi, D. (2022). Condition C in German A-movement: Tackling challenges in experimental research on reconstruction. Journal of Linguistics, 59(3):577–622.</p> <p>Stockwell, R., Meltzer-Asscher, A., and Sportiche, D. (2021). There is reconstruction for condition c in english questions. In Farinelly, A. and Hil, A., editors, Nels 51: Proceedings of the fifty-first annual meeting of the North East Linguistic Society, volume 2, pages 205–214. GLSA.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p> <p>Wilcox, E., Vani, P., and Levy, R. (2021). A targeted assessment of incremental processing in neural language models and humans. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 939–952, Online. Association for Computational Linguistics.</p> <p>Wilson, M., Petty, J., and Frank, R. (2023). How abstract is linguistic generalization in large language models? experiments with argument structure. Transactions of the Association for Computational Linguistics, 11:1377–1395.</p>]]></content><author><name></name></author><category term="LLM"/><category term="linguistic"/><category term="experiment"/><category term="empirical"/><category term="syntax"/><category term="referring"/><category term="expression"/><category term="Principle-C"/><summary type="html"><![CDATA[Unpublished paper exploring how to use LLMs for empirical studies of syntax]]></summary></entry><entry><title type="html">Was ist Argument Mining?</title><link href="https://omseeth.github.io/blog/2024/argument_mining/" rel="alternate" type="text/html" title="Was ist Argument Mining?"/><published>2024-10-06T10:05:00+00:00</published><updated>2024-10-06T10:05:00+00:00</updated><id>https://omseeth.github.io/blog/2024/argument_mining</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/argument_mining/"><![CDATA[<p>In Abschnitt <strong>1</strong> stelle ich eine Definition für Argumente vor und gehe auf das Argumentieren als kommunikatives Phänomen ein. In dem zweiten Abschnitt <strong>2</strong> erläutere ich Modellierungen von Argumenten. Abschließend skizziere ich in <strong>3</strong> eine Übersicht zu Argument Mining als Aufgabe der natürlichen Sprachverarbeitung.</p> <h2 id="1-argumentieren-als-sprachliches-und-kommunikatives-phänomen">1 Argumentieren als sprachliches und kommunikatives Phänomen</h2> <p>Ein Argument kann nach der <em>Stanford Encyclopedia of Philosophy</em> wie folgt definiert werden: “as a complex symbolic structure where some parts, known as the premises, offer support to another part, the conclusion” (Dutilh Novaes, 2022). Ein Argument ist also ein Gebilde, das mindestens aus zwei Teilen besteht – wobei wir die Definition noch um ein Gebilde aus <em>Propositionen</em> (d.h. wahrheitsfähigen Aussagen) erweitern sollten. In einem Argument stehen nach der Definition mindestens zwei Propositionen insoweit zueinander, sodass eine der Propositionen in einer stützenden Beziehung zu einer anderen steht. Die letztere wird als Konklusion bezeichnet. Geläufig sind aber auch Bezeichnungen wie ‘Behauptung’ oder ‘Standpunkt’.</p> <p>Die Stützkraft einer Prämisse kann unterschiedlich definiert werden (Dutilh Novaes, 2022). Prämissen können die Wahrheit einer Konklusion garantieren; sie können sie wahrscheinlicher oder akzeptabler machen. Auf der Grundlage dieser Einsicht können Argumente auch als deduktiv, induktiv, und abduktiv typisiert werden. Die erste umfangreiche Untersuchung zum deduktiven und induktiven Argumentieren wurde bereits in der Antike von Aristoteles (2007) vorgelegt. Deduktive Argumente führen zu einer Konklusion, die durch die Wahrheitswerte der Prämissen garantiert werden kann. Bei induktiven Argumenten werden Prämissen meist in Form von Beobachtungen zur Regularitäten vorgelegt, die in die Zukunft gerichtete Konklusionen wahrscheinlich machen. Der Philosoph Charles Sanders Peirce führt in seinen Vorlesungen in den 1860er Jahren schließlich das abduktive Argument (1982) ein, das sich von einem induktiven insofern unterscheidet, als das bereits einige Beobachtungen eine Konklusion möglicherweise erklären können und sie auf diese Weise akzeptabel erscheinen lassen.</p> <p>Der Austausch von Argumenten kann als Argumentieren bezeichnet werden. Es ist eine dialogische Praxis, bei der meist die Behauptung einer Aussage zur Folge hat, dass diese weiter explizit mit Prämissen gestützt werden muss, um potentiell von einem Dialogpartner akzeptiert zu werden. Während einer Argumentation lässt sich beobachten, dass Prämissen auch genutzt werden, um Behauptungen zu untergraben. Allgemein wird beim Argumentieren vorausgesetzt, dass alle DialogpartnerInnen rational und ernsthaft an einem Austausch interessiert sind. Gleichzeitig ist die Sprache der Argumente nach Stede und Schneider (2019) oft subjektiv geprägt. (Dies mag verwundern, wenn zumindest der Anspruch eines ernsthaften Argumentierens eine inter-subjektive Übereinkunft ermöglichen sollte.)</p> <p>Für Dutilh Novaes (2022) gibt es drei Ziele des Argumentierens. Zum einen können Meinungsverschiedenheiten durch Argumente aufgelöst werden, das heißt das Argumentieren dient dazu, einen Konsens herbeizuführen (1). Es kann jedoch zum anderen in einer rein adversarialen Auseinandersetzung verhaftet bleiben, zum Beispiel wenn politische KandidatInnen in öffentlichen Debatten ausschließlich eine Zuhörerschaft von ihrem Standpunkt überzeugen wollen (2). Nach Dutilh Novaes (2022) wird in den Wissenschaften das Argumentieren auch als eine epistemische Praxis eingesetzt, mit der Absicht, das Wissen aller Beteiligten zu erweitern (3).</p> <p>Der Verweis auf das tatsächliche Argumentieren deutet in eine Richtung, bei der sich klassische Konzepte der Argumentation von der kommunikativen Praxis unterscheiden. Argumente sind keinesfalls immer gut durchdacht und erfolgreich in ihrer Zielsetzung, ein Gegenüber zu überzeugen. Ein Argument, das weder wahrheitserhaltend noch einen Standpunkt wahrscheinlich oder akzeptabel macht, wird als ‘Fehlschluss’ bezeichnet. In ihrer <em>Introduction to Logic</em> stellen Copi und Cohen (2005, S. 126-7) mindestens 17 Typen solcher Fehlschlüsse vor. Zum Beispiel wird häufig versucht, das Gegenüber durch das Hervorrufen von Mitleid von einer Behauptung zu überzeugen.</p> <h2 id="2-modellierung-von-argumenten">2 Modellierung von Argumenten</h2> <p>Zur Modellierung von Argumenten haben sich im 20. Jahrhundert drei Ansätze etabliert, die jeweils unterschiedliche Schwerpunkte setzen: das Toulmin-Modell, die <em>Neue Rhetorik</em>, und eine Modellstruktur nach Dung (1995).</p> <p>Das Toulmin-Modell (Toulmin, 2003) teilt Argumentkomponenten in sechs mögliche Typen auf: <em>Claim</em>, <em>Data</em>, <em>Warrant</em>, <em>Backing, *Qualifier</em>, <em>Rebuttal</em>. Dabei besteht jedes Argument aus mindestens einer Behauptung (<em>Claim</em>), die durch Annahmen (<em>Data</em>) gestützt wird, und zwar auf der Grundlage einer Art Garantie (<em>Warrant</em>), die Annahmen und Behauptung miteinander verbindet. Die Garantie kann noch durch zusätzliche Aussagen gestützt werden (<em>Backing</em>). Das Toulmin-Modell lässt auch widerlegende Einschübe (<em>Rebuttal</em>) und Qualifizierungen der Komponenten (<em>Qualifier</em>) zu. Der Fokus des Toulmin-Modells liegt nicht auf einen Schlusstyp (sowohl induktive als auch deduktive Argumente können mit dem Schema modelliert werden). Es begrenzt sich allerdings auf die interne Struktur <em>eines</em> Argumentes.</p> <p>Die <em>Neue Rhetorik</em> nach Perelman und Olbrechts-Tyteca (1991) legt ihren Schwerpunkt auf die Überzeugungskraft von Argumenten. Entscheidender Faktor des Argumentierens sind für die AutorInnen die Zuhörerschaft und wie diese für einen Standpunkt rhetorisch gewonnen werden kann. Deshalb sollten Argumente der Rhetorik nach insbesondere anhand ihrer perlokutionären Beschaffenheit verstanden und bewertet werden.</p> <p>Dung (1995) hingegen modelliert nicht die interne Struktur eines Argumentes, sondern wie mehrere Argumente zueinander stehen. Im Fokus seines Ansatzes sind Angriffsbeziehungen zwischen Argumenten. Dung (1995) führt auch eine neue Abstraktionsebene ein: Jedes Argument kann als Knoten eines Graphen interpretiert werden. Die Kanten des Graphen entsprechen Angriffen, wobei die Richtung der Kante zwischen zwei Argumenten auf das angegriffene Argument zeigt.</p> <p>Eine offene Frage bleibt an dieser Stelle, inwieweit Argumente als Bäume abstrahiert werden können. Ein Baum ist ein spezieller Graph, bei dem alle Knoten zusammenhängen und die Kanten keine Kreise bilden. Ansätze zur Modellierung von Argumenten (Palau und Moens, 2009; Peldszus und Stede, 2013; Musi et al., 2018; Hewett et al., 2019), die beispielsweise auch Schnittmengen mit der <em>Rhetorical Structure Theory</em> (Mann und Thompson, 1988) haben, interpretieren Argumente oft als gewurzelte Bäume, bei denen die Konklusion der Wurzel entspricht und die übrigen Komponenten auf diese mit gerichteten Kanten verweisen. In einer Korpusanalyse zu Argumenten geben (Niculae et al., 2017, S. 986) wiederum an, dass um die 20\% ihrer analysierten Argumentstrukturen nicht durch Bäume dargestellt werden können. Auch Pietron et al. (2024) stellen hierarchische Modellierungen von Argumenten mit Bäumen infrage, da Argumente in der kommunikativen Praxis vielfältigere Strukturen aufweisen würden.</p> <h2 id="3-argument-mining-in-der-natürlichen-sprachverarbeitung">3 Argument Mining in der natürlichen Sprachverarbeitung</h2> <p>Das <em>Argument Mining</em> hat sich in der natürichen Sprachverarbeitung zu Beginn des 21. Jahrhunderts, insbesondere ab den 2010er Jahren, als ein Forschungsbereich entwickelt, bei dem die automatische Identifikation und Extraktion von Argumenten und deren Strukturen verfolgt wird. Die Datengrundlage bildet meist geschriebene Sprache, in der Argumente präsentiert werden, wobei weniger literarische Texte genutzt werden, sondern vornehmlich Essays zu kontroversiellen Themen oder Kommentare aus dem Internet zu meist politischen Themen.</p> <p>Eine allgemein anerkannte Definition des <em>Argument Minings</em> gibt es in der Wissenschaftsgemeinschaft nicht. Palau und Moens (2009), Peldszus und Stede (2013), Persing und Ng (2016), Eger et al. (2017), Stede und Schneider (2019), oder Lawrence und Reed (2019) präsentieren jeweils unterschiedliche Definitionen. Ein sehr detaillierter Ansatz wird von \citeStede und Schneider (2019) vorgelegt. Demnach lässt sich <em>Argument Mining</em> in folgende sieben Aufgaben aufteilen (gekürzte Übersetzung aus Stede und Schneider, 2019, S.6-7):</p> <ul> <li>Identifizierung von argumentativem Text</li> <li>Segmentiertung des Textes in argumentative Diskurseinheiten</li> <li>Identifizierung der zentralen Behauptung</li> <li>Identifizierung der Rolle oder Funktion der Einheiten</li> <li>Identifizierung von Relationen zwischen den Einheiten</li> <li>Aufbau einer strukturellen Repräsentation</li> <li>Identifizierung von Typen und der Qualität der Argumentation</li> </ul> <p>Eine prägnantere Definition legen Palau und Moens (2009, S. 5) vor. Ihrer Definition nach lässt sich <em>Argument Mining</em> in drei Aufgaben teilen: (1) Identifizierung von Argumenten im Text, (2) Identifizierung der internen Struktur der Argumente, und (3) Identifizierung von Interaktionen zwischen Argumenten. Während Stede und Schneider (2019) den Schwerpunkt ihrer Definition eher auf die Mikrostruktur eines Argumentes legen (Aufgaben 2.-5.), gehört für Palau und Moens (2009) die Makrostruktur zwischen mehreren Argumenten zu gleichen Teilen zum Aufgabenbereich des <em>Argument Minings</em>. Palau und Moens (2009) lassen wiederum eine qualitative Bewertung von Argumenten außen vor.</p> <p>Es gibt auch keine Einheitlichkeit bei der Frage, was die ‘Bausteine’ des ‘Argument Minings’ sind. Bei den möglichen Argumentkomponenten gibt es sowohl Ansätze mit unterschiedlicher Granularität als auch welche, die die Rollen der Argumentkomponenten umdeuten. Palau und Moens (2009) und Feng und Hirst (2011) nutzen zum Beispiel den <em>Araucaria</em>-Korpus (Reed, 2006), bei dem Argumentkomponenten als Prämissen und Konklusionen repräsentiert werden. Peldszus (2014), der den vorläufigen <em>Microtext</em>-Korpus entwickelt, weist Argumentkomponenten noch besondere Rollen zu, die als Opponent und Proponent bezeichnet werden, und sich aus der dialogischen Form des Argumentierens mit einem Für und Wider ableiten. Stab und Gurevych (2014a) nutzen den <em>Persuasive Essay</em>-Korpus, nach welchem Argumente aus Prämissen, Behauptungen, und zentralen Behauptungen (<em>Major Claim</em>) bestehen. Für Rinott et al. (2015) bestehen argumentative Passagen aus einem Thema, einer Behauptung sowie kontextabhängiger Evidenz. Niculae et al. (2017) nutzen den CDCP-Korpus, der keine argumentativen Komponenten von anderen Propositionen unterscheidet, sondern die letzteren in fünf Klassen (<em>Fact</em>, <em>Policy</em>, <em>Reference</em>, <em>Testimony</em>, <em>Value</em>) aufteilt und unter diesen Beziehungen herstellt. (Dass Wertaussagen (<em>Policy</em>, <em>Value</em>) auch als Propositionen, das heißt Aussagen mit Wahrheitsgehalt gewertet werden, wird vorausgesetzt.) Ein hybrider Ansatz wird schließlich von Schaefer et al. (2023) vorgestellt, wonach Behauptungen weiter als <em>Fact</em>, <em>Policy</em>, und <em>Value</em>, und Prämissen als <em>Testimony</em>, <em>Statistics</em>, <em>Hypthetical Instance</em>, <em>Real-example</em> und <em>Common-ground</em> typisiert werden können.</p> <p>Ein ebenfalls heterogenes Bild gibt es mit Hinsicht möglicher Relationen zwischen Argumentkomponenten. Palau und Moens (2009) geben keine konkreten Relationen an, sondern stützen sich auf eine Vielfalt von möglichen Strukturen nach Walton et al. (2008) (z.B. <em>fulfillment</em> oder <em>causal</em> Relationen, oder verschiedene Angriffsrelationen wie <em>Rebuttal</em> oder <em>Undercutter</em>), die sie implizit mit Regeln einer kontextfreien Grammatik einzufangen versuchen. Peldszus (2014) folgt in Teilen dem Toulmin-Modell und definiert fünf mögliche Argumentrelationen: <em>Support</em>, <em>Example</em>, <em>Rebuttal</em>, <em>Undercutter</em>, <em>Linked</em>. Für Stab und Gurevych (2014a) gibt es nur <em>Support</em> und <em>Attack</em>. Rinott et al. (2015) beschränken wiederum mögliche Relationen zwischen argumentativen Propositionen auf rein unterstützende. Ähnlich verfahren auch Park und Cardie (2018), für die es bei der <em>Support</em>-Relation noch eine Unterteilung in <em>Reason</em> und <em>Evidence</em> gibt.</p> <p>In dem weiteren Verlauf werde ich für die drei Bereiche des <em>Argument Minings</em> nach der kompakteren Definition von Palau und Moens (2009) beispielhaft einige Ansätze vorstellen.</p> <p>Die Identifizierung von Argumenten in einem Text kann über die Segmentierung von Texteinheiten in argumentative oder nicht-argumentative Passagen realisiert werden. Für dieses Ziel nutzen Ajjour et al. (2017) einen Ansatz, bei dem sie für jeden möglichen Token eines Textes ein entsprechendes Label ermitteln. Für ihren Ansatz verwenden sie Texte aus drei Korpora unterschiedlicher Domänen (Essays, Nachrichten, online Kommentare). Die Modelle ihrer Experimente unterscheiden sich, inwieweit sie umliegende Tokens für die Klassifikation mit berücksichtigen: Eine lineare <em>Support Vector Machine</em> mit <em>Features</em> klassifiziert nur jeweils einen Token für sich, ein <em>Conditional Random Field</em> für Sequenzen berücksichtigt hingegen umliegende Tokens innerhalb eines Fensters. Als drittes lassen Ajjour et al. (2017) in ihrem bidirektionalen <em>Long Short-Term Memory</em>-Modell die Informationen aller vorangegangen und nachfolgenden Tokens mit in die Labelprädiktion eines jeden Tokens einfließen.</p> <p>Die Analyse der internen Struktur eines Argumentes kann über die Klassifikation von Argumentkomponenten- und relationen erreicht werden. Auf der Grundlage des <em>Persuasive Essay</em>-Korpus nutzen Stab und Gurevych (2014b) ebenfalls ein Modell basierend auf einer <em>Support Vector Machine</em>. Stab und Gurevych (2014b) entwickeln dafür umfangreiche <em>Feature</em>-Sets, die auf der Grundlage struktureller Merkmale wie Interpunktion oder Satzlänge, lexikalischer Merkmale wie <em>n-grams</em>, von Adverbien, syntaktischer Merkmale, einschlägiger Diskursmarker, oder kontextueller Merkmale wie umliegenden Sätze oder Anzahl von Nebensätzen eine Prädiktion machen. Im Falle der Komponenten klassifizieren sie, ob es sich um Prämisse, Behauptung, oder zentrale Behauptung, und im Falle der Relationen, ob es sich um <em>Support</em>-Relationen handelt.</p> <p>Schließlich kann das Verhältnis von Argumenten zueinander aus einer Makroperspektive wie bei Dung (1995) untersucht werden. Carstens und Toni (2015) nehmen dafür eine Minimaldefinition eines Argumentes an, wonach bereits einzelne Sätze argumentativen Gehalt haben und damit als Argumente behandelt werden können. Unter dieser Annahme klassifizieren sie ausschließlich Relationen (<em>Support</em> und <em>Attack</em>) zwischen mehreren Argumenten. Dafür nutzen Carstens und Toni (2015) ein <em>Random Forest</em>-Modell mit einem selbst entwickelten Korpus. Carstens und Toni (2015) vertreten die Annahme, dass sich aus der Bestimmung einer Relation automatisch ableiten lässt, welche Sätze auch Argumente sind.</p> <h2 id="zusätzliche-resourcen">Zusätzliche Resourcen</h2> <p>Ich klassifiziere Propositionen und deren argumentative Relationen mit BERT im folgenden Projekt: <a href="https://github.com/omseeth/AM_BERT_classification">Argument Mining with BERT classification</a></p> <h2 id="bibliographie">Bibliographie</h2> <p>Ajjour, Y., Chen, W.-F., Kiesel, J., Wachsmuth, H., und Stein, B. (2017). Unit Segmentation of Argumentative Texts. In Habernal, I., Gurevych, I., Ashley, K., Cardie, C., Green, N., Litman, D., Petasis, G., Reed, C., Slonim, N., und Walker, V., Herausgeber, Proceedings of the 4th Workshop on Argument Mining, Seiten 118–128, Copenhagen, Denmark. Association for Computational Linguistics.</p> <p>Aristoteles (2007). BAND 3/I.1 Analytica priora. Buch I. Akademie Verlag, Berlin.</p> <p>Copi, I. und Cohen, C. (2005). Introduction to Logic. Pearson/Prentice Hall.</p> <p>Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence, 77(2):321–357</p> <p>Dutilh Novaes, C. (2022). Argument and Argumentation. In Zalta, E. N. und Nodelman, U., Herausgeber, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2022. Auflage.</p> <p>Eger, S., Daxenberger, J., und Gurevych, I. (2017). Neural End-to-End Learning for Computational Argumentation Mining. In Barzilay, R. und Kan, M.-Y., Herausgeber, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Seiten 11–22, Vancouver, Canada. Association for Computational Linguistics.</p> <p>Feng, V. W. und Hirst, G. (2011). Classifying arguments by scheme. In Lin, D., Matsumoto, Y., und Mihalcea, R., Herausgeber, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Seiten 987–996, Portland, Oregon, USA. Association for Computational Linguistics.</p> <p>Hewett, F., Prakash Rane, R., Harlacher, N., und Stede, M. (2019). The Utility of Discourse Parsing Features for Predicting Argumentation Structure. In Stein, B. und Wachsmuth, H., Herausgeber, Proceedings of the 6th Workshop on Argument Mining, Seiten 98–103, Florence, Italy. Association for Computational Linguistics.</p> <p>Lawrence, J. und Reed, C. (2019). Argument Mining: A Survey. Computational Linguistics, 45(4):765–818.</p> <p>Mann, W. C. und Thompson, S. A. (1988). Rhetorical Structure Theory: Toward a functional theory of text organization. Text - Interdisciplinary Journal for the Study of Discourse, 8(3):243–281.</p> <p>Musi, E., Alhindi, T., Stede, M., Kriese, L., Muresan, S., und Rocci, A. (2018). A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations. In International Conference on Language Resources and Evaluation.</p> <p>Niculae, V., Park, J., und Cardie, C. (2017). Argument Mining with Structured SVMs and RNNs. In Barzilay, R. und Kan, M.-Y., Herausgeber, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Seiten 985–995, Vancouver, Canada. Association for Computational Linguistics.</p> <p>Palau, R. M. und Moens, M.-F. (2009). Argumentation mining: the detection, classification and structure of arguments in text. In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09, Seite 98–107, New York, NY, USA. Association for Computing Machinery.</p> <p>Park, J. und Cardie, C. (2018). A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments. In Calzolari, N., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Hasida, K., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S., und Tokunaga, T., Herausgeber, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p> <p>Peirce, C. S. (1982). Writings of Charles S. Peirce: A Chronological Edition, Volume 5: 1884-1886. Indiana University Press</p> <p>Peldszus, A. (2014). Towards segment-based recognition of argumentation structure in short texts. In Green, N., Ashley, K., Litman, D., Reed, C., und Walker, V., Herausgeber, Proceedings of the First Workshop on Argumentation Mining, Seiten 88–97, Baltimore, Maryland. Association for Computational Linguistics.</p> <p>Peldszus, A. und Stede, M. (2013). From Argument Diagrams to Argumentation Mining in Texts: A Survey. Int. J. Cogn. Inform. Nat. Intell., 7(1):1–31.</p> <p>Pietron, M., Olszowski, R., und Gomu lka, J. (2024). Efficient argument classification with compact language models and ChatGPT-4 refinements.</p> <p>Perelman, C. und Olbrechts-Tyteca, L. (1991). The New Rhetoric: A Treatise on Argumentation. University of Notre Dame Press.</p> <p>Persing, I. und Ng, V. (2016). End-to-End Argumentation Mining in Student Essays. In Knight, K., Nenkova, A., und Rambow, O., Herausgeber, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seiten 1384–1394, San Diego, California. Association for Computational Linguistics.</p> <p>Reed, C. (2006). Preliminary results from an argument corpus. In Berm´udez, E. M. und Miyares, L. R., Herausgeber, Linguistics in the Twenty-first Century, Seiten 185––196. Cambridge Scholars Press.</p> <p>Rinott, R., Dankin, L., Alzate Perez, C., Khapra, M. M., Aharoni, E., und Slonim, N. (2015). Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection. In M`arquez, L., Callison-Burch, C., und Su, J., Herausgeber, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Seiten 440–450, Lisbon, Portugal. Association for Computational Linguistics.</p> <p>Schaefer, R., Knaebel, R., und Stede, M. (2023). Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays. In Alshomary, M., Chen, C.-C., Muresan, S., Park, J., und Romberg, J., Herausgeber, Proceedings of the 10th Workshop on Argument Mining, Seiten 76–88, Singapore. Association for Computational Linguistics.</p> <p>Stab, C. und Gurevych, I. (2014a). Annotating Argument Components and Relations in Persuasive Essays. In Tsujii, J. und Hajic, J., Herausgeber, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, Seiten 1501–1510, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.</p> <p>Stab, C. und Gurevych, I. (2014b). Identifying Argumentative Discourse Structures in Persuasive Essays. In Moschitti, A., Pang, B., und Daelemans, W., Herausgeber, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seiten 46–56, Doha, Qatar. Association for Computational Linguistics.</p> <p>Stede, M. und Schneider, J. (2019). Argumentation Mining. Morgan Claypool, San Rafael (CA).</p> <p>Toulmin, S. E. (2003). The Uses of Argument. Cambridge University Press, 2. Auflage.</p> <p>Walton, D., Reed, C., und Macagno, F. (2008). Argumentation Schemes. Cambridge University Press.</p>]]></content><author><name></name></author><category term="argument"/><category term="mining"/><category term="NLP"/><category term="proposition"/><category term="conclusion"/><summary type="html"><![CDATA[Eine Einführung zum Forschungsfeld des Argument Minings]]></summary></entry><entry><title type="html">Questioning Recursion as a Distinctive Feature of Human Language</title><link href="https://omseeth.github.io/blog/2024/recursive_language/" rel="alternate" type="text/html" title="Questioning Recursion as a Distinctive Feature of Human Language"/><published>2024-10-03T10:00:00+00:00</published><updated>2024-10-03T10:00:00+00:00</updated><id>https://omseeth.github.io/blog/2024/recursive_language</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/recursive_language/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>How can we define language? What difference is there between a string of random sounds and a meaningful combination of signs, communicating information between two beings? A first approximation toward a definition of language could be by postulating that language is a way of representing the world in terms of material entities, such as sounds. We might call these entities signs with meaning. While this is a definition of language broadly speaking, we might want to add that such signs can also be systematically combined with each other, resulting in more complex expressions. By adding this aspect, we arrive at a narrower definition that language is a system of signs with combinatory rules.</p> <p>According to Noam Chomsky, this narrow definition is the basis for what makes human language unique. In more detail, Chomsky argues with Marc Hauser and W. Tecumseh Fitch (2002, abbreviated henceforth as HCF) that humans cannot only combine expressions, but they can do so endlessly by embedding phrases within other phrases. They call this aspect ‘recursion’. According to HCF, this is the distinctive feature that distinguishes human language from communicative systems of animals.</p> <p>The goal of this essay is to discuss HCF’s claim that recursion is the distinctive property of human language. I begin with outlining recursion and explain in what sense recursion appears to be unique to human language. In the second part of this essay, I present ornithological findings. I introduce how starlings demonstrate recursive understanding by recognizing different grammar types. Finally, I propose the hypothesis that the difference between human language and communicative systems used by animals, such as birds, is rather a matter of quantity than quality.</p> <h2 id="an-overview-of-recursion">An Overview of Recursion</h2> <p>Recursive structures are omnipresent in our lives: be it in terms of counting, be it in terms of speaking. Roughly put, recursion can be understood as a function calling upon itself repeatedly. As a first attempt, we might formalize this idea as follows: \(R → R\). But what is more important, this function can even be repeated when additional information is being added. Consider: \(R → R+a\). Such a function provides us with one explanation of how we count. For instance, the rules \(S → 0C\), \(C → C+1\), \(C → ε\) will lead to any natural number, depending on how many times \(C\) is being repeated, until the loop is being terminated with \(ε\). In a similar vein, natural language appears to make extensive use of recursion. This aspect of language is also prominently evident in children’s memory games, such as the ‘My mother went to market’ game where each player must add an item of free choice to the initial phrase, including everything that has previously been said by other players. More complex examples of recursion in language include sentences within sentences. Technically, we can endlessly embed sentences, consisting of subclauses that have additional subclauses and so on. Recursion is one fundamental aspect of how we count or speak, and it is, consequently, part of every other activity or technology that is based on counting and speaking.</p> <p>According to Michael Corballis (2007), it is important to note that recursion is not mere repetition. He believes that recursion, as the principle is evident in how we speak, allows us to freely choose how things might be combined and embedded within each other. That is, recursive rules do not only allow us to repeatedly add another instance. We also can easily jump to higher numbers, skip intermediate additions, multiply terms. The children’s game that has previously been mentioned might not be the best example illustrating recursion because recursion allows us to add whole sentences, several words, or no element at all.</p> <p>While this first sketch of recursion explains a central feature of counting and speaking, it is worthy to keep in mind that human beings only use recursion in a limited sense. By observing our day-to-day activities, we see that we will stop counting at some point. Considering language, each layer that is being added to a sentence through recursion makes it harder to follow what has previously been said. This is so because we have cognitive limits. In other words, we can “overstretch [our] working memory”, says Corballis (2007, p. 244).</p> <h2 id="recursion-as-a-distinctive-feature-of-human-language">Recursion as a Distinctive Feature of Human Language</h2> <p>After comparative research in animal communication, HCF (2002) believe that recursion is the distinctive property of human natural language. While different animal species can communicate with each other in a myriad of ways, through alarm calls, by mobbing predators, by asking for help, or by sharing information about valuable food sources, HCF argue that they “lack the rich expressive and open-ended power of human language” (2002, p. 1570). HCF even grant that animals have conceptual representations of the world. However, these concepts appear closely linked to specific functions. Human concepts, on the other hand, are enmeshed in a broader system, without any “straightforward word-thing relationship”, and “can be linked to virtually any [other] concept that humans [] entertain” (Hauser et al., 2002, p. 1576). This difference brings HCF to the conclusion that we “must entertain the possibility of an independently evolved mechanism” (2002, p. 1576). The mechanism that explains the richness and open-ended structure of human natural language lies for the authors in recursion.</p> <p>With respect to the discussion of recursion in human versus animal language, avian communication is often discussed in more detail (Corballis, 2007; Hauser et al., 2002). For it appears that birds have something like recursion because of their variety of sounds. However, it is argued that birds only have a finite state grammar, that is, a set of limited rules that underlies their communication. The idea of a finite state grammar (also known as regular grammar) consists in restricted combinations of the sort that each rule is determined by the fixed pattern: \(R → aB\) and / or \(R → a\). However, context free or context sensible grammars, which are each more complex respectively, allow either for variations on the right side or variations on both sides of the rule. For this reason, context free grammars can generate expressions of the type \(R → A^n B^n\). It is assumed that this truly allows for the richness of arbitrary combinations that characterizes human language. For the structure of such a grammar can account for center-embedded phrases, such as: ‘I am looking research I need for the essay I must hand in up.’ These constructions would not be possible with a finite state grammar that is confined to a fixed production of binary patterns (see for a visualization of the difference Figure 1).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grammar_differences-480.webp 480w,/assets/img/grammar_differences-800.webp 800w,/assets/img/grammar_differences-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/grammar_differences.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> While the finite state grammar would only allow for a fixed pattern of binary addition, the context free grammar allows for center-embedding. This is an adoption of Gentner’s et al.’s figure (2006, p. 1204).</p> <h2 id="ornithological-findings-concerning-recursion-and-finite-state-grammars">Ornithological Findings Concerning Recursion and Finite State Grammars</h2> <p>There are at least two ways of how one can question the argument that the most developed animals only master a finite state grammar, whereas human beings are capable of processing more complex grammar structures. The first strategy would be to prove that animals can master a context free grammar, too. The second one could probe the power of finite state grammars, which may also account for some degree of recursion.</p> <p>As to the first strategy, Timothy Gentner and his colleagues (2006) created an experiment with the aim to train starlings recognizing different sound motifs, rattles and warbles, that they had recorded and replayed in front of the birds. The motifs were patterned to resemble either a finite state grammar, creating a string of binary sounds, or a context free grammar with center-embedding. The result from simple tests has been that the starlings could learn to recognize and to differently respond to the different sounds sequences, representing each grammar type. To corroborate their results, they further enlarged the sequences by \(n=3\) and \(n=4\) for their finite \((AB)^n\) and context free \(A^nB^n\) sound structures. Again, the birds were capable of producing different reactions to each of the structures. Gentner and his colleagues also assured that the starlings had no similar reactions to arbitrary patterns. While they leave open the possibility that the birds might have used other cognitive heuristics, such as approximating context free patterns through finite state rules, Gentner and his colleagues conclude from their experiment that “starlings can recognize syntactically well-formed strings, including those that use a recursive centre-embedding rule” (2007, p. 1206).</p> <p>The caveat to Gentner’s findings points at the second strategy of questioning the limitations of animal communication. At the outset, we should note that recursion appears in finite state grammars because these grammars can reiteratively produce an infinite string of symbols. This has already been recognized by Chomsky (1956; Chomsky &amp; Miller, 1958) in his early papers on finite languages. So the point is not that human natural language is defined by reiterative rules, but by some context free or higher grammar, allowing for greater varieties of recursion. But what if center-embedding could be achieved with the means of a finite state grammar, too? This still is an ongoing research question where syntacticians (Roche &amp; Schabes, 1997; Dacjuk &amp; van Noord, 2002) attempt to represent a more complex grammar through the means of a stricter one. A detailed discussion of this topic would go beyond this essay. However, it is worthy to keep such a possibility in mind when discussing the properties of language(s).</p> <h2 id="final-discussion">Final Discussion</h2> <p>To conclude, recursion as a reiterative rule cannot be the distinctive property characterizing human natural language because it is possible to loop through finite state grammar rules that have been attributed to animals. Although Chomsky, Hauser, and Fitch (2002) as well as Corballis (2007) speak of ‘recursion’ when singling human language out, it has been evident from the discussion that they have something slightly different in mind that goes beyond the minimal definition of recursion as a self-applicative rule. For them, recursion implies the possibility of grammatically complex structures, allowing center-embedding. For this reason, the essay has additionally discussed the possibility of context free grammars in avian communication by presenting research on starlings. Here, Gentner et al. (2006) offer empirical evidence that starlings can master complex recursive patterns, as in \(R → A^n B^n\) combinations. Therefore, it appears that recursion, neither in its simplest form, nor in its more complex applications, is the distinctive property of human language.</p> <p>While Gentner et al. (2006) believe that humans are not the only ones having recursion, they also acknowledge that there is a difference between communicative systems of birds and human natural language. Obviously, the number of words and syntactical structures over which birds have command, and which they can process, is more restricted than those of humans. We can assume that the most advanced birds use about 100-200 signals (Ballentine &amp; Hyman, 2021) and might have some basic ordering rules, of which some are even recursive. Therefore, it is fair to say that the difference between species is “quantitative rather than qualitative”, especially with respect to “distinctions in cognitive mechanisms” (Gentner et al., 2006, p. 1206). This conclusion also fits Ray Jackendoff’s and Steven Pinker’s (2005) critique of HCF’s claim that the distinction between animals’ and human beings’ communication systems should rather be gradual than categorical.</p> <p>When quantitative differences between human language and animal communication are undeniable, we should not forget that human memory is limited, too. In fact, it has been argued by Fred Karlsson (2007) that actual embedding in spoken language is almost absent and that it appears in written discourse only up to three times. While it is common to assume that a certain quantitative difference ought to be explained with a different quality, or with “an independently evolved mechanism”– as HCF (2002, p. 1576) argue – memory limits in humans makes one wonder where we should draw the line between animal language and human language. While human grammars allow for endless center-embedding in principle, it is rarely used. What if bird grammars also allow for center-embedding principally, but we just have not observed it so far?</p> <p>Another strategy to question recursion in terms of center-embedding as a distinctive property of human language would be to grant that birds only use finite state grammars. However, it would be worthwhile discussing whether these grammars could approximate recursive patterns, such as center-embedding. While there still is need for research on this topic, part of the idea is also driven by the fact that humans’ cognitive capacities are finite. So human natural language might be less context sensible or even less context free in some respects and in that less recursive, as assumed.</p> <h2 id="bibliography">Bibliography</h2> <p>Ballentine, B. &amp; Hyman, J. (2021). Bird Talk: An Exploration of Avian Communication. Cornell University Press.</p> <p>Chomsky, N. (1956). Three models for the description of language. The Institute of Radio Engineers, Inc I.R.E. transactions on information theory, 2 (3), pp. 113-124. https://doi.org/10.1109/TIT.1956.1056813</p> <p>Chomsky, N. &amp; Miller, G. (1958). Finite State Languages. Information and control, 1 (2), pp. 91-112. https://doi.org/10.1016/S0019-9958(58)90082-2</p> <p>Corballis, M. (2007). The Uniqueness of Human Recursive Thinking. American Scientist, 95 (3, May/Jun 2007), pp. 240, 242-248. https://doi.org/10.1511/2007.65.240</p> <p>Daciuk, J. &amp; van Noord, G. (2002). Finite Automata for Compact Representation of Language Models in NLP. In Watson, B.W., Wood, D. (Eds.) Implementation and Application of Automata. pp. 65-73. CIAA 2001. Lecture Notes in Computer Science, 2494. https://doi.org/10.1007/3-540-36390-4_6</p> <p>Gentner, T., Fenn, K., Margoliash, D. &amp; Nusbaum, H. (2006). Recursive syntactic pattern learning by songbirds. Nature, 440 (7088), pp. 1204–1207. https://doi.org/10.1038/nature04675</p> <p>Hauser, M., Chomsky, N. &amp; Fitch, T. (2002). The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?. Science, 298 (5598), pp. 1569-1579. https://doi.org/10.1126/science.298.5598.1569</p> <p>Jackendoff, R. &amp; Pinker, S. (2005). The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky). Cognition, 97 (2005), pp. 211-225. https://doi.org/10.1016/j.cognition.2005.04.006</p> <p>Kallmayer, L. (2010). Parsing Beyond Context Free Grammars. Springer.</p> <p>Karlsson, F. (2007). Constraints on Multiple Center-Embedding of Clauses. Journal of Linguistics, 43 (2), pp. 365-392. https://doi.org/10.1017/S0022226707004616</p> <p>Roche, E. &amp; Schabes, Y. (1997). Finite-State Language Processing. MIT Press.</p>]]></content><author><name></name></author><category term="recursion"/><category term="language"/><category term="animal"/><category term="linguistics"/><category term="finite"/><category term="state"/><category term="grammar"/><category term="context"/><category term="free"/><category term="grammar"/><summary type="html"><![CDATA[Putting human language into perspective with animal linguistics]]></summary></entry><entry><title type="html">Transformer-Modelle wie GPT und BERT erklärt (German version)</title><link href="https://omseeth.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="Transformer-Modelle wie GPT und BERT erklärt (German version)"/><published>2024-09-13T16:40:16+00:00</published><updated>2024-09-13T16:40:16+00:00</updated><id>https://omseeth.github.io/blog/2024/transformer</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/transformer/"><![CDATA[<p>Um die theoretische Grundlage zum Verständnis von Modellen wie GPT und BERT zu schaffen, umreiße ich in diesem Blogbeitrag einige Konzepte der Transformerarchitektur. Dazu diskutiere ich in <strong>1</strong> <em>feedforward</em> neuronale Netze. In Abschnitt <strong>2</strong> beschreibe ich rekurrente neuronale Netze mit einer Enkodierer-Dekodierer-Architektur und dem ersten Aufmerksamkeitsmechanismus. In <strong>3</strong> führe ich alle Elemente zusammen, um ein Transformer-Modell zu beschreiben. Abschließend gehe ich in <strong>4</strong> auf einige Aspekte der GPT-Modelle und BERTs ein.</p> <p>Dieser Beitrag verfolgt zwei Ziele. Zum einen sollen Transformer-Modelle über ihre historische Genese erklärt werden, deshalb empfehle ich die Abschnitte <strong>1</strong> und <strong>2</strong> nachzuvollziehen, wobei das Augenmerk hier auf der Verarbeitung von Sequenzen mit einer Enkodierer-Dekodierer-Struktur liegen sollte. Zum anderen liegt den Transformer-Modellen ein ‘neuer’ (d.h. für das Jahr 2017 neuer) Selbstaufmerksamkeitsmechanismus zugrunde, bei welchem sich auch ein mathematisches Verständnis lohnt. Einmal mehr wird es hoffentlich für die Leserschaft nicht schwierig sein, diesen zu verstehen, sobald ein Gespür für vorangegangene Aufmerksamkeitsmechanismen da ist. Dieser Blogeintrag soll eine Hilfe sein, um Transformer-Modelle zu verstehen. Nichtsdestotrotz empfehle ich die zitierten Veröffentlichungen ebenfalls zu lesen. Eine chronologische Lektüre ist dazu sinnvoll.</p> <h2 id="1-feedforward-neuronale-netze">1 <em>Feedforward</em> neuronale Netze</h2> <h4 id="11-definition-eines-neuronalen-netzes">1.1 Definition eines neuronalen Netzes</h4> <p>Abstrakt betrachtet ist ein neuronales Netz zunächst ein Ansatz, um eine Funktion \(f(X; \theta)\) zu approximieren, durch die mit den Parametern \(\theta\) die Eingabewerte \(X\) abgebildet werden können (Goodfellow et al. 2016). Als Klassifikator würde ein Netz zum Beispiel mit den richtigen Parametern \(f(x)=y\) vorhersagen, wobei \(y\) einem Klassenlabel entspricht (Goodfellow et al. 2016).</p> <p>Neuronale Netze bestehen aus einer Mehrzahl von verknüpften Funktionen, die mit Hilfe eines gerichteten kreisfreien Graphen eine Eingabe bis zur Ausgabe verarbeiten. Die jeweiligen Funktionen können auch als Schichten (<em>layers</em>) \(h_{i}\) mit \(i \in N\) und \(N\) als entsprechende Tiefe des Netzes bezeichnet werden. Die letzte Schicht wird als Ausgabeschicht \(a\) bezeichnet (siehe <strong>Fig. 1</strong>). Anstatt dass man jede Funktion einer Schicht nur als eine Abbildung eines Vektors auf einen Vektor betrachtet, sollte man die Funktionen eher als Kompositionen von Einheiten verstehen, die zusammen Vektoren auf Skalare abbilden, aus denen jeweils ein neuer Vektor geformt wird (Goodfellow et al. 2016).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neural_network_ger-480.webp 480w,/assets/img/neural_network_ger-800.webp 800w,/assets/img/neural_network_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/neural_network_ger.png" class="img-fluid mx-auto d-block" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> Ein neuronales Netz mit Eingabe \(x\), Schicht \(h_i\), und Ausgabeschicht \(a\)</p> <p>Das Konvolut an verknüpften Funktionen eines neuronalen Netztes umfasst auch nicht-lineare Funktionen (Aktivierungsfunktionen). Wir können die Schichten wie folgt definieren: \begin{equation} h_{linear} := W \cdot x + b \end{equation} \begin{equation} h_{nicht-linear} := \sigma(W \cdot x + b) \end{equation} wobei \(W\) eine Matrix mit Gewichten, \(x\) die Eingabe, \(b\) zusätzliche Biases, und \(\sigma\) eine Aktivierungsfunktion (z.B: <em>Sigmoid</em>, <em>Tanh</em>, <em>Softmax</em>) sind. Ein neuronales Netz wird als <em>feedforward</em> bezeichnet, wenn von der Eingabe bis zur Ausgabe des Informationsflusses keine Form von Feedback berücksichtigt wird (Goodfellow et al. 2016).</p> <h4 id="12-training-eines-neuronalen-netzes">1.2 Training eines neuronalen Netzes</h4> <p>Die Approximation der Parameter \(\theta\) (damit sind die Gewichtungen und Biases des Netzes gemeint) folgt dem typischen Schema des maschinellen Lernens aus drei Schritten für jede Trainingsinstanz.</p> <ol> <li>Für \(x \in X\) sagt das Netz einen Wert \(y\) vorher.</li> <li>Dieser Wert wird mit einer weiteren Funktion, einer Verlustfunktion (oft auch als <em>objective function</em> bezeichnet), ‘bewertet’. Der Verlust gibt eine Information darüber ab, inwieweit sich die Vorhersage des Netzes von dem Soll-Wert unterscheidet (typische Verlustfunktionen sind z.B. <em>Mean Squared Error</em> oder <em>Cross Entropy</em>). Durch den Einbezug eines Zielwertes (manchmal auch als <em>Ground Truth</em> bezeichnet) kann hier auch vom überwachten Lernen gesprochen werden.</li> <li>Um zukünftige Verluste zu reduzieren, passt ein Optimierungsalgorithmus alle Parameter des Netzes auf der Grundlage der Verlustfunktion an. Ziel ist es, den Verlust durch Annäherung an sein globales (oder zumindest ein gutes lokales) Minimum zu minimieren. Der Einfluss der einzelnen Parameter auf den Verlust wird durch ihre Gradienten bestimmt. Diese können mit Hilfe der Kettenregel für das gesamte Netz berechnet wird. Auf der Grundlage der Gradienten können die Parameter des Modells so verändert werden, dass der Verlust bei nachfolgenden Vorhersagen verringert wird.</li> </ol> <h2 id="2-rekurrente-netze-mit-einer-enkodierer-dekodierer-architektur">2 Rekurrente Netze mit einer Enkodierer-Dekodierer-Architektur</h2> <h4 id="21-definition-eines-rekurrenten-neuronalen-netzes">2.1 Definition eines rekurrenten neuronalen Netzes</h4> <p>Im Unterschied zu den <em>feedforward</em> Netzen werden bei rekurrenten neuronalen Netzen Informationen innerhalb einer Schicht mit den Zuständen \(h_{i}^{t}\) (auch <em>hidden states</em> bzw. verborgene Zustände genannt) ausgetauscht. Jeder Zustand zum Zeitpunkt \(t\) erhält nicht nur Informationen aus der Eingabe \(x\), sondern auch aus den vorangegangenen Eingaben, z.B. von \(x_{t-1}\), mit ihren jeweiligen Zuständen, also aus \(h_{i}^{t-1}\) usw. (vgl. <strong>Fig. 2</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/recurrent_net_ger-480.webp 480w,/assets/img/recurrent_net_ger-800.webp 800w,/assets/img/recurrent_net_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/recurrent_net_ger.png" class="img-fluid mx-auto d-block" width="40%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2:</strong> Status eines neuronalen Netzes ohne (<em>feedforward</em>) und mit Feedback (rekurrent) sowie jeweils einer Eingabeschicht \(x\), einer verborgenen Schicht \(h_1\) und einer Ausgabe \(a\)</p> <p>Der Vorteil rekurrenter neuronaler Netze wie dem <em>Long Short-Term Memory</em>-Modell (kurz LSTM in Hochreiter und Schmidhuber, 1997) liegt darin, insbesondere sequenzielle Daten wie zum Beispiel Sprache sehr gut modellieren zu können. Bei Sätzen sollte ein Netz die Vorhersage einer Eingabe auch von vorangegenangen Eingaben abhängig machen. Wie man mit Ferdinand de Saussure inspiriert pointieren kann: Die Bedeutung eines Wortes leitet sich aus dem Zusammenspiel der Differenzen der umliegenden Wörter ab (de Saussure, 1931). So kann auch ein neuronales Netz wenig Sinn aus einer isolierten Betrachtung eines jeden Wortes ableiten. Werden hingegen die Bedeutungen der umliegenden Worteingaben mit in einer Schicht eines rekurrenten Netzes einbezogen, das heißt insgesamt eine Sequenz, können dadurch komplexere Zusammenhänge abgebildet werden.</p> <h4 id="22-ein-auto-regressives-sprachmodell">2.2 Ein auto-regressives Sprachmodell</h4> <p>Mit diesen Werkzeugen können wir bereits ein einfaches Sprachmodell zur Vorhersage von Sequenzen entwickeln. Angenommen wir wollen ein Modell, das eine Sequenz \(w\) mit \(w=(w_{1}, w_{2}, w_{3}, ..., w_{n})\) generiert, wobei \(n\) der Länge der Wörter entspricht, die zu der Sequenz, zum Beispiel einem Satz, gehören. Ein viel genutzter Ansatz in der natürlichen Sprachverarbeitung ist, die Vorhersage jedes Wortes durch alle vorangegangenen Wörter der Sequenz abzuleiten. Diese Idee können wir mit der Kettenregel für Wahrscheinlichkeiten wie folgt darstellen: \begin{equation} p(w) = p(w_{1}) \cdot p(w_{2}|w_{1}) \cdot p(w_{3}|w_{1}, w_{2}) \cdot … \cdot p(w_{n}|w_{1}, …, w_{n-1}) \end{equation} An dieser Stelle ist es bereits sinnvoll, nachzuvollziehen, dass wir hier eine auto-regressive Vorhersage der Sequenz verfolgen, bei der jedes Wort als abhängig von allen vorherigen Wörtern behandelt wird.</p> <p>Auf das maschinelle Lernen anhand der Daten \(w\) übertragen folgt aus der vorgeschlagenen Sprachmodellierung, dass wir folgende Funktion approximieren wollen: \begin{equation} p(w; \theta) \end{equation} d.h. wir suchen die besten Parameter \(\theta\) mit vielen Sprachdaten (auch Korpora genannt) für unser Modell, mit denen wir eine Vorhersage für eine Sequenz von Wörtern \(w\) erreichen können, die den genutzten Daten entspricht. Die Approximation können wir durch ein Training sowohl mit einem einfachen <em>feedforward</em> neuronalen Netz als auch einem rekurrenten realisieren. Das rekurrente hat den Vorteil, dass es mit der Weiterreichung von zusätzlichen Informationen durch die Status innerhalb jeder seiner Schichten besser die vorangegangenen Wörter mit einbezieht.</p> <h4 id="23-enkodierer-dekodierer-modelle-zur-maschinellen-übersetzung">2.3 Enkodierer-Dekodierer-Modelle zur maschinellen Übersetzung</h4> <p>Mit LSTM-Modellen entwickeln Sutskever et al. (2014) eine Sequenz-zu-Sequenz-Architektur zur maschinellen Übersetzung. Bei ihrem Ansatz kommen zwei wichtige Ideen zusammen. Zum einen (a) soll eine Übersetzung durch die Ursprungssprache bedingt werden, d.h. ein übersetzter Satz \(A\) (Ausgabe) hängt von seinem Ursprungssatz \(E\) (Eingabe) ab. Zum anderen (b) können Übersetzungen nicht immer wortwörtlich vollzogen werden. Aus diesem Grund ist es sinnvoll, dass ein Modell den ganzen Ursprungssatz berücksichtigt, bevor es eine potenzielle Übersetzung vorhersagt.</p> <p>Die erste Idee (a) führt zu den <em>bedingten</em> Sprachmodellen: \begin{equation} p(w | c; \theta) \end{equation} Bei diesen Modellen hängt die Vorhersage der Wortsequenz nicht nur von jedem vorangegangen Wort ab, sondern wird auch durch den für die Übersetzung wichtigen Ursprungssatz \(c\) bedingt. Prinzipiell kann es sich aber auch um andere Informationen handeln, die so in die Vorhersage mit einfließen würden.</p> <p>Die zweite Idee (b) setzen Sutskever et al. (2014) um, indem sie eine Architektur aus zwei Teilen, einem Enkodierer und einem Dekodierer (siehe <strong>Fig. 3</strong>), entwickeln (vgl. auch Cho et al. 2014). Wobei der Enkodierer den Ursprungssatz in eine feste Repräsentation \(c\) zusammenfasst und dann diese dem Dekodierer zum Vorhersagen der Übersetzung in der Zielsprache übergibt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/seq2seq_ger-480.webp 480w,/assets/img/seq2seq_ger-800.webp 800w,/assets/img/seq2seq_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/seq2seq_ger.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3:</strong> Sequenz-zu-Sequenz-Architektur mit Enkodierer und Dekodierer</p> <p>Für den Enkodierer nutzen Sutskever et al. (2014) ein LSTM-Modell, dem Vektorrepräsentationen (auch <em>Embeddings</em> genannt) für die Wörter einer Eingabesequenz aus der Ursprungssprache zugeführt werden. Es werden <em>Embeddings</em> aus dem einfachen Grund verwendet, da neuronale Netze nur mit Zahlen und nicht mit Buchstaben operieren können. Die verborgenen Status dieser Eingaben werden daraufhin durch das Modell zu einem finalen Zustand \(c\) zusammengeführt: \begin{equation} c = q({h^{1},…,h^{T}}) \end{equation} wobei \(q\) dem LSTM-Modell entspricht und \(T\) der Länge der Eingabesequenz. Der Zustand \(c\) wird dem Dekodierer übergeben.</p> <p>Der Dekodierer besteht auch aus einem LSTM-Modell, welches auf der Grundlage des übergebenen Zustandes Wort für Wort eine Übersetzung in der Zielsprache vorhersagt. Dabei werden jedes übersetzte Wort und der Enkodierer-Endzustand der ursprünglichen Eingabe \(c\) regressiv dem Dekodierer so lange zugeführt, bis das Modell die Übersetzung abschließt: \begin{equation} p(w)= \prod_{t=1}^{T}p(w_{t}|{w_{1},…,w_{t-1}},c) \end{equation} Es schließt die Übersetzung ab, sobald es den Token <strong>&lt;eos&gt;</strong> vorhersagt. Mit diesem besonderen Token zeigen wir dem Modell bereits während des Trainings, an welcher Stelle Sequenzen beginnen und enden. In seinen Prädiktionen wird das Modell deshalb auch im besten Fall am Ende einer Sequenz noch diesen Token vorhersagen und den Inferenzprozess dadurch selbst terminieren.</p> <p>Abschließend noch ein weiteres Wort zum Training eines Sequenz-zu-Sequenz-Modells: während des Trainings werden dem Enkodierer des Modells Sätze aus der Ursprungssprache und dessen Dekodierer deren Übersetzungen entsprechend eines Hyperparameters (z.B. mit <em>Professor Forcing</em> (Goyal et al., 2016)) gezeigt, wodurch die Gewichte \(\theta\) beider Kodierer stets zusammen gelernt und aufeinander abgestimmt werden können. Das Training eines solchen Modells wird wieder über eine Verlustfunktion wie <em>Cross Entropy</em> und einer Optimierung durch die Gradienten realisiert.</p> <h4 id="24-der-erste-aufmerksamkeitsmechanismus">2.4 Der erste Aufmerksamkeitsmechanismus</h4> <p>Um die Qualität der Übersetzungen, insbesondere für lange Sequenzen, zu verbessern, führen Bahdanau et al. (2014) einen Aufmerksamkeitsmechanismus ein. Die Schwäche der Architektur nach Sutskever et al. (2014) liegt darin, dass die zu übersetzende Eingabe in eine einzige Repräsentation \(c\) gezwängt wird, mit welcher der Dekodierer eine Übersetzung finden muss. Allerdings spielen für eine Übersetzung nicht alle Wörter eines Satzes eine gleich große Rolle und auch kann die Beziehung unter den Wörtern variieren. Ob der Artikel in ‘the annoying man’ für ‘l’homme ennuyeux’ mit ‘le’ oder ‘l´’ übersetzt wird, hängt im Französischen beispielsweise davon ab, ob auf den Artikel ein Vokal folgt, gegebenenfalls mit einem stummen ‘h’ davor (Bahdanau et al. 2014). Bahdanau et al. (2014) entwickeln deshalb einen Mechanismus, der diesen Nuancen besser gerecht wird (vgl. <strong>Fig. 4</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_seq_ger-480.webp 480w,/assets/img/attention_seq_ger-800.webp 800w,/assets/img/attention_seq_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention_seq_ger.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4:</strong> Aufmerksamkeitsgewichte für eine Eingabe mit Bezug auf die Ausgabe an der Position <em>i=2</em></p> <p>Die um Aufmerksamkeit erweiterte Architektur übermittelt dem Dekodierer statt \(c\) für jede Eingabe kontextabhängige Zustände \(c_{i}\): \begin{equation} c_{i} = \sum_{t=1}^{T}a_{it}h^{t} \end{equation} Das Gewicht \(a_{it}\) für jeden Zustand \(h^{t}\) (in (Bahdanau et al. 2014) auch ‘Annotation’ genannt), wird wie folgt ermittelt: \begin{equation} a_{it} = \frac{\exp(e_{it})}{\sum_{k=1}^{T}\exp(e_{ik})} \end{equation} wobei \(a_{it}\) eine Normalisierung (<em>Softmax</em>-Funktion) für das Modell \(e_{it}\) ist. Dieses Modell ist wiederum ein <em>Feedforward</em>-Netz mit einer einzelnen Schicht, das bewertet, wie gut die Eingabe zum Zeitpunkt \(t\) mit der Ausgabe an Position \(i\) übereinstimmt. Damit erhält insgesamt jede Eingabe \(x^{1}...x^{T}\) eine eigene Menge an Aufmerksamkeitsgewichten, die in \(c_{i}\) resultieren, einem Kontextvektor, der dem Dekodierer hilft für jede Eingabe die passende Ausgabe (z.B. ‘l’homme’) zu bestimmen.</p> <h2 id="3-transformer-modelle-mit-selbstaufmerksamkeit">3 Transformer-Modelle mit Selbstaufmerksamkeit</h2> <h4 id="31-die-struktur-eines-transformer-modells">3.1 Die Struktur eines Transformer-Modells</h4> <p>Die Transformerarchitektur (Vaswani et al., 2017) führt einige der zuvor genannten Elemente zusammen. Die Architektur verleiht dabei dem Aufmerksamkeitsmechanismus eine wesentlich größere Rolle und verzichtet auf rekurrente Strukturen.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_encoder_ger-480.webp 480w,/assets/img/transformer_encoder_ger-800.webp 800w,/assets/img/transformer_encoder_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_encoder_ger.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 5:</strong> Enkodierer eines Transformer-Modells</p> <p>Der Enkodierer der Transformerarchitektur besteht aus Ebenen mit jeweils zwei Komponenten, durch die die eingehenden Informationen verarbeitet werden (vgl. <strong>Fig. 5</strong>). Eingaben werden als erstes einer Schicht mit einem Selbstaufmerksamkeitsmechanismus <strong>parallel</strong> zugeführt, der in Vaswani et al. (2017) vorgestellt wird. Nachdem dieser Mechanismus angewendet wurde, werden die Informationen normalisiert und daraufhin einer weiteren Schicht mit einem <em>feedforward</em> neuronalen Netz übergeben. Die Verarbeitung der Eingaben findet auf dieser Schicht wiederum <strong>einzeln</strong> statt. Für den Zwischenschritt der Normalisierung werden Mittelwerte und Standardabweichungen nach dem Prinzip der <em>Layer Normalization</em> berechnet (Ba et al. 2016; es gibt auch <em>Root Square Layer Normalization</em> von Zhang &amp; Sennrich (2019), die z.B. in Llama 2 angewendet wurde). Zusätzlich wird die normalisierte Ausgabe mit der Ausgabe der vorangegangenen Schicht addiert. Dies wird auch als ‘<em>Residual Connection</em>’ bezeichnet und ist eine Methode, um dem Problem verschwindender Gradienten während der <em>Backpropagation</em> etwas entgegenzuwirken.</p> <h4 id="32-embeddings-mit-positioneller-enkodierung">3.2 <em>Embeddings</em> mit positioneller Enkodierung</h4> <p>Wie bei den vorigen Sequenz-zu-Sequenz-Architekturen wird die Eingabe zuerst in <em>Embeddings</em> überführt. Die <em>Embeddings</em> werden aber zusätzlich mit einer Positionsenkodierung versehen, die über eine Frequenzdarstellung (Sinus- und Kosinusfunktionen) realisiert wird. Dies begründet sich wie folgt. Im Gegensatz zu den rekurrenten Ansätzen verarbeitet die Aufmerksamkeitsschicht eines Transformerkodierers eine Eingabesequenz auf einmal und zum Beispiel im Falle einer Übersetzung nicht Wort für Wort. Ohne eine zusätzliche Information zur Position einer jeden Eingabe innerhalb einer Sequenz würden den Kodierern die wichtige Information fehlen, wie die einzelnen Wörter aufeinander folgen.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embeddings_ger-480.webp 480w,/assets/img/embeddings_ger-800.webp 800w,/assets/img/embeddings_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embeddings_ger.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 6:</strong> Beispielsequenz, deren Tokens in <em>Embeddings</em> mit \(d=4\) überführt werden</p> <p>Für die Verarbeitung der Eingabe hält die Transformerarchitektur eine <em>Embedding</em>-Matrix für alle Vokabeln der Daten bereit, die in das Training eines Tranformer-Modells einfließen. Die Größe der Matrix entspricht der Anzahl der Vokabeln (sonst als Tokens bezeichnet, zu denen auch bspw. Satzzeichen zählen) Kreuz einer gewählten Dimension (also n x d), mit der jedem Eingabetoken genau eine Zeile innerhalb der Matrix zugeordnet werden kann (vgl. <strong>Fig. 6</strong>). Die Anzahl der Spalten entspricht der gewählten Dimension. Die Matrixwerte für die Tokentypen werden während der Initialisierung eines Transformer-Modells zufällig ausgewählt. Es sind die Zeilen dieser Matrix, die auch als <em>Embeddings</em> bezeichnet werden. Man kann auch sagen, dass jeder Tokentyp eine Vektorrepräsentation besitzt. Wobei diese Vektoren in einem weiteren Schritt mit der positionalen Enkodierung addiert so der Eingabe eine einzigartige Darstellung verleihen. Entscheidend ist, dass die <em>Embeddings</em> der Transformerarchitektur sich im Laufe eines Trainings auch verändern, d.h. entsprechend der Daten angepasst, also ‘gelernt’ werden können. (Anmerkung 1: der erste Schritt ist, die Eingabetokens in eine Indexdarstellung zu überführen, mit der jedem Token eine Zeile in der <em>Embedding</em>-Matrix zugeordnet werden kann; Anmerkung 2 – es hat mich einige Zeit gekostet, das zu verstehen: BERT-Embeddings und dergleichen sind nicht die Embeddings aus der Eingabe; diese Embeddings sind vielmehr das Resultat aus der Verarbeitung der Aufmerksamkeitsmechanismen, und werden am Ende einer Transformer-Schicht entnommen.)</p> <p>Zur Veranschaulichung der Positionsenkodierung ist es hilfreich, zunächst ein stark vereinfachtes Beispiel zu betrachten. Angenommen man teilt jedem <em>Embedding</em> einfach die Positionen (<strong><em>pos</em></strong>) des jeweiligen Tokens in Form von ganzen Zahlen mit \(n \in N\), wobei \(N\) der Länge der Eingabetokens inklusive <strong>&lt;eos&gt;</strong>-Token entspricht, zu. Wenn wir den Token “heiße” und dessen <em>Embedding</em> [0.11, 0.45, 0.23, 0.77] (ausgedachte Werte) auswählen, dann ließe sich für den Token innerhalb der Sequenz “&lt;eos&gt; ich heiße max &lt;eos&gt;” auf diese Weise eine Positionsenkodierung von [2, 2, 2, 2] ermitteln. Der Vektor hätte diese Werte, weil wir die zweite Position der Sequenz (Sequenz beginnt bei 0) und eine <em>Embedding</em>-Dimension von \(d=4\) ausgewählt haben. Der Transformer-Architektur entsprechend könnten wir diesen Vektor anschließend auf das <em>Embedding</em> des Tokens addieren [2.11, 2.45, 2.23, 2.77] und hätten diesem auf diese Weise zusätzliche Informationen hinzugefügt. Dieser Ansatz würde jedoch zu mehreren Problemen führen. Zu nennen sind hier beispielsweise große Werte für lange Sequenzen, deren Positionsenkodierung die Werte der <em>Embeddings</em> stark beeinflussen würden, und es würde ein relativer Bezug wiederkehrender Positionsmuster fehlen.</p> <p>Vaswani et al. (2017, S. 6) stellen deshalb eine Positionsenkodierung vor, durch die jedem Token-<em>Embedding</em> über das Bogenmaß der trigonometrischen Funktionen zusätzliche Informationen zu der Position des Tokens innerhalb einer Sequenz zugeführt werden. Die Vorteile dieses Ansatzes sind unter anderem, dass die Positionsenkodierungswerte auf ein Intervall von \([-1,1]\) beschränkt werden können, und die Periodizität der trigonometrischen Funktionen auch erlaubt, wiederkehrende Muster abzubilden. Denn bestimmte Abstände zwischen Positionen werden ähnliche Werte erzeugen. Dadurch kann das Modell leichter lernen, dass sich bestimmte Muster oder Abstände zwischen Tokens in unterschiedlichen Bereichen einer Sequenz wiederholen, unabhängig von der genauen Position in der Sequenz.</p> <p>Die Positionen der Tokens werden nach Vaswani et al. (2017) wie folgt berechnet: \begin{equation} PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} \begin{equation} PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} <strong><em>pos</em></strong> entspricht der absoluten Position innerhalb einer Eingabesequenz der Länge <strong>N</strong>, der Wert \(10000\) ist eine gewählte Konstante, und <strong>i</strong> verweist auf die Indezes der <em>Embeddings</em>. Zum Beispiel bei einer gewählten Dimension der <em>Embedding</em>-Vektoren mit \(d=4\) gilt \(i \in I= \{0, 1, 2, 3\}\). Schließlich erlauben die beiden Sinus- und Kosinusfunktionen für gerade und ungerade Indizes unterschiedliche Werte zu ermitteln. Für alle gerade Indizes eines <em>Embeddings</em> können wir (10) und für alle ungeraden (11) verwenden. Erwähnenswert ist hier, dass die Frequenzen der Sinus- und Kosinusfunktionen der <strong>PE</strong> von der gewählten Dimension abhängig sind. Kleine <em>Embedding</em>-Dimensionen führen zu höheren Frequenzen (feinere Positionsauflösungen) und hohe Dimensionen zu niedrigeren Frequenzen (gröbere Positionsauflösungen). Anhand dieser Vorgaben wird schließlich für jede Eingabe eine Positionsenkodierungs-Matrix berechnet – also für jeden Token ein Positionsvektor (vgl. <strong>Fig. 7</strong>) – welche wir daraufhin auf die Token-<em>Embeddings</em> addieren können. Im Zusammenspiel mit den <em>Embeddings</em> wird dem Transformer-Modell dadurch eine kontextsensitive Repräsentation der Tokens zur weiteren Verarbeitung überreicht.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_encoding_ger-480.webp 480w,/assets/img/positional_encoding_ger-800.webp 800w,/assets/img/positional_encoding_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/positional_encoding_ger.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 7:</strong> Beispielsequenz, deren Tokens in Positionsenkodierungen mit \(d=4\) überführt werden</p> <h4 id="33-selbstaufmerksamkeitsmechanismus">3.3 Selbstaufmerksamkeitsmechanismus</h4> <p>Im Gegensatz zu dem Aufmerksamkeitsmechanismus nach Bahdanau et al. (2014) entwickeln Vaswani et al. (2017) einen Selbstaufmerksamkeitsmechanismus, den sie auch als ‘skalierte Skalarprodukt-Aufmerksamkeit’ beschreiben (Vaswani et al., 2017, S.3). Die für Transformer genutzte Selbstaufmerksamkeit kann vereinfacht zunächst mit der Operation aus (8) verglichen werden (Raschka et al., 2022). Wir können für einen kontextsensitiven Vektor \(z_{i}\) einer Eingabe an der Stelle \(i\) die Aufmerksamkeit wie folgt berechnen (Raschka et al., 2022): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}x^{j} \end{equation} wobei \(a_{ij}\) nicht mit einem Status \(h^{t}\), sondern mit den Eingaben \(x^{j}\) multipliziert wird, mit \(j\in{\{1, ..., T\}}\) einer Eingabesequenz der Länge \(T\) (vgl. die Summe über alle \(x^{j}\) in (12)). Im Unterschied zu Bahdanau et al. (2014) ist \(a\) dabei keine Normalisierung von einfachen <em>feedforward</em> Netzen \(e_{ij}\), sondern eine <em>Softmax</em>-Normalisierung über die Skalarprodukte \(\Omega\) der Eingabe \(x^{i}\) bezogen auf alle anderen Eingaben \(X=x^{1}, ..., x^{T}\) (Raschka et al., 2022): \begin{equation} a_{ij} = \frac{\exp(\omega_{ij})}{\sum_{j=1}^{T}\exp(\omega_{ij})} \end{equation} mit (Raschka et al., 2022): \begin{equation} \omega_{ij} = x^{(i)T}x^{j} \end{equation} Was wir hier gleichzeitig im Unterschied zum Aufmerksamkeitsmechanismus von Bahdanau et al. (2014) sehen, bei welchem die Aufmerksamkeit insbesondere die Ausgabe des Dekodierers (dort an Ausgabeposition <em>i</em>) mit einbezieht, ist, dass das Aufmerksamkeitsgewicht in (13) mit (14) sich auf die anderen Eingaben einer Sequenz bezieht. Eben aus diesem Grund ist es sinnvoll, von einer <em>Selbstaufmerksamkeit</em> zu sprechen.</p> <p>Zu dieser Darstellung der Aufmerksamkeit fügen Vaswani et al. (2017) eine weitere Veränderung für jede Eingabe \(x^{i}\) hinzu, und zwar wird das Gewicht \(a\) nicht mit \(x^{j}\) multipliziert, sondern mit einem Wert \(v^{j}\): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}v^{j} \end{equation} Denn Vaswani et al. (2017) überführen jedes \(x^{i}\) in ein Tripel aus (\(v^{i}\), \(k^{i}\), \(q^{i}\)) mittels den Projektionsmatrizen (\(W_{v}\), \(W_{k}\), \(W_{q}\) – die hier auch als zusätzliche lineare Schichten aufgefasst werden können). Die Idee dahinter entstammt dem <em>Information Retrieval</em>, das mit Wert-, Schlüssel-, Abfragetripeln arbeitet (wegen Values, Keys, Queries die Abkürzungen v, k, q). Die <strong>V</strong>, <strong>K</strong>, <strong>Q</strong> in <strong>Fig. 5</strong> und <strong>Fig. 8</strong> entsprechen: \(V=XW_{v}\), \(K=XW_{k}\) sowie \(Q=XW_{q}\) (vgl. <strong>Fig. 8</strong>). Die Skalarprodukte des Selbstaufmerksamkeitsmechanismus für jede Eingabe werden in Vaswani et al. (2017) auch nicht mit (14) berechnet, sondern mit den Abfrage- und Schlüsselwerten (Raschka et al., 2022): \begin{equation} \omega_{ij} = q^{(i)T}k^{j} \end{equation} Kurz: Neben der Aufmerksamkeit einer Eingabe \(x^i\) gegenüber allen anderen Eingaben innerhalb einer Sequenz \(X\) wird die Selbstaufmerksamkeit noch durch verschiedene Darstellungen aller umliegenden \(x^j \in X\) in Form der Abfrage-, Schlüssel-, Wertrepräsentationen berechnet.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projection_matrices_ger-480.webp 480w,/assets/img/projection_matrices_ger-800.webp 800w,/assets/img/projection_matrices_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/projection_matrices_ger.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Aufmerksamkeit für eine Eingabe X mit mehreren Köpfen (<em>heads</em>)</p> <p>Die Aufmerksamkeitsgewichte werden abschließend mit der Dimension der <em>Embeddings</em> noch skaliert (\(\frac{\omega_{ij}}{\sqrt{d}}\)) und können \(h\)-Mal parallel berechnet, wobei \(h\) einer gewählten Anzahl an Köpfen (auch <em>Attention Heads</em> gennant) entspricht. Vaswani et al. (2017) wählen \(h=8\) Köpfe, deren Werte konkateniert abschließend der <em>Layer</em>-Normalisierung in den Kodierern weitergereicht werden (siehe <strong>Fig. 8</strong> und <strong>Fig. 5</strong>). Die Verwendung mehrerer Köpfe wird als ‘<em>Multi-head Attention</em>’ bezeichnet. Die zusätzliche Skalierung begründen Vaswani et al. (2017, S. 4) mit der Beobachtung, dass zu große Werte der Skalarprodukte (vgl. (14)) die für die zusätzliche Normalisierung genutzte <em>Softmax</em>-Funktion in einen Bereich führen, der beim Lernen in sehr kleine Gradienten resultiert.</p> <h4 id="34-der-transformer-dekodierer">3.4 Der Transformer-Dekodierer</h4> <p>Der Dekodierer der Transformerarchitektur folgt strukturell dem Enkodierer. Er enthält jedoch noch eine zusätzliche Schicht (vgl. <strong>Fig. 9</strong>). In dieser Schicht werden die ausgegebenen Informationen des Enkodierers (z.B. der enkodierte Ursprungssatz einer Übersetzung) über die Wert- und Schlüsselwerte \(V\), \(K\) mitberücksichtigt. Die Abfragewerte \(Q\) kommen hingegen von der vorangegangenen Aufmerksamkeitsschicht des Dekodierers. Durch die Kombination der Informationen sowohl des Enkodierers als auch des Dekodierers wird diese weitere Schicht auch als ‘Cross-Aufmerksamkeitsschicht’ bezeichnet. Da dabei Enkodierer-Informationen mit einbezogen werden, lässt sich bei dem ganzen Modell (Enkodierer + Dekodierer) zudem von einem <em>bedingten</em> Sprachmodell sprechen, wie dieses zuvor in (5) vorgestellt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_decoder_ger-480.webp 480w,/assets/img/transformer_decoder_ger-800.webp 800w,/assets/img/transformer_decoder_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_decoder_ger.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 9:</strong> Dekodierer eines Transformer-Modells</p> <p>Die Selbstaufmerksamkeitsschicht des Dekodierers erlaubt es, Teile der Dekodierereingabe zu verdecken (dies wird auch als <em>Masking</em> beschrieben, geanuer gesagt <em>Causal Masking</em>). Ein Transformer-Enkodierer lässt hingegen grundsätzlich eine Betrachtung aller Eingaben gleichzeitig zu. Das Masking spielt eine wichtige Rolle bei dem Ziel des Dekodierers: z.B. die Vorhersage einer Übersetzung. Zur Prädiktion einer Übersetzungssequenz arbeitet der Dekodierer sich jeweils autoregressiv von Token zu Token vor (z.B. von links nach rechts, vgl. hierzu auch (7)). Das bedeutet, dass beim Inferenzieren jede Prädiktion nur mit Hilfe vorangegangener Tokens arbeitet, die anderen bleiben im übertragenen Sinne maskiert. Für das Training können dem Modell noch durch <em>Professor Forcing</em> korrekte Übersetzungen als Eingabe mit zugeführt werden, um eine Fehlerfortpflanzung zu minimieren – wie bei den zuvor beschriebenen Sequenz-zu-Sequenz-Modellen. Grundsätzlich ist das Trainingsziel die Vorhersagen des Modells anhand der Übersetzungslösungen zu optimieren. Ein Übersetzungsprozess terminiert in jedem Fall, wenn der Token <strong>&lt;eos&gt;</strong> oder die zuvor definierte maximale Sequenzlänge erreicht sind.</p> <p>Abschließend noch ein Wort zu der linearen Schicht am Endes des Dekodierers (vgl. <strong>Fig. 9</strong>). Die unmittelbare Ausgabe aus den Aufmerksamkeitsblöcken umfasst eine durch das Modell angereicherte Repräsentation der Eingabe-<em>Embeddings</em> \(h_{i}\). In diese Repräsentation sind durch die Aufmerksamkeitsmechanismen und den <em>feedforward</em>-neuronalen Netzen zusätzliche Informationen umliegender Tokens sowie des Enkodierers mit eingeflossen. Es gilt jedes \(h_{i}\) wieder in eine Darstellung des Vokabulars zu überführen. Dafür stellt die lineare Schicht eine Projektionsmatrix \(W\) bereit. Diese ähnelt einer Schicht eines neuronalen Netztes mit dem Unterschied, dass hier keine nicht-lineare Aktivierungsfunktion den Informationsfluss noch weiter verändert.</p> <p>Betrachten wir dazu ein Beispiel. Angenommen dem Modell liegt eine Vokabelgröße von \(10 000\) zugrunde und für die <em>Embeddings</em> bzw. die Status des Modells wählen wir exemplarisch eine Dimension von \(d=512\). Dann können wir mit \(W\) (10000 x 512) alle \(h_{i}\) in einen <em>Logits</em>-Vektor überführen, der der Dimension des Vokabulars entspricht, und dessen Wert gleichzeitig die Approximation des Modells dafür ist, wie wahrscheinlich welcher der Token des Vokabulars ist: \begin{equation} logits = W \cdot h_{i} + b \end{equation} wobei \(b\) als zusätzlicher Bias auf noch einen Einfluss auf die Abbildung nimmt. Auf der Grundlage dieses <em>Logits</em>-Vektor (z.B. \(logits = [3.4, -1.2, 0.5, ..., 2.7]\)) kann schlussendlich die <em>Softmax</em>-Aktivierung, mit der die Werte des Vektors in Wahrscheinlichkeiten überführt werden, den wahrscheinlichsten Token für den ausgegebenen Status \(h_{i}\) des Dekodierers vorhersagen. An dieser Stelle ließen sich aber auch andere Dekodierungsstrategien (z.B. <em>Beam-Search</em> oder <em>Greedy Decoding</em>) einsetzen.</p> <p>Insgesamt unterscheiden sich Inferenz und Training bei einem Transformer-Modell nicht von anderen neuronalen Netzen (siehe Kapitel <strong>1.2</strong>).</p> <h2 id="4-gpt-bert-und-co">4 GPT, BERT und co</h2> <p>Derweil die ursprüngliche Transformerarchitektur zur maschinellen Übersetzung entwickelt wurde, haben sich Transformer-Modelle bei anderen Aufgaben ebenfalls bewährt. Am bekanntesten sind große Sprachmodelle wie <em>Generative Pre-trained Transformer</em>-Modelle (Radford et al., 2018) von OpenAI, die eine Eingabe (Prompt) mit einem Transformer-Dekodierer sozusagen ‘weiterschreiben’, d.h. die nächsten <em>Tokens</em> der Sequenz vorhersagen. Sprachmodelle wie GPT bestehen dabei nur aus Dekodierer-Ebenen. Das <em>Bidirectional Encoder Representations from Transformers</em>-Modell (kurz BERT, Devlin et al., 2019) ist wiederum ein reiner Transformer-Enkodierer. Das heißt mit BERT können keine neuen Wörter oder Sätze in einer Zielsprache <em>autoregressiv</em> generiert werden. BERT stellt dafür Enkodierungen bereit, mit deren Hilfe sich zum Beispiel Klassifikationsaufgaben lösen lassen. Grundsätzlich besteht die Entwicklung der meisten Transformer-Modelle aus zwei Phasen: einem Vortraining und einem Feintuning für gezielte Anwendungen. Zum Schluss stelle ich diese beiden Schritte anhand von BERT vor.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/language_model_types_ger-480.webp 480w,/assets/img/language_model_types_ger-800.webp 800w,/assets/img/language_model_types_ger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/language_model_types_ger.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 10:</strong> Verschiedene Transformerarchitekturen: Enkodierer-Dekodierer zur maschinellen Übersetzung (auch <em>MTM</em> genannt), einzelne Dekodierer zur Generierung von Sequenzen (auch als <em>LM</em> bezeichnet), und einzelne Enkodierer für <em>Downstream</em>-Aufgaben (auch häufig als <em>MLM</em> bezeichnet)</p> <h4 id="41-das-bidirectional-encoder-representations-from-transformers-modell">4.1 Das <em>Bidirectional Encoder Representations from Transformers</em>-Modell</h4> <p>Für BERT trainieren Devlin et al. (2019) zunächst den Enkodierer eines Transformer-Modells vor dem Hintergrund zweier Aufgaben. Die erste Aufgabe des BERT-Trainings besteht aus einer maskierten Sprachmodellierung (<em>Masked Language Modelling</em>). Das Modell bekommt dabei Sätze gezeigt, bei denen es 15% zufällig ausgewählte Wörter, die verdeckt werden, vorhersagen muss. Die zweite Aufgabe besteht aus einer binären Klassifikation zweier Sätze, und zwar mit dem Ziel, vorherzusagen, ob diese aufeinander folgen oder nicht. Wobei das Modell 50% korrekte Satzfolgen und 50% inkorrekte Folgen gezeigt bekommt. Da das Modell ausschließlich einen Enkodierer verwendet – und keine ‘rechtsbündige’ Maskierung der jeweils nächsten Tokens innerhalb einer Sequenz wie bei einem Transformer-Dekodierer vorgenommen wird – kann das Training auch als bi-direktional bezeichnet werden. Der Enkodierer hat Zugang zu jeder Eingabe von beiden Seiten. Devlin et al. (2019) bezeichnen das Training ihres Enkodierers auf den beiden Aufgaben als Vortraining.</p> <p>In einem weiteren Schritt nutzen Devlin et al. (2019) das vortrainiertes BERT-Modell für weitere Experimente aus der natürlichen Sprachverarbeitung. Dafür feintunen sie BERT beispielsweise, um die plausibelste Wortfolge für Sätze aus dem Datensatz <em>Situations With Adversarial Generations</em> (Zellers et al., 2018) zu klassifizieren. Dazu bekommt BERT für das Training verschiedene mögliche Fortsetzungen gezeigt. Aus diesen muss das Modell die plausibelste auswählen. Da BERT ursprünglich vor dem Hintergrund anderer Aufgaben vortrainiert wurde, die Gewichte des Modells jedoch auch für solche neue Aufgaben verwendet und angepasst werden können, kann ein solches Feintuning BERTs auch als eine Form des Transferlernens bezeichnet werden.</p> <h2 id="zusätzliche-ressourcen">Zusätzliche Ressourcen</h2> <ul> <li>Ich kann Brendan Bycrofts Visualisierung von Transformer-Modellen empfehlen: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></li> <li>Mit die besten Erklärungen und Visualisierungen zu verschiedenen Modellierungsansätzen in NLP, einschließlich der Transformer-Architektur, stellt Lena Voita bereit: <a href="https://lena-voita.github.io/nlp_course.html">https://lena-voita.github.io/nlp_course.html</a></li> <li>Weitere hilfreiche Visualisierungen zu BERT gibt Jay Alammar: <a href="https://jalammar.github.io/illustrated-bert/">https://jalammar.github.io/illustrated-bert/</a></li> </ul> <h2 id="bibliographie">Bibliographie</h2> <p>Ba, J., Kiros, J.R., &amp; Hinton, G.E. (2016). Layer Normalization. ArXiv, abs/1607.06450.</p> <p>Bahdanau, D., Cho, K., und Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. <em>CoRR</em>, abs/1409.0473.</p> <p>Cho, K., van Merri ̈enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., und Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Moschitti, A., Pang, B., und Daelemans, W., Herausgeber, <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, Seiten 1724–1734, Doha, Qatar. Association for Computational Linguistics.</p> <p>de Saussure, F. (1931). <em>Cours de Linguistique Generale</em>. Payot, Paris.</p> <p>Devlin, J., Chang, M.-W., Lee, K., und Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J., Doran, C., und Solorio, T., Herausgeber, <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Volume 1 (Long and Short Papers), Seiten 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Goodfellow, I., Bengio, Y., und Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p> <p>Goyal, A., Lamb, A. M., Zhang, Y., Zhang, S., Courville, A. C., und Bengio, Y. (2016). Professor Forcing: A New Algorithm for Training Recurrent Networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., und Garnett, R., Herausgeber, Advances in <em>Neural Information Processing Systems</em>, Band 29. Curran Associates, Inc.</p> <p>Hochreiter, S. und Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Comput.</em>, 9(8):1735–1780.</p> <p>Radford, A., Narasimhan, K., Salimans, T., und Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. Technical report, OpenAI.</p> <p>Raschka, S., Liu, Y., und Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sutskever, I., Vinyals, O., und Le, Q. V. (2014). Sequence to sequence learning with neural networks. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS’14, Seite 3104–3112, Cambridge, MA, USA. MIT Press.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., und Polosukhin, I. (2017). Attention is All you Need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., und Garnett, R., Herausgeber, <em>Advances in Neural Information Processing Systems</em>, Band 30. Curran Associates, Inc.</p> <p>Zellers, R., Bisk, Y., Schwartz, R., und Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Riloff, E., Chiang, D., Hockenmaier, J., und Tsujii, J., Herausgeber, <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, Seiten 93–104, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Zhang, B., and Sennrich, R. (2019). Root mean square layer normalization. <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc., Red Hook, NY, USA.</p> <p><strong>Version 1.1</strong></p>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><category term="neural"/><category term="nets"/><category term="feed-forward"/><category term="attention-mechanism"/><category term="transformer"/><category term="GPT"/><category term="BERT"/><category term="transfer-learning"/><category term="German"/><summary type="html"><![CDATA[Vom neuronalen Netz bis zu GPT und BERT, eine kontextualisierte Erklärung der Transformer-Architektur]]></summary></entry><entry><title type="html">Transformer models such as GPT and BERT explained (English version)</title><link href="https://omseeth.github.io/blog/2024/transformer_en/" rel="alternate" type="text/html" title="Transformer models such as GPT and BERT explained (English version)"/><published>2024-09-13T16:40:16+00:00</published><updated>2024-09-13T16:40:16+00:00</updated><id>https://omseeth.github.io/blog/2024/transformer_en</id><content type="html" xml:base="https://omseeth.github.io/blog/2024/transformer_en/"><![CDATA[<p>To provide the theoretical basis for understanding models such as GPT and BERT, I outline some concepts of the Transformer architecture in this blog post. To do this, I discuss feedforward neural networks in <strong>1</strong>. In section <strong>2</strong>, I describe recurrent neural networks with an encoder-decoder architecture and the first attention mechanism. In <strong>3</strong>, I bring all the elements together to describe a Transformer model. Finally, in <strong>4</strong>, I discuss some specifics of GPT models and BERT.</p> <p>This article has two aims. On the one hand, Transformer models are to be explained via their historical genesis, which is why I recommend reading sections <strong>1</strong> and <strong>2</strong>, whereby the focus here should be on the processing of sequences with an encoder-decoder structure. Secondly, the Transformer models are based on a ‘new’ (i.e. new for 2017) self-attention mechanism, which is also worth understanding mathematically. Once again, it will hopefully not be difficult for readers to understand this once they have an intuition for previous attention mechanisms. This blog entry is intended as an aid to understanding the Transformer model. Nevertheless, I recommend that you also read the publications cited. It makes sense to read them in chronological order.</p> <h2 id="1-feedforward-neural-networks">1 Feedforward neural networks</h2> <h4 id="11-definition-of-a-neural-network">1.1 Definition of a neural network</h4> <p>In abstract terms, a neural network is initially an approach for approximating a function \(f(X; \theta)\) that can be used to map the input values \(X\) with the parameters \(\theta\) (Goodfellow et al. 2016). For example, as a classifier, a network with the correct parameters would predict \(f(x)=y\), where \(y\) corresponds to a class label (Goodfellow et al. 2016).</p> <p>Neural networks consist of a number of linked functions that process an input to an output using a directed circle-free graph. The respective functions can also be referred to as layers \(h_{i}\) with \(i \in N\) and \(N\) as the corresponding depth of the network. The last layer is called the output layer \(a\) (cf. <strong>Fig. 1</strong>). Instead of considering each function of a layer as a mapping of a vector to a vector, the functions should rather be understood as compositions of units that together map vectors to scalars that together will form a new vector (Goodfellow et al. 2016).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neural_network_en-480.webp 480w,/assets/img/neural_network_en-800.webp 800w,/assets/img/neural_network_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/neural_network_en.png" class="img-fluid mx-auto d-block" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 1:</strong> A neural network with input \(x\), layer \(h_i\), and output layer \(a\)</p> <p>The set of linked functions of a neural network also includes non-linear functions (activation functions). We can define the layers as follows: \begin{equation} h_{linear} := W \cdot x + b \end{equation} \begin{equation} h_{non-linear} := \sigma(W \cdot x + b) \end{equation} where \(W\) is a matrix with weights, \(x\) the input, \(b\) additional biases, and \(\sigma\) an activation function (e.g., sigmoid, tanh, softmax). A neural network is described as feedforward if no form of feedback is taken into account from the input to the output of the information flow (Goodfellow et al. 2016).</p> <h4 id="12-training-of-a-neural-network">1.2 Training of a neural network</h4> <p>The approximation of the parameters \(\theta\) (i.e. the weights and biases of the network) follows the typical machine learning scheme of three steps for each training instance.</p> <ol> <li>For \(x \in X\) the network predicts a value \(y\).</li> <li>This value is ‘evaluated’ with another function, a loss function (also often called an objective function). The losses provide information on the extent to which the prediction of the network differs from a target value (typical loss functions are e.g. Mean Squared Error or Cross Entropy). By including a target value (sometimes also referred to as ground truth), this can also be referred to as supervised learning. We’ll see that training a language model on text is a form of self-supervised learning.</li> <li>To reduce future losses, an optimization algorithm adjusts all parameters of the network based on the loss function. The goal is to minimize the loss by approximating its global (or at least a good local) minimum. The influence of each parameter on the loss is determined through its gradient, which is computed using the chain rule across the entire network. By leveraging these gradients, the model updates its parameters in a way that reduces the loss over subsequent predictions.</li> </ol> <h2 id="2-recurrent-neural-networks-with-an-encoder-decoder-architecture">2 Recurrent neural networks with an encoder-decoder architecture</h2> <h4 id="21-definition-of-a-recurrent-neural-network">2.1 Definition of a recurrent neural network</h4> <p>In contrast to feedforward networks, recurrent neural networks exchange information within a layer with the states \(h_{i}^{t}\) (also called hidden states). Each state at time \(t\) not only receives information from the input, but also from previous inputs, such as \(x_{t-1}\), and their states, i.e. from \(h_{i}^{t-1}\) and so on (cf. <strong>Fig. 2</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/recurrent_net_en-480.webp 480w,/assets/img/recurrent_net_en-800.webp 800w,/assets/img/recurrent_net_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/recurrent_net_en.png" class="img-fluid mx-auto d-block" width="40%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 2:</strong> Status of a neural network without (feedforward) and with feedback (recurrent) and in each case an input layer \(x\), a hidden layer \(h_1\) and an output \(a\)</p> <p>The advantage of recurrent neural networks such as the Long Short-Term Memory Model (LSTM for short in Hochreiter and Schmidhuber, 1997) is that they are particularly good at modelling sequential data such as speech. A net should base its prediction for an input from a sentence also on previous words. As inspired by Ferdinand de Saussure: The meaning of a word is derived from the interplay of the differences of the surrounding words (de Saussure, 1931). Thus, even a neural network can derive little meaning from an isolated consideration of each word. However, if the meanings of the surrounding word inputs are included in a layer of a recurrent network, i.e. a sequence as a whole, more information is taken into account.</p> <h4 id="22-auto-regressive-language-models-lms">2.2 Auto-regressive language models (LMs)</h4> <p>With these tools, we can already develop a simple language model (LM) for predicting sequences. Suppose we want a model that generates a sequence \(w\) with \(w=(w_{1}, w_{2}, w_{3}, ..., w_{n})\), where \(n\) corresponds to the length of the words belonging to the sequence, for example a sentence. A much-used approach in natural language processing is to derive the prediction of each word from all previous words in the sequence. We can illustrate this idea with the chain rule for probabilities as follows: \begin{equation} p(w) = p(w_{1}) \cdot p(w_{2}|w_{1}) \cdot p(w_{3}|w_{1}, w_{2}) \cdot … \cdot p(w_{n}|w_{1}, …, w_{n-1}) \end{equation} At this point, it is already useful to understand that we are following an auto-regressive prediction of the sequence, where each word is treated as dependent on all previous words.</p> <p>Applied to machine learning using the data \(w\), it follows from the proposed language modelling that we want to approximate the following function: \begin{equation} p(w; \theta) \end{equation} i.e. we are looking for the best parameters \(\theta\) with language data (also called corpora) for our model, with which we can achieve a prediction for a sequence of words \(w\) that corresponds to the data used. We can realise the approximation by training both a simple feedforward neural network and a recurrent one. The recurrent neural network has the advantage that it better incorporates the preceding words by passing additional information through the states within each of its layers.</p> <h4 id="23-encoder-decoder-models-for-machine-translation-mt">2.3 Encoder-decoder models for machine translation (MT)</h4> <p>Using LSTMs, Sutskever et al. (2014) develop a sequence-to-sequence architecture for machine translation (MT). Their approach combines two important ideas. Firstly, a translation should be conditioned by the original language, i.e. a translated sentence \(O\) (output) depends on its original sentence \(I\) (input). Secondly, translations cannot always be carried out literally. For this reason, it makes sense for a model to consider the whole original sentence before predicting a potential translation.</p> <p>The first idea (a) leads to the <em>conditional</em> language models: \begin{equation} p(w | c; \theta) \end{equation} In these models, the prediction of the word sequence not only depends on each preceding word, but is also conditioned by the source sentence \(c\), which is so important for the translation. In principle, however, it can also be other information that would be included in the prediction.</p> <p>The second idea is implemented by Sutskever et al. (2014) by developing an architecture consisting of two parts, an encoder and a decoder (see <strong>Fig. 3</strong>) (see also Cho et al. 2014). Whereby the encoder summarizes the source sentence into a fixed representation \(c\) and then passes it to the decoder to predict the translation in the target language.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/seq2seq_en-480.webp 480w,/assets/img/seq2seq_en-800.webp 800w,/assets/img/seq2seq_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/seq2seq_en.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 3:</strong> Sequence-to-sequence architecture with encoder and decoder</p> <p>For the encoder, Sutskever et al. (2014) use an LSTM model that is fed vector representations (also called embeddings) for the words of an input sequence from the original language. Embeddings are used for the simple reason that neural networks can only operate with numbers and not letters. The hidden states of these inputs are then merged by the model into a final state \(c\): \begin{equation} c = q({h^{1},…,h^{T}}) \end{equation} where \(q\) corresponds to the LSTM model and \(T\) to the length of the input sequence. The state \(c\) is passed to the decoder.</p> <p>The decoder also consists of an LSTM model, which predicts a translation in the target language word by word based on the input state. Each translated word and the final encoder state of the original input \(c\) are regressively fed to the decoder until the model completes the translation: \begin{equation} p(w)= \prod_{t=1}^{T}p(w_{t}|{w_{1},…,w_{t-1}},c) \end{equation} It completes the translation as soon as it predicts the token <strong>&lt;eos&gt;</strong>. We use this special token to show the model where sequences begin and end during training. In its predictions, the model will therefore also predict this token at the end of a sequence in the best case and thus terminate the inference process itself.</p> <p>Finally, one more word about the training of a sequence-to-sequence model: during training, the encoder of the model is shown sentences from the original language and its decoder is shown their translations according to a hyperparameter (e.g. with Professor Forcing (Goyal et al., 2016)), whereby the weights \(\theta\) of the encoder as well as the decoder can always be learned together and matched to each other. This training can be again achieved with a loss function like Cross Entropy and optimization based on gradients.</p> <h4 id="24-the-first-attention-mechanism">2.4 The first attention mechanism</h4> <p>To improve the quality of translations, especially for long sequences, Bahdanau et al. (2014) introduce an attention mechanism. The weakness of Sutskever et al.’s (2014) architecture is that the input to be translated is forced into a single representation \(c\), with which the decoder must find a translation. However, not all words in a sentence play an equally important role in a translation and the relationship between the words can also vary. Whether the article in ‘the annoying man’ for ‘l’homme ennuyeux’ is translated as ‘le’ or ‘l´’, for example, depends in French on whether the article is followed by a vowel, possibly with a silent ‘h’ in front of it (Bahdanau et al. 2014). Bahdanau et al. (2014) therefore develop a mechanism that does better justice to these nuances (cf. <strong>Fig. 4</strong>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_seq_en-480.webp 480w,/assets/img/attention_seq_en-800.webp 800w,/assets/img/attention_seq_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention_seq_en.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 4:</strong> Attention weights for an input with reference to the output at position <em>i=2</em></p> <p>The architecture extended by attention transmits context-dependent states \(c_{i}\) to the decoder instead of \(c\) for each input: \begin{equation} c_{i} = \sum_{t=1}^{T}a_{it}h^{t} \end{equation} The weight \(a_{it}\) for each state \(h^{t}\) (also called ‘annotation’ in (Bahdanau et al. 2014)) is determined as follows: \begin{equation} a_{it} = \frac{\exp(e_{it})}{\sum_{k=1}^{T}\exp(e_{ik})} \end{equation} where \(a_{it}\) is a normalization (softmax function) for the model \(e_{it}\). This model is again a feedforward network with a single layer that evaluates how well the input at time \(t\) matches the output at position \(i\). Thus, overall, each input \(x^{1}...x^{T}\) receives its own set of attention weights, resulting in \(c_{i}\), a context vector that helps the decoder determine the appropriate output (e.g. ‘l’homme’) for each input.</p> <h2 id="3-transformer-models-with-self-attention">3 Transformer models with self attention</h2> <h4 id="31-the-structure-of-a-transformer-model">3.1 The structure of a Transformer model</h4> <p>The Transformer architecture (Vaswani et al., 2017) combines some of the previously mentioned elements. The architecture gives the attention mechanism a much greater role and dispenses with recurrent structures.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_encoder_en-480.webp 480w,/assets/img/transformer_encoder_en-800.webp 800w,/assets/img/transformer_encoder_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_encoder_en.png" class="img-fluid mx-auto d-block" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 5:</strong> Encoder of a Transformer model</p> <p>The encoder of the Transformer architecture consists of stacks, each with two components, through which the incoming information is processed (see <strong>Fig. 5</strong>). Inputs are first fed in <strong>parallel</strong> to a layer with a self-attention mechanism, which is presented in Vaswani et al. (2017). After this mechanism has been applied, the information is normalized and then passed to another layer with a feedforward neural network. With this network, the processing of the input takes place <strong>individually</strong>. With an intermediate normalization step, mean values and standard deviations are calculated according to the principle of <em>layer normalization</em> (Ba et al. 2016; there is also root square layer normalization by Zhang &amp; Sennrich (2019), which was used in Llama 2, for example). In addition, the normalized output is added to the output of the previous layer. This is also referred to as ‘residual connection’ and is a method to counteract the problem of disappearing gradients during backpropagation.</p> <h4 id="32-embeddings-with-positional-encoding">3.2 Embeddings with positional encoding</h4> <p>As with the previous sequence-to-sequence architectures, the input is first converted into embeddings. However, the embeddings are additionally provided with positional encoding, which is realized via a frequency representation (sine and cosine functions). The reason for this is as follows. In contrast to the recurrent approaches, the attention layer of a Transformer encoder processes an input sequence all at once and not word by word in the case of a translation, for example. Without additional information on the position of each input within a sequence, the coders would lack the important information on how the individual words follow each other.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embeddings_en-480.webp 480w,/assets/img/embeddings_en-800.webp 800w,/assets/img/embeddings_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embeddings_en.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 6:</strong> Example sequence whose tokens are converted into embeddings with \(d=4\)</p> <p>For processing the input, the Transformer architecture provides an embedding matrix for all vocabulary of the data that is used in the training of a Transformer model. The size of the matrix corresponds to the number of vocabulary words (otherwise referred to as tokens, which also include punctuation marks, for example) with a selected dimension (i.e. n x d), with which each input token can be assigned exactly one row within the matrix (cf. <strong>Fig. 6</strong>). The number of columns corresponds to the selected dimension. The matrix values for the token types are randomly selected during the initialization of a Transformer model. It is the rows of this matrix that are also referred to as embeddings. It can also be said that each token type has a vector representation. In a further step, these vectors are added to the positional encoding to give the input a unique representation. It is crucial that the embeddings of the Transformer architecture also change in the course of training, i.e. they can be adapted according to the data, i.e. ‘learned’. (Note 1: the first step is to convert the input tokens into an index representation based on a tokenizer, with which each token can be assigned a row in the embedding matrix; note 2 – and this took me some time to understand: BERT embeddings and the like are not those embeddings from the input; these embeddings are rather the result of the attention mechanisms and extracted from the end of a Transformer layer).</p> <p>To illustrate positional encoding, it is helpful to first consider a very simplified example. Assume that each embedding is simply assigned the positions (<strong><em>pos</em></strong>) of the respective token in the form of integers with \(n \in N\), where \(N\) corresponds to the length of the input tokens including <strong>&lt;eos&gt;</strong> tokens. If we select the token “heiße” and its embedding [0.11, 0.45, 0.23, 0.77] (imaginary values), then a positional encoding of [2, 2, 2, 2] could be determined for the token within the sequence “&lt;eos&gt; ich heiße max &lt;eos&gt;” as follows. The vector of the token would have these values because we chose the second position of the sequence (sequence starts at 0) and an embedding dimension of \(d=4\). According to the Transformer architecture, we could then add this vector to the embedding of the token [2.11, 2.45, 2.23, 2.77] and thus add additional information to it. However, this approach would lead to several problems; for example: large position values for long sequences would override the values of the embeddings and a relative reference of recurring positional patterns would be missing.</p> <p>Vaswani et al. (2017, p. 6) therefore present positional encodings that provide each token embedding with additional information about the position of the token within a sequence via the radians of the trigonometric functions. The advantages of this approach include the fact that the positional encoding values can be limited to an interval of \([-1,1]\), and the periodicity of the trigonometric functions also allows recurring patterns to be mapped. This is so because certain distances between positions will produce similar values. This makes it easier for the model to learn that certain patterns or distances between tokens are repeated in different areas of a sequence, regardless of the exact position in the sequence.</p> <p>According to Vaswani et al. (2017), the positions of the tokens are calculated as follows: \begin{equation} PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} \begin{equation} PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d}}}) \end{equation} <strong><em>pos</em></strong> corresponds to the absolute position within an input sequence of length <strong>N</strong>, the value \(10000\) is a selected constant, and <strong>i</strong> refers to the indices of the embeddings. For example, for a selected dimension of the embedding vectors with \(d=4\), \(i \in I= \{0, 1, 2, 3\}\) applies. Finally, the two sine and cosine functions allow different values to be determined for even and odd indices. We can use (10) for all even indices of an embedding and (11) for all odd indices. It is worth mentioning here that the frequencies of the sine and cosine functions of the <strong>PE</strong> depend on the selected dimension. Small ebedding dimensions lead to higher frequencies (finer position resolutions) and high dimensions to lower frequencies (coarser position resolutions). Based on these specifications, a positional encoding matrix is finally calculated for each input – i.e. a position vector for each token (see <strong>Fig. 7</strong>). In combination with the embeddings, the Transformer model is given a context-sensitive representation of the tokens for further processing in this fashion.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_encoding_en-480.webp 480w,/assets/img/positional_encoding_en-800.webp 800w,/assets/img/positional_encoding_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/positional_encoding_en.png" class="img-fluid mx-auto d-block" width="90%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 7:</strong> Exemplary sequence, whose tokens are mapped to positional encodings with \(d=4\)</p> <h4 id="33-self-attention">3.3 Self-attention</h4> <p>In contrast to the attention mechanism according to Bahdanau et al. (2014), Vaswani et al. (2017) develop a self-attention mechanism, which they also describe as ‘scaled scalar product attention’ (Vaswani et al., 2017, p.3). In simplified terms, the self-attention used for Transformers can first be compared with the operation from (8) (Raschka et al., 2022). We can calculate the attention for a context-sensitive vector \(z_{i}\) of an input at position \(i\) as follows (Raschka et al., 2022): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}x^{j} \end{equation} where \(a_{ij}\) is not multiplied by a state \(h^{t}\), but by the inputs \(x^{j}\), with \(j\in{\{1, ..., T\}}\) an input sequence of length \(T\) (cf. the sum over all \(x^{j}\) in (12)). In contrast to Bahdanau et al. (2014), \(a\) is not a normalization of simple feedforward networks \(e_{ij}\), but a softmax normalization over the scalar products \(\Omega\) of the input \(x^{i}\) related to all other inputs \(X=x^{1}, ..., x^{T}\) (Raschka et al., 2022): \begin{equation} a_{ij} = \frac{\exp(\omega_{ij})}{\sum_{j=1}^{T}\exp(\omega_{ij})} \end{equation} with (Raschka et al., 2022): \begin{equation} \omega_{ij} = x^{(i)T}x^{j} \end{equation} What we also see here, in contrast to the attention mechanism of Bahdanau et al. (2014), in which attention includes in particular the output of the decoder (there at output position <em>i</em>), is that the attention weight in (13) with (14) refers to the other inputs of a sequence. For this very reason, it makes sense to speak of self-attention.</p> <p>To this representation of attention, Vaswani et al. (2017) add a further change for each input \(x^{i}\), namely the weight \(a\) is not multiplied by \(x^{j}\), but by a value \(v^{j}\): \begin{equation} z_{i} = \sum_{j=1}^{T}a_{ij}v^{j} \end{equation} Vaswani et al. (2017) transform each \(x^{i}\) into a triple of (\(v^{i}\), \(k^{i}\), \(q^{i}\)) using the projection matrices (\(W_{v}\), \(W_{k}\), \(W_{q}\) – which can also be understood here as additional linear layers). The idea behind this comes from <em>information retrieval</em>, which works with value, key and query triples (hence the abbreviations v, k, q). The <strong>V</strong>, <strong>K</strong>, <strong>Q</strong> in <strong>Fig. 5</strong> and <strong>Fig. 9</strong> correspond to: \(V=XW_{v}\), \(K=XW_{k}\) and \(Q=XW_{q}\) (cf. <strong>Fig. 9</strong>). The scalar products of the self-attention mechanism for each input are also not calculated with (14) in Vaswani et al. (2017), but with the query and key values (Raschka et al., 2022): \begin{equation} \omega_{ij} = q^{(i)T}k^{j} \end{equation} In short: In addition to the attention of an input \(x^i\) to all other inputs within a sequence \(X\), the self-attention is calculated by different representations of all surrounding \(x^j \in X\) in the form of query, key and value representations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projection_matrices_en-480.webp 480w,/assets/img/projection_matrices_en-800.webp 800w,/assets/img/projection_matrices_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/projection_matrices_en.png" class="img-fluid mx-auto d-block" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 8:</strong> Attention for input \(X\) with multiple heads</p> <p>Finally, the attention weights are scaled with the dimension of the embeddings (\(\frac{\omega_{ij}}{\sqrt{d}}\)) and can be calculated \(h\) times in parallel, where \(h\) corresponds to a selected number of heads (also called attention heads). Vaswani et al. (2017) choose \(h=8\) heads, whose values are concatenated and finally passed on to the layer normalization in the coders (see <strong>Fig. 8</strong> and <strong>Fig. 5</strong>). The use of multiple heads is referred to as ‘multi-head attention’. Vaswani et al. (2017, p. 4) justify the additional scaling with the observation that too large values of the scalar products (see (14)) lead the softmax function used for additional normalization into a range that results in very small gradients during learning.</p> <h4 id="34-the-transformer-decoder">3.4 The Transformer decoder</h4> <p>The decoder of the Transformer architecture follows the structure of the encoder. However, it contains an additional layer (see <strong>Fig. 9</strong>). In this layer, the information output by the encoder (e.g. the encoded original sentence of a translation) is also taken into account via the value and key values \(V\), \(K\). The query values \(Q\), on the other hand, come from the previous attention layer of the decoder. Due to the combination of information from both the encoder and the decoder, this additional layer is also referred to as the ‘cross-attention layer’. Since encoder information is included, the whole model (encoder + decoder) can also be referred to as a <em>conditional</em> language model, as previously presented in (5).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_decoder_en-480.webp 480w,/assets/img/transformer_decoder_en-800.webp 800w,/assets/img/transformer_decoder_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer_decoder_en.png" class="img-fluid mx-auto d-block" width="70%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 9:</strong> Decoder of a Transformer model</p> <p>The self-attention layer of the decoder allows parts of the decoder input to be masked (this is also described as masking, or more precisely causal masking). A Transformer encoder, on the other hand, allows all inputs to be viewed simultaneously. Masking plays an important role in the goal of the decoder: e.g. the prediction of a translation. To predict a translation sequence, the decoder works autoregressively from token to token (e.g. from left to right, see also (7)). This means that during inferencing, each prediction only works with the help of previous tokens, the others remain masked in a figurative sense. For training, correct translations can be added to the model as input by Professor Forcing in order to minimize error propagation – as with the sequence-to-sequence models described above. Basically, the training goal is to optimize the predictions of the model based on the translation solutions. A translation process terminates in any case when the token <strong>&lt;eos&gt;</strong> or the previously defined maximum sequence length is reached.</p> <p>Finally, a word about the linear layer at the end of the decoder (see <strong>Fig. 9</strong>). The immediate output from the attention blocks comprises a representation of the input embeddings \(h_{i}\) enriched by the model. Additional information from surrounding tokens and the encoder has been incorporated into this representation through the attention mechanisms and the feedforward neural networks. Now each \(h_{i}\) must be converted back into a representation of the vocabulary. For this purpose, the linear layer provides a projection matrix \(W\). This is similar to a layer of a neural network with the difference that no non-linear activation function further alters the information flow.</p> <p>Let’s look at an example. Suppose the model is based on a vocabulary size of \(10 000\) and we choose a dimension of \(d=512\) as an example for the embeddings or the status of the model. We can then use \(W\) (10000 x 512) to convert all \(h_{i}\) into a logits vector that corresponds to the dimension of the vocabulary and whose value is also the model’s approximation of how likely each of the vocabulary’s tokens is: \begin{equation} logits = W \cdot h_{i} + b \end{equation} where \(b\) as an additional bias has an influence on the mapping. Based on this logits vector (e.g. \(logits = [3.4, -1.2, 0.5, ..., 2.7]\)), the softmax activation, with which the values of the vector are converted into probabilities, can finally predict the most probable token for the output status \(h_{i}\) of the decoder. However, other decoding strategies (e.g. beam search or greedy decoding) could also be used at this point.</p> <p>Overall, inference and training in a Transformer model do not differ from other neural networks (see chapter <strong>1.2</strong>).</p> <h2 id="4-gpt-bert-and-co">4 GPT, BERT and co</h2> <p>While the original Transformer architecture was developed for machine translation, Transformer models have also proven themselves in other tasks. The best known are large language models such as Generative Pre-trained Transformer models (Radford et al., 2018) from OpenAI, which ‘continue’ an input (prompt) with a Transformer decoder, i.e. predict the next tokens of the sequence. Language models such as GPT consist of only decoder stacks. The Bidirectional Encoder Representations from Transformers model (BERT for short, Devlin et al., 2019) is in turn a pure Transformer encoder. This means that BERT cannot be used to generate new words or sentences in a target language through autoregression. Instead, BERT provides encodings that can be used to solve classification tasks, for example. In general, the development of most Transformer models consists of two phases: pre-training, and fine-tuning for specific applications. Finally, I will present these two steps with BERT.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/language_model_types_en-480.webp 480w,/assets/img/language_model_types_en-800.webp 800w,/assets/img/language_model_types_en-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/language_model_types_en.png" class="img-fluid mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Fig. 10:</strong> Different Transformer architectures: encoder-decoders for machine translation (also called MTM), single decoders for sequence generation (also called LM), and single encoders for downstream tasks (also often called MLM)</p> <h4 id="41-the-bidirectional-encoder-representations-from-transformers-model">4.1 The Bidirectional Encoder Representations from Transformers model</h4> <p>For BERT, Devlin et al. (2019) first train the encoder of a Transformer model against the background of two tasks. The first task of the BERT training consists of masked language modeling (MLM). The model is shown sentences in which it has to predict 15% randomly selected words that are masked. The second task consists of a binary classification of two sentences with the aim of predicting whether or not they follow each other. The model is shown 50% correct sentence sequences and 50% incorrect sequences. Since the model only uses an encoder – and no ‘right-aligned’ masking of the next tokens within a sequence is performed as with a Transformer decoder – the training can also be described as bi-directional. The encoder has access to the inputs from both sides. Devlin et al. (2019) refer to the training of their encoder on the two tasks as pre-training.</p> <p>In a further step, Devlin et al. (2019) use the pre-trained BERT model for experiments in natural language processing. For example, they fine-tune BERT to classify the most plausible word sequence for sentences from the data set Situations With Adversarial Generations (Zellers et al., 2018). To do this, BERT is shown various possible continuations for training. The model must select the most plausible one from these. Since BERT was originally pre-trained against the background of other tasks, but the weights of the model can also be used and adapted for such new tasks, such fine-tuning of BERT can also be described as a form of transfer learning.</p> <h2 id="additional-resources">Additional resources</h2> <ul> <li>I can recommend Brendan Bycroft’s visualizations of Transformer models: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></li> <li>Lena Voita provides excellent explanations and visualizations of different modeling approaches in NLP, including the Transformer architecture: <a href="https://lena-voita.github.io/nlp_course.html">https://lena-voita.github.io/nlp_course.html</a></li> <li>Jay Alammar provides further helpful visualizations of BERT: <a href="https://jalammar.github.io/illustrated-bert/">https://jalammar.github.io/illustrated-bert/</a></li> </ul> <h2 id="bibliography">Bibliography</h2> <p>Ba, J., Kiros, J.R., &amp; Hinton, G.E. (2016). Layer Normalization. ArXiv, abs/1607.06450.</p> <p>Bahdanau, D., Cho, K., und Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. <em>CoRR</em>, abs/1409.0473.</p> <p>Cho, K., van Merri ̈enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Moschitti, A., Pang, B., and Daelemans, W., editors, <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, Seiten 1724–1734, Doha, Qatar. Association for Computational Linguistics.</p> <p>de Saussure, F. (1931). <em>Cours de Linguistique Generale</em>. Payot, Paris.</p> <p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J., Doran, C., und Solorio, T., editors, <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p> <p>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p> <p>Goyal, A., Lamb, A. M., Zhang, Y., Zhang, S., Courville, A. C., and Bengio, Y. (2016). Professor Forcing: A New Algorithm for Training Recurrent Networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., und Garnett, R., Herausgeber, Advances in <em>Neural Information Processing Systems</em>, Band 29. Curran Associates, Inc.</p> <p>Hochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Comput.</em>, 9(8):1735–1780.</p> <p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. Technical report, OpenAI.</p> <p>Raschka, S., Liu, Y., and Mirjalili, V. (2022). <em>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em>. Packt Publishing.</p> <p>Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS’14, pages 3104–3112, Cambridge, MA, USA. MIT Press.</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is All you Need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., und Garnett, R., editors, <em>Advances in Neural Information Processing Systems</em>, Band 30. Curran Associates, Inc.</p> <p>Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Riloff, E., Chiang, D., Hockenmaier, J., und Tsujii, J., editors, <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 93–104, Brussels, Belgium. Association for Computational Linguistics.</p> <p>Zhang, B., and Sennrich, R. (2019). Root mean square layer normalization. <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc., Red Hook, NY, USA.</p> <p><strong>Version 1.1</strong></p>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><category term="neural"/><category term="nets"/><category term="feed-forward"/><category term="attention-mechanism"/><category term="Transformer"/><category term="GPT"/><category term="BERT"/><category term="transfer-learning"/><category term="English"/><summary type="html"><![CDATA[From neural networks to GPT and BERT, a contextualized explanation of the Transformer architecture]]></summary></entry></feed>